[
  {
    "objectID": "Week2.html",
    "href": "Week2.html",
    "title": "3  Week 2: Data Cleaning & Pipeline Setup",
    "section": "",
    "text": "3.1 Overview\nThe team expanded the dataset, finalized the fine-tuning pipeline, completed baseline and human evaluation preparations, and built initial tools and documentation to support quality checks and research development.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2: Data Cleaning & Pipeline Setup</span>"
    ]
  },
  {
    "objectID": "Week2.html#rohan-aby-deliverables",
    "href": "Week2.html#rohan-aby-deliverables",
    "title": "3  Week 2: Data Cleaning & Pipeline Setup",
    "section": "3.2 Rohan Aby Deliverables",
    "text": "3.2 Rohan Aby Deliverables\nAI assistance: ChatGPT was used to develop the 200-question comprehensive test dataset (accessed Nov, 2025).\n\n3.2.1 200-question comprehensive test dataset\nThe dataset was expanded to 100 more questions using AI. The questions were divided into qusetions on biblical contextualization (25 questions), Cultural Sensitivity (25 questions), Controversial Topics (i.e. cow slaughter, forced conversion) (25 Questions), Regional Variations (25 Questions)\n\n\n3.2.2 RAG baseline evaluation complete (CSI scoring)\nRohan selected 20 questions from the list and scored the responses based on the CSI frameowork. He scored the original model (Download the results of original model’s responses) vs the fine tuned model (Download the results of fine tuned responses). The result was that the orignal model performed better than the fine tuned model as can be seen in the image below. The original model had a score of 99.2 while the fine tuned model scored 85.55. The responses from the original model was more detailed and contained more information while the fine tuned model was short and did not contain much information. The scoring of the models can be seen below.  Fig. 1: CSI Score visualization",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2: Data Cleaning & Pipeline Setup</span>"
    ]
  },
  {
    "objectID": "Week1.html",
    "href": "Week1.html",
    "title": "2  Week 1: Foundations & Data Collection",
    "section": "",
    "text": "2.1 Overview\nThe team established the foundational plans, infrastructure, evaluation framework, and architecture needed to begin data collection and model fine-tuning in Week 1.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: Foundations & Data Collection</span>"
    ]
  },
  {
    "objectID": "Week1.html#rohan-aby-deliverables",
    "href": "Week1.html#rohan-aby-deliverables",
    "title": "2  Week 1: Foundations & Data Collection",
    "section": "2.2 Rohan Aby Deliverables",
    "text": "2.2 Rohan Aby Deliverables\nAI assistance: ChatGPT was used to develop the CSI framework document, 100-question baseline evaluation dataset, Evaluation pipeline code and the Metrics definition document (accessed Nov, 2025).\n\n2.2.1 CSI framework document with detailed rubric\nRohan created the Cultural Sensitivity Index (Download CSI Framework document)\nThe Cultural Sensitivity Index Framework uses 4 main variables to judge the model’s performance. The variables used to judge the models performance are accuracy, tone, context, and empathy. Accuracy evaluates factual correctness about cultural practices, festivals, history, and social norms. Tone measures how respectful, inclusive, and free of stereotypes or bias the language is. Context assesses whether the response reflects appropriate cultural framing and situational relevance. Empathy captures understanding of lived experience, cultural values, and emotional nuance. Each of these four criteria is multiplied by a number which indicates how important the criteria each is in the total CSI score. The score for each response is as follows\nCSI Score = (0.3 * Accuracy) + (0.3 * Tone) + (0.2 * Context) + (0.2 * Empathy)\nFrom this formula we can infer that accuracy and tone are more important than context or Empathy. Each of the variables are scored on a scale from one to five. One being the lowest score and five being the best score.\n\n\n2.2.2 100-question baseline evaluation dataset\nRohan created the 100 questions. (Download the 100 Indian Cultural Questions document)\nThe 100 questions can be divided into 4 categories. They are as follows.\n\nIndian Festivals and Biblical Parallels (Diwali / Christmas themes) (20 questions each)\nCaste Sensitivity in Christian Contexts (20 questions each)\nHindu–Christian Cultural Bridges (20 questions each)\nRegional Variations (North vs South India) (20 questions each)\nGenerational Differences in Faith Expression (20 questions each)\n\n\n\n2.2.3 Evaluation pipeline code\nThe evaluation pipeline codes consists of three python files that are used to automate the scoring calculations.\nevaluate_automated_metrics.py - Automatically evaluates the model outputs using standard NLP metrics. The code is shown below.\nimport pandas as pd\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom rouge import Rouge\nfrom sentence_transformers import SentenceTransformer, util\nimport math\n\ndef compute_perplexity(probabilities):\n    return math.exp(-sum(math.log(p) for p in probabilities) / len(probabilities))\n\ndef evaluate_automated_metrics(df):\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n    rouge = Rouge()\n    results = []\n\n    for _, row in df.iterrows():\n        ref = row['reference']\n        gen = row['generated']\n\n        # BLEU\n        bleu = sentence_bleu([ref.split()], gen.split())\n\n        # ROUGE\n        rouge_scores = rouge.get_scores(gen, ref)[0]['rouge-l']['f']\n\n        # Semantic Similarity\n        emb1 = model.encode(ref, convert_to_tensor=True)\n        emb2 = model.encode(gen, convert_to_tensor=True)\n        sim = util.pytorch_cos_sim(emb1, emb2).item()\n\n        results.append({\n            \"BLEU\": bleu,\n            \"ROUGE-L\": rouge_scores,\n            \"Semantic Similarity\": sim\n        })\n\n    return pd.DataFrame(results)\ncompute_csi_scores.py - Calculates the Cultural Sensitivity Index CSI) for model responses using a human or rubric-based scoring template. The code is shown below.\nimport pandas as pd\n\ndef compute_csi_score(row):\n    weights = {\"Accuracy\": 0.3, \"Tone\": 0.3, \"Context\": 0.2, \"Empathy\": 0.2}\n    return sum(row[k] * w for k, w in weights.items())\n\ndef apply_csi_rubric(df):\n    df[\"CSI_Score\"] = df.apply(compute_csi_score, axis=1)\n    return df[[\"Question_ID\", \"Accuracy\", \"Tone\", \"Context\", \"Empathy\", \"CSI_Score\"]]\n\ncompare_models.py - Aggregates and compares metrics across multiple models (RAG, Fine-Tuned, Baseline). The code is shown below.\nimport pandas as pd\n\ndef compare_models(rag_df, finetuned_df, baseline_df):\n    comparison = pd.DataFrame({\n        \"Metric\": [\"BLEU\", \"ROUGE-L\", \"Semantic Similarity\", \"CSI_Score\"],\n        \"RAG\": [rag_df[\"BLEU\"].mean(), rag_df[\"ROUGE-L\"].mean(), rag_df[\"Semantic Similarity\"].mean(), rag_df[\"CSI_Score\"].mean()],\n        \"Fine-Tuned\": [finetuned_df[\"BLEU\"].mean(), finetuned_df[\"ROUGE-L\"].mean(), finetuned_df[\"Semantic Similarity\"].mean(), finetuned_df[\"CSI_Score\"].mean()],\n        \"Baseline\": [baseline_df[\"BLEU\"].mean(), baseline_df[\"ROUGE-L\"].mean(), baseline_df[\"Semantic Similarity\"].mean(), baseline_df[\"CSI_Score\"].mean()]\n    })\n    return comparison\n\n\n2.2.4 Metrics definition document\nThis document is a table detailing various metrics used for the evaluation of language generation models. (Download the Metrics definition document)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: Foundations & Data Collection</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ConLingo Quarto Book",
    "section": "",
    "text": "1 ConLingo 2.0",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>ConLingo 2.0</span>"
    ]
  },
  {
    "objectID": "index.html#problem-statement",
    "href": "index.html#problem-statement",
    "title": "ConLingo Quarto Book",
    "section": "1.1 Problem Statement",
    "text": "1.1 Problem Statement\nCulture is an essential aspect of society and how we relate and think in our day-to-day lives. Because culture is necessary to help make sense of what occurs around us in different contexts, it is important as the advancement of AI that we consider how and if different cultures are well-represented in an LLM. The cultural bias implicit in AI models is what this project aims to understand and ways to improve cultural understanding of the original model.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>ConLingo 2.0</span>"
    ]
  },
  {
    "objectID": "index.html#project-context",
    "href": "index.html#project-context",
    "title": "ConLingo Quarto Book",
    "section": "1.2 Project Context",
    "text": "1.2 Project Context\nWe will be tackling this research problem by specifically fine-tuning a base model such as the Llama Model, on a specific culture, in this case India, and seeing how the variety of data influences the model. Additionally, we will be comparing the models created from these data sources to an existing Retrieval Augmented Generation (RAG) implementation which was trained on 300,000+ data points of Reddit posts, threads and comments from the subreddits r/India.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>ConLingo 2.0</span>"
    ]
  },
  {
    "objectID": "index.html#research-question",
    "href": "index.html#research-question",
    "title": "ConLingo Quarto Book",
    "section": "1.3 Research Question",
    "text": "1.3 Research Question\nWe want to answer the question, how does a dataset affect a model in the fine tuning process. In our case specifically, Supervised Fine Tuning (SFT), and secondly, can our finetuned model perform at par or better than the existing RAG implementation.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>ConLingo 2.0</span>"
    ]
  },
  {
    "objectID": "index.html#project-timeline",
    "href": "index.html#project-timeline",
    "title": "ConLingo Quarto Book",
    "section": "1.4 Project Timeline",
    "text": "1.4 Project Timeline\n\n\n\n\n\n\n\nWeek\nFocus\n\n\n\n\nWeek 1\nFoundations & Data Collection\n\n\nWeek 2\nData Cleaning & Pipeline Setup\n\n\nWeek 3\nData Searching, Data Vetting, Dataset Collection, Data Cleaning\n\n\nWeek 4\nFine-Tuning the Model on Titan\n\n\nWeek 5\nCompare Fine-tuned model with RAG implementation and ChatGPT-5 & Research Paper",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>ConLingo 2.0</span>"
    ]
  },
  {
    "objectID": "index.html#team",
    "href": "index.html#team",
    "title": "ConLingo Quarto Book",
    "section": "1.5 Team",
    "text": "1.5 Team\n\nTeam Leader: Rohan Aby\nTeam Members: Moses Madale, Suwilanji Mwanza, William Richards",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>ConLingo 2.0</span>"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "ConLingo Quarto Book",
    "section": "1.6 Acknowledgments",
    "text": "1.6 Acknowledgments\nSpecial thanks to: - Dr. V for guiding us",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>ConLingo 2.0</span>"
    ]
  },
  {
    "objectID": "Week3.html",
    "href": "Week3.html",
    "title": "4  Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning",
    "section": "",
    "text": "4.1 Overview\nIn Week 3, the team focused on searching, evaluating, and collecting culturally relevant datasets across multiple sources, developing a cultural vetting framework, defining cultural values, improving data quality and formatting, categorizing data into modern and traditional culture, and collaboratively selecting five high-quality datasets for further preparation.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span>"
    ]
  },
  {
    "objectID": "Week3.html#rohan-aby-deliverables",
    "href": "Week3.html#rohan-aby-deliverables",
    "title": "4  Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning",
    "section": "4.2 Rohan Aby Deliverables",
    "text": "4.2 Rohan Aby Deliverables\nAI assistance: ChatGPT was used to develop the scripts used for collecting and sorting the Wikipedia data related to India (accessed Nov, 2025).\n\n4.2.1 Collection and sorting of Wikipedia data related to India.\nRohan collected Wikipedia data by writing two python files which were written to create to download a subset of English Wikipedia articles and then categorize those articles based on their relevance to Indian culture and history between the years 1900 and 2025.\nLoadIndiaDataSet1900_2025.py was created to load and filter the dataset. It uses a regular expression to grab data that only has a range from 1900 to 2025. The articles had to contain at least one mention of a year within the 1900–2025 range in their text. The resulting subset of articles is saved to a local directory named “wikipedia_1900_2025”. The LoadIndiaDataSet1900_2025.py can be seen below.\nfrom datasets import load_dataset\nimport re\n\n# Load English Wikipedia snapshot (latest available)\ndataset = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split=\"train\")\n\n# Define regex for years between 1900 and 2025\nyear_pattern = re.compile(r\"\\b(19[0-9]{2}|20[0-2][0-9]|2025)\\b\")\n\ndef mentions_year_range(example):\n    text = example[\"text\"]\n    return bool(year_pattern.search(text))\n\n# Filter dataset for pages mentioning years 1900–2025\ndataset_filtered = dataset.filter(mentions_year_range)\n\nprint(f\"Filtered dataset size: {len(dataset_filtered)}\")\ndataset_filtered.save_to_disk(\"wikipedia_1900_2025\")\nSortIndiaData.py was created to take the filtered data from the first script and sorts it into specific categories related to Indian culture. It creates an output directory named “india_categorized”. It uses a dictionary structure to define two main categories, “Non-Material Culture” and “Material Culture”, which are further divided into specific subcategories, each with a list of associated keywords (e.g., “Religion & Spirituality” has keywords like “hinduism,” “yoga,” “temple,” etc.). The script iterates through every article in the loaded dataset and performs a two-step filtering process: A helper function (is_india_paragraph) ensures a paragraph is kept only if it contains the words “india,” “indian,” or “bharat.” This is done paragraph-by-paragraph to avoid including entire articles that only mention India once in an unrelated context. It then checks the remaining India-related paragraphs to see if they contain any of the keywords defined for the current subcategory being processed. The SortIndiaData.py can is shown below.\nimport os\nimport json\nfrom datasets import load_from_disk\nimport re\n\n# ------------------------------\n# Load dataset\n# ------------------------------\ndataset_dir = \"wikipedia_1900_2025\"\ndataset = load_from_disk(dataset_dir)\nprint(f\"Total articles loaded: {len(dataset)}\")\n\n# ------------------------------\n# Define categories & keywords\n# ------------------------------\ncategories = {\n    \"Non-Material Culture\": {\n        \"Values & Beliefs\": [\"values\", \"beliefs\", \"morality\", \"ethics\"],\n        \"Regional Indian Superstitions & Beliefs\": [\"superstition\", \"omen\", \"ritual\", \"folk belief\"],\n        \"Norms & Customs\": [\"norms\", \"customs\", \"traditions\", \"practices\"],\n        \"India News Headlines\": [\"news\", \"headlines\", \"current events\", \"india\"],\n        \"Language\": [\"language\", \"dialect\", \"linguistic\", \"vernacular\"],\n        \"Religion & Spirituality\": [\"hinduism\", \"buddhism\", \"sikhism\", \"islam\", \"christianity\", \"spirituality\", \"temple\", \"yoga\"],\n        \"Arts and Literature\": [\"art\", \"literature\", \"painting\", \"music\", \"dance\", \"poetry\"],\n        \"Social Organization\": [\"caste\", \"community\", \"society\", \"family\", \"clan\"],\n    },\n    \"Material Culture\": {\n        \"Artifacts & Technology\": [\"artifact\", \"tool\", \"technology\", \"weapon\", \"craft\"],\n        \"Government & Economic Systems\": [\"government\", \"politics\", \"economy\", \"trade\", \"industry\"],\n    }\n}\n\n# ------------------------------\n# Create output directory\n# ------------------------------\noutput_dir = \"india_categorized\"\nos.makedirs(output_dir, exist_ok=True)\n\n# ------------------------------\n# Helper function: check India context\n# ------------------------------\ndef is_india_paragraph(paragraph):\n    paragraph_lower = paragraph.lower()\n    # Keep paragraph only if it mentions India/Indian/Bharat\n    return any(x in paragraph_lower for x in [\"india\", \"indian\", \"bharat\"])\n\n# ------------------------------\n# Filter and categorize\n# ------------------------------\nfor super_category, subcats in categories.items():\n    for subcat_name, keywords in subcats.items():\n        filtered_articles = []\n        for example in dataset:\n            title = example.get(\"title\", \"\").strip()\n            text = example.get(\"text\", \"\")\n\n            if not text:\n                continue\n\n            # Split into paragraphs to avoid unrelated global mentions\n            paragraphs = [p.strip() for p in text.split(\"\\n\") if p.strip()]\n            india_paragraphs = [p for p in paragraphs if is_india_paragraph(p)]\n\n            if not india_paragraphs:\n                continue\n\n            # Check category keywords in India paragraphs\n            matched_paragraphs = []\n            for p in india_paragraphs:\n                if any(k.lower() in p.lower() for k in keywords):\n                    matched_paragraphs.append(p)\n\n            if matched_paragraphs:\n                filtered_articles.append({\"title\": title, \"text\": \"\\n\".join(matched_paragraphs)})\n\n        # Save subcategory as JSONL\n        subcat_file = os.path.join(output_dir, f\"{subcat_name.replace(' ', '_')}.jsonl\")\n        with open(subcat_file, \"w\", encoding=\"utf-8\") as f:\n            for article in filtered_articles:\n                json.dump(article, f)\n                f.write(\"\\n\")\n\n        print(f\"{subcat_name}: {len(filtered_articles)} articles saved.\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span>"
    ]
  },
  {
    "objectID": "Week1.html#william-richards-deliverables",
    "href": "Week1.html#william-richards-deliverables",
    "title": "2  Week 1: Foundations & Data Collection",
    "section": "2.3 William Richards Deliverables",
    "text": "2.3 William Richards Deliverables\nWilliams first deliverable was to establish a good architecture to our original plan, creating a development roadmap and risk mitigation table along with the architectural diagram.\nArchitecture Diagram \nDevelopment Roadmap   Risk Mitigation Table\n\n\n\nRisk Mitigation Table\n\n\nNow that the (original) layout of the project was set in stone, validation of the data needed to be done to ensure the new and improved model performs accurately with no risk of bad data interfering with ConLingo 2.0.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: Foundations & Data Collection</span>"
    ]
  },
  {
    "objectID": "Week1.html#development-roadmap",
    "href": "Week1.html#development-roadmap",
    "title": "2  Week 1: Foundations & Data Collection",
    "section": "2.4 Development Roadmap",
    "text": "2.4 Development Roadmap\n  Risk Mitigation Table\n\n\n\nRisk Mitigation Table\n\n\nNow that the (original) layout of the project was set in stone, validation of the data needed to be done to ensure the new and improved model performs accurately with no risk of bad data interfering with ConLingo 2.0.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: Foundations & Data Collection</span>"
    ]
  },
  {
    "objectID": "Week2.html#william-richards-deliverables",
    "href": "Week2.html#william-richards-deliverables",
    "title": "3  Week 2: Data Cleaning & Pipeline Setup",
    "section": "3.3 William Richards Deliverables",
    "text": "3.3 William Richards Deliverables\nWilliams task was to validate the data by creating visualizations for each set of criteria to ensure the data aligned with them for fine-tuning. These criteria included Cultural Accuracy, Biases or Stereotyping, and Tone or Respect. This was the most time-intensive deliverable, as he reviewed each dataset manually to determine which entries were suitable. After completing his review, he ran the data through the then-new ChatGPT 5.1, which confirmed 95% of his validations.\nIn addition, he worked on establishing the future direction for the project by creating a simple outline for the research paper that would ultimately serve as the final report.\nData Quality Validation\n\nlibrary(tidyverse)\nlibrary(rstudioapi)\n\ndata_path &lt;- \"Appendix/WilliamRichards/data/conlingo_week2_200_labeled.csv\"\ntb_plots &lt;- read.csv(data_path)\nglimpse(tb_plots)\n\n# Cultural Accuracy Plot: How accurate is the data to the respective culture?\n#   Scaling: 0 = Not at all\n#            1 = Simple references\n#            2 = Highly relevant\nggplot(tb_plots, aes(x = \"Cultural Accuracy\", y = cultural_accuracy, color = keep)) +\n  geom_jitter(width = 0.15, alpha = 0.7) +\n  coord_cartesian(ylim = c(-0.1, 2.1)) +\n  labs(\n    title = \"Cultural Accuracy (0–2)\",\n    x = \"\",\n    y = \"Score\",\n    color = \"Keep\"\n  ) +\n  theme_classic()\n\n# Bias / Stereotype Plot: Is than an element of bias or blatant stereotyping?\n#   Scaling: 0 = No\n#            1 = Yes\nstereo_plot &lt;- ggplot(tb_plots, aes(x = \"Stereotype\", y = stereotype, color = keep)) +\n  geom_jitter(width = 0.15, alpha = 0.7) +\n  coord_cartesian(ylim = c(-0.1, 1.1)) +\n  labs(\n    title = \"Bias / Stereotype Flag (0–1)\",\n    x = \"\",\n    y = \"Flag\"\n  ) +\n  theme_classic()\nstereo_plot\n\n# Tone / Respect Plot: Does the data present itself respectfully?\n#   Scaling: 0 = Disrespectful\n#            1 = Neutral\n#            2 = Highly respectful\ntone_plot &lt;- ggplot(tb_plots, aes(x = \"Tone\", y = tone, color = keep)) +\n  geom_jitter(width = 0.15, alpha = 0.7) +\n  coord_cartesian(ylim = c(-0.1, 2.1)) +\n  labs(\n    title = \"Tone / Respect (0–2)\",\n    x = \"\",\n    y = \"Score\"\n  ) +\n  theme_minimal()\ntone_plot\n\nCultural Accuracy Visualization: \nBias / Stereotype Visualization:\n\n\n\nBias / Stereotype Visualization\n\n\nTone / Respect Visualization: \nResearch Paper Outline\nI. Introduction A. Problem: Cultural contextualization for biblical content B. Thesis\n\nRelated Work A. RAG Systems B. Fine-tuning for Cultural tasks C. Indian NLP\nMethodologies\n\nA. Data Collection B. Model Architecture C. Evaluation\n\nResults A. Comparison Introduction B. RAG vs. Fine-tuned\n\nV. Discussion A. When to use what approach? B. Post-execution reflection\n\nConclusion A. Thesis Restate B. Recommendations for OneHope",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2: Data Cleaning & Pipeline Setup</span>"
    ]
  },
  {
    "objectID": "Week3.html#william-richards-deliverables",
    "href": "Week3.html#william-richards-deliverables",
    "title": "4  Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning",
    "section": "4.3 William Richards Deliverables",
    "text": "4.3 William Richards Deliverables\nSpecified Data Collection\nAI Assistance: ChatGPT was used to receive the modern and traditional csv files and add in Bible verses, additional activities, and prompt/answer columns for each example (accessed Nov 4, 2025)\n\n4.3.1 Modern VS Traditional\nWilliam was tasked with collecting 1,000 examples from the datasets selected by Suwilanji in the previous week’s deliverable. His responsibility was to clean these datasets and organize them into separate categories: modern or traditional.\nModern = Hinglish, Spotify, Indian News Survey, and World Values Survey\nTraditional = Regional Indian Superstitions and Pew Indian Survey\n\n\n4.3.2 Data Cleaning\n\nlibrary(tidyverse)\nlibrary(rstudioapi)\n\nscript_dir = dirname(rstudioapi::getActiveDocumentContext()$path)\nsetwd(script_dir)\n\n\n# Modern\n#######################################################\ndata_path_h &lt;- \"Appendix/WilliamRichards/data/train.tsv\"\ntb_hinglish = read.csv(data_path_h)\n\n# Locate NAs\nNAs_found &lt;- FALSE\nfor (nm in names(tb_hinglish)) {\n  if (sum(is.na(tb_hinglish[[nm]]))) {\n    message(paste(nm, \"has NA's\"))\n    NAs_found &lt;- TRUE\n  }\n}\nif (!NAs_found) {\n  print(\"No NAs found\")\n}\n# Drop any rows with NA's\ntb_hinglish &lt;- tb_hinglish %&gt;% \n  drop_na()\nwrite.csv(tb_hinglish, file = \"hinglish.csv\", row.names = FALSE)\n\n\n\ndata_path_s &lt;- \"Appendix/WilliamRichards/data/top_podcasts.csv\"\ntb_spotify = read.csv(data_path_s)\n\n# Locate NAs\nNAs_found &lt;- FALSE\nfor (nm in names(tb_spotify)) {\n  if (sum(is.na(tb_spotify[[nm]]))) {\n    message(paste(nm, \"has NA's\"))\n    NAs_found &lt;- TRUE\n  }\n}\nif (!NAs_found) {\n  print(\"No NAs found\")\n}\n# Drop any rows with NA's\ntb_spotify &lt;- tb_spotify %&gt;% \n  drop_na()\nwrite.csv(tb_spotify, file = \"spotify.csv\", row.names = FALSE)\n\ndata_path_w &lt;- \"Appendix/WilliamRichards/data/world_values_survey_india.xlsx\"\n\ntb_world_vals = read.csv(data_path_w)\n\n# Locate NAs\nNAs_found &lt;- FALSE\nfor (nm in names(tb_world_vals)) {\n  if (sum(is.na(tb_world_vals[[nm]]))) {\n    message(paste(nm, \"has NA's\"))\n    NAs_found &lt;- TRUE\n  }\n}\nif (!NAs_found) {\n  print(\"No NAs found\")\n}\n# Drop any rows with NA's\ntb_world_vals &lt;- tb_world_vals %&gt;% \n  drop_na()\nwrite.csv(tb_world_vals, file = \"world_values.csv\", row.names = FALSE)\n\n\ndata_path_n &lt;- \"Appendix/WilliamRichards/data/india-news-headlines.csv\"\ntb_news = read.csv(data_path_n)\n\n# Locate NAs\nNAs_found &lt;- FALSE\nfor (nm in names(tb_news)) {\n  if (sum(is.na(tb_news[[nm]]))) {\n    message(paste(nm, \"has NA's\"))\n    NAs_found &lt;- TRUE\n  }\n}\nif (!NAs_found) {\n  print(\"No NAs found\")\n}\n# Drop any rows with NA's\ntb_news &lt;- tb_news %&gt;% \n  drop_na()\nwrite.csv(tb_news, file = \"news.csv\", row.names = FALSE)\n\n\n# Traditional\n#######################################################\ndata_path_s &lt;- \"Appendix/WilliamRichards/data/train.csv\"\n\ntb_stitions = read.csv(data_path_s)\n\n\n# Locate NAs\nNAs_found &lt;- FALSE\nfor (nm in names(tb_stitions)) {\n  if (sum(is.na(tb_stitions[[nm]]))) {\n    message(paste(nm, \"has NA's\"))\n    NAs_found &lt;- TRUE\n  }\n}\nif (!NAs_found) {\n  print(\"No NAs found\")\n}\n# Drop any rows with NA's\ntb_stitions &lt;- tb_stitions %&gt;% \n  drop_na()\nwrite.csv(tb_stitions, file = \"india_superstitions.csv\", row.names = FALSE)\n\ndata_path_p &lt;- \"Appendix/WilliamRichards/data/India Religion Public Data - Pew Research Center (All Vars).csv\"\ntb_pew = read.csv(data_path_p)\n\n# Locate NAs\nNAs_found &lt;- FALSE\nfor (nm in names(tb_pew)) {\n  if (sum(is.na(tb_pew[[nm]]))) {\n    message(paste(nm, \"has NA's\"))\n    NAs_found &lt;- TRUE\n  }\n}\nif (!NAs_found) {\n  print(\"No NAs found\")\n}\n# Drop any rows with NA's\ntb_pew &lt;- tb_pew %&gt;% \n  drop_na()\nwrite.csv(tb_pew, file = \"india_pew.csv\", row.names = FALSE)\n\n\n\n4.3.3 What Changed?\nOnce data example was cleaned, analyzed, and selected to be added to their respective csv file (modern or traditional) new criteria needed to be included.​\nAddition of each example also came with the addition of a relatable Bible verses to help the supervised model appeal to the Biblical nature of ConLingo. For the children-focused nature of the model,additional activities that relate to the relative examples and finally a prompt and answer to each example were added to give a good moral and academic-like focus to the training model.​\nThese additions to the cleaned data really aid in giving nuance and purpose to training the model.\n\nlibrary(tidyverse)\nlibrary(rstudioapi)\n\nscript_dir = dirname(rstudioapi::getActiveDocumentContext()$path)\nsetwd(script_dir)\n\ndata_path_m &lt;- \"Appendix/WilliamRichards/data/ConLingo_modern_500_v2.csv\"\ntb_modern = read.csv(data_path_m)\ndata_path_t &lt;- \"Appendix/WilliamRichards/data/ConLingo_traditional_500_v2.csv\"\n\ntb_traditional = read.csv(data_path_t)\n\nView(tb_modern)\nView(tb_traditional)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "AI4Bharat. (2024). Sangraha: Large-scale Indic language corpus. https://huggingface.co/datasets/ai4bharat/sangraha\nAI4Bharat. (2024). IndicBERT and IndicBART models. https://ai4bharat.iitm.ac.in/\nHoward Community College. (n.d.). The elements of culture. https://pressbooks.howardcc.edu/soci101/chapter/3-2-the-elements-of-culture/\nPew Research Center. (2021). Religion in India: Tolerance and segregation. https://www.pewresearch.org\nSahgal, N., & Evans, J. (2021). India Survey Dataset [Data set]. Pew Research Center. https://www.pewresearch.org/dataset/india-survey-dataset/\nWorld Values Survey. (2022). India dataset documentation. https://www.worldvaluessurvey.org/WVSDocumentationWV7.jsp\nCommon Crawl. (n.d.). Overview [Web site]. Retrieved November 30, 2025, from https://commoncrawl.org/overview\nNaamapadam Dataset. (2023). Named entity annotated dataset for Indic languages. AI4Bharat.\nIndicGLUE Benchmark. (2023). Benchmark tasks for Indic languages. AI4Bharat.\nGeeksforGeeks. (2025, October 8). What is Transfer Learning? GeeksforGeeks. https://www.geeksforgeeks.org/machine-learning/ml-introduction-to-transfer-learning/\nGeeksforGeeks. (2025, July 23). Supervised Fine-Tuning (SFT) for LLMs. GeeksforGeeks. https://www.geeksforgeeks.org/artificial-intelligence/supervised-fine-tuning-sft-for-llms/\nPadgaonkar, S., & Ghoshal, S. (2024, May 7). Finetuning in large language models. Oracle AI & Data Science Blog. https://blogs.oracle.com/ai-and-datascience/finetuning-in-large-language-models\nGeeksforGeeks. (2025, October 8). What is Transfer Learning? GeeksforGeeks. https://www.geeksforgeeks.org/machine-learning/ml-introduction-to-transfer-learning/",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "Week1.html#suwilanji-deliverables",
    "href": "Week1.html#suwilanji-deliverables",
    "title": "2  Week 1: Foundations & Data Collection",
    "section": "2.4 Suwilanji Deliverables",
    "text": "2.4 Suwilanji Deliverables\n\n2.4.1 Overview of the Week 1 Deliverable\nThe Week 1 deliverable lays the foundation for building a culturally grounded fine-tuning dataset for the ConLingo AI system, with a focus on data collection. Establishing a data collection strategy for the fine-tuning implementation.\n\n\n2.4.2 Week 1 Deliverables:\n\nIdentify example sources for data\nSet up data collection infrastructure\nCollect the initial 500 examples of data\nEthics and bias check - ensuring ethical data collection\n\n\n\n2.4.3 What Was Accomplished:\n\n2.4.3.1 1. Arriving at a Definition of Culture\nTo create a culturally diverse dataset representative of India, I first established a working definition of culture and aligned it with the dataset categories created based off of the culture definition for ConLingo.\n\nUsing the sociological framework from Pressbooks Howard Community College and supplemental examples, I identified the following components of culture:\n\nSymbols: gestures, icons, religious symbols, shared images.\n\nLanguage: idioms, semantics, dialects, scripts; linguistic relativity (Sapir–Whorf hypothesis).\n\nNorms: rules/expectations for behavior (e.g., respect, authority, hospitality).\n\nRituals: ceremonies, festivals, rites of passage.\n\nValues: moral beliefs regarding good/bad, desirable/undesirable.\n\nArtifacts: material objects—food, dress, architecture, arts.\n\nReligion: beliefs, doctrines, devotional practices, interfaith interactions.\n\nThese became the seven cultural buckets used for dataset classification purposes:\nSymbolic, Language, Norms, Rituals, Values, Artifacts, Religious. The goal was that the data sets we found must be able to fit into at least one of these 7 categories.\n\n\n2.4.3.2 2. Identify Examples from Sources\nI began by searching for an existing dataset or model that already contains high-quality data for the region we are targeting. A dataset that I discovered was AI4Bharat, and thus a major focus of Week 1 was exploring the AI4Bharat ecosystem (AI4Bharat, n.d.), as it contains the largest curated resources on Indian languages and cultural content.\nAI4Bharat is an India-based company with large partnerships with Google and Microsoft, aiming to develop AI models that reflect Indian culture and understanding. They have heavily invested in curating datasets that represent their culture.\nKey findings included:\n\n\n\n2.4.4 Core AI4Bharat Models (Khan et al., 2024)\n\nIndicBERT: A multilingual ALBERT-based model covering 12 major Indic languages; trained on ~9B tokens.\n\nIndicBART: A seq2seq model suitable for translation and generation tasks.\n\nIndicInstruct: Instruction-following datasets for English/Hindi, including WikiHow, FLAN v2, Dolly, and more.\n\nIndicXTREME: Benchmark suite for classification, QA, structure prediction, and retrieval.\n\nFBI Framework: Meta-evaluation system assessing evaluator robustness (Doddapaneni et al., 2024)\n\n\n\n2.4.5 Primary Dataset Sources (AI4Bharat)\n\nSangraha: 251B-token pretraining dataset across 22 Indic languages—largest cleaned Indic corpus.\n\nIndicGLUE: Benchmark for NLU across 11 Indic languages.\n\nNaamapadam: The largest named-entity-annotated dataset for Indic languages.\n\nIndicNLG Benchmark: Natural language generation tasks across Indic languages.\n\nMILU: Multi-domain Indic understanding benchmark.\n\nIndicNLP Corpora: 8.9B-token dataset; cultural topics across major languages.\n\nThese sources were great as they ranged from data sets that can be used to pre-train a model to evaluation frameworks and the like. Many of them were millions to billions of data points, which is great. They had many of the Indian languages covered, with manually collected and transcribed data.\nI moved into looking at additional data sources, besides the large AI4Bharat corpus.\n\n\n2.4.6 Additional Data Sources Consulted\nThese datasets supplement cultural categories not fully represented in AI4Bharat:\n\nReddit India Communities: r/india, r/AskAnIndian, r/hinduism, r/Christianity (via PRAW).\n\nWorld Values Survey (India subset): moral, political, and social values insights.\n\nPew India Survey: interfaith relations, religion, nationalism, caste, and social norms.\n\nCommon Crawl (India-filtered): festival mentions, cultural knowledge, news.\n\nReddit would be useful for finding out what topics are relevant today in the India region (r/India). The data would also be more conversational. However, after further exploration, using the API for Reddit would not be allowed, as they have a policy stating that their data cannot be used for training an AI model.\nThe World Values Survey is a survey that has collected data on the values, beliefs, and attitudes of people in various countries. The Wave 7 has an India survey that aims to see how these beliefs change over time, and the data is free to use and access. (World Values Survey Association, n.d.)\nThe Pew India Survey Data set is a large survey that included 29,999 Indian adults about “religious beliefs and practices, religious identity, nationalism, and tolerance in Indian society. The survey was administered face-to-face from Nov. 17, 2019, to March 23, 2020.” (Sahgal & Evans, 2021)\nCommon Crawl is a web scraping application that maintains a large corpus of petabytes of data, regularly collected, which contains web page data. The idea with the data set was to filter by region; however, that is not how data collection would work. Alternatively, you would need to be on a different AWS server to access region-specific data. It is a bit more complex and was ultimately ruled out. (Common Crawl, n.d.).\n\n\n2.4.7 3. Cultural Buckets & Dataset Categorization\nUsing the cultural definition and dataset sources described above, I asked AI to create a cultural bucket table, placing each dataset into the dominant cultural dimension(s) it represents. I also asked it to search for additional data sets that would be good additions to each bucket for further exploration.\n\n2.4.7.1 Cultural Category Table\n\n\n\n\n\n\n\n\n\nCategory \nDescription / Focus \nExisting Datasets \nGaps / Suggested Additions \n\n\nSymbolic \nMyths, icons, symbols, imagery in religion or national identity. \nSangraha, IndicNLP, Reddit (r/hinduism, r/India), Pew Survey (sections on national pride). \nAdd: Indian mythological texts (Mahabharata, Ramayana excerpts from Project Gutenberg); Emblem datasets from Indian Heritage Data Portal. \n\n\nLanguage \nLinguistic diversity, idioms, semantics, syntax. \nIndicBERT, IndicGLUE, MILU, Sangraha, IndicNLP Corpora, Naamapadam. \nAdd: Language–dialect corpora (e.g., Bodo, Manipuri from Bhashini initiative). \n\n\nNorms \nBehavioral expectations, etiquette, social order. \nReddit (r/AskAnIndian), Pew Survey, Common Crawl (filtered by “how to behave,” “should one…”). \nAdd: Indian Etiquette Corpora from newspapers or sociology textbooks; extract rule-like sentences from WikiHow-India subset. \n\n\nRituals \nReligious or cultural ceremonies, lifecycle events, festivals. \nCommon Crawl (with “Diwali,” “Puja,” etc.), Pew Survey, Sangraha texts, Reddit (r/hinduism). \nAdd: Digital Library of India (ritual manuals, cultural ethnographies). \n\n\nValues \nMoral or philosophical ideals (karma, tolerance, family, respect). \nPew Survey, World Values Survey India, Reddit (moral debates), MILU for culturally specific QA. \nAdd: OpenSubtitles India (for implicit moral contexts in dialogues). \n\n\nArtifacts \nTangible cultural products — art, clothing, architecture, tools. \nCommon Crawl (filtered “heritage,” “textiles,” “temple architecture”), Sangraha. \nAdd: Indian Heritage Portal or Archaeological Survey of India archives. \n\n\nReligious \nSpiritual texts, beliefs, rituals, interfaith dialogues. \nPew India Survey, Sangraha, IndicBERT, Reddit (r/Hinduism, r/Christianity), Common Crawl. \nAdd: Sacred Books of the East (digitized translations), Digital Library of India scripture scans. \n\n\n\nThese buckets were cross-referenced with the 5,000+ cultural examples targeted for the fine-tuning dataset.\n\n\n\n\n2.4.8 4. Fine-Tuning Process & How the Data Fits\n(High-level outline used for Week 1 documentation)\nAdditionally, to begin preparing for the fine-tuning process, I conducted preliminary research on the data standards and types required for fine-tuning. What type of data is best for he collected (and categorized) data feeds into the fine-tuning pipeline is as follows:\n\n\n\n\n\n\n\nStage \nTask - relevant to India datasets\n\n\n(1) Base Fine-Tuning \nGeneral Indian language understanding. \n\n\n(2) Instruction Fine-Tuning \nConversational, context-aware behavior. \n\n\n(3) Cultural Alignment \nTeach norms, values, and religious sensibility. \n\n\n(4) Ethical / Alignment Fine-Tuning \nEnsure responses are Helpful, Honest, Harmless (HHH). \n\n\n\n\n2.4.8.1 Step 1: Data Curation\n\nPull data from AI4Bharat, World Values Survey, Pew, and other data sources,\n\nOrganize by cultural category.\n\n\n\n2.4.8.2 Step 2: Preprocessing\n\nClean text: remove noise, normalize, and keep originality.\n\nConvert to instruction-response pairs for LLaMA-based fine-tuning. This would only be applicable if we do Supervised Learning.\n\nThis will be a major adjustment to the data as most data does not come in this format.\n\nTag metadata:\n\n\nregion\n\ntone\n\nlanguage\n\n\n\n\n2.4.8.3 Step 3: Dataset Construction\n\nBuild a 5,000+ example fine-tuning dataset.\n\nEnsure balance across categories (symbolic, norms, rituals, values, etc.).\n\nEnsure access across languages (Hindi, Tamil, Telugu, Malayalam, Bengali, etc.).\n\n\n\n2.4.8.4 Step 4: Fine-Tuning Setup\n\nSelect model: Possibly LLaMA-3 8B (best fit for Titan GPU).\n\n\n\n2.4.8.5 Step 5: Evaluation Plan Preview\n\nUse the Cultural Sensitivity Index (CSI) developed in Week 1.\n\nCompare:\n\nRAG baseline (Original ConLingo)\n\nBase LLaMA-3 (Example model)\n\nFine-tuned LLaMA-3 (Example model)\n\n\nConduct both automated testing and human evaluation.\n\nThe Week 1 work ensures that the training data is valid, representative, culturally sensitive, and ready for preprocessing in Week 2 and fine-tuning in Week 3.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: Foundations & Data Collection</span>"
    ]
  },
  {
    "objectID": "Week1.html#arriving-at-a-definition-of-culture",
    "href": "Week1.html#arriving-at-a-definition-of-culture",
    "title": "2  Week 1: Foundations & Data Collection",
    "section": "6.1 1. Arriving at a Definition of Culture",
    "text": "6.1 1. Arriving at a Definition of Culture\nTo create a culturally diverse dataset representative of India, I first established a working definition of culture and aligned it with the dataset categories created based off of the culture definition for ConLingo.\n\nUsing the sociological framework from Pressbooks Howard Community College and supplemental examples, I identified the following components of culture:\n\nSymbols: gestures, icons, religious symbols, shared images.\n\nLanguage: idioms, semantics, dialects, scripts; linguistic relativity (Sapir–Whorf hypothesis).\n\nNorms: rules/expectations for behavior (e.g., respect, authority, hospitality).\n\nRituals: ceremonies, festivals, rites of passage.\n\nValues: moral beliefs regarding good/bad, desirable/undesirable.\n\nArtifacts: material objects—food, dress, architecture, arts.\n\nReligion: beliefs, doctrines, devotional practices, interfaith interactions.\n\nThese became the seven cultural buckets used for dataset classification purposes:\nSymbolic, Language, Norms, Rituals, Values, Artifacts, Religious. The goal was that the data sets we found must be able to fit into at least one of these 7 categories.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: Foundations & Data Collection</span>"
    ]
  },
  {
    "objectID": "Week1.html#identify-examples-from-sources",
    "href": "Week1.html#identify-examples-from-sources",
    "title": "2  Week 1: Foundations & Data Collection",
    "section": "6.2 2. Identify Examples from Sources",
    "text": "6.2 2. Identify Examples from Sources\nI began by searching for an existing dataset or model that already contains high-quality data for the region we are targeting. A dataset that I discovered was AI4Bharat, and thus a major focus of Week 1 was exploring the AI4Bharat ecosystem (AI4Bharat, n.d.), as it contains the largest curated resources on Indian languages and cultural content.\nAI4Bharat is an India-based company with large partnerships with Google and Microsoft, aiming to develop AI models that reflect Indian culture and understanding. They have heavily invested in curating datasets that represent their culture.\nKey findings included:\n\n6.2.1 Core AI4Bharat Models (Khan et al., 2024)\n\nIndicBERT: A multilingual ALBERT-based model covering 12 major Indic languages; trained on ~9B tokens.\n\nIndicBART: A seq2seq model suitable for translation and generation tasks.\n\nIndicInstruct: Instruction-following datasets for English/Hindi, including WikiHow, FLAN v2, Dolly, and more.\n\nIndicXTREME: Benchmark suite for classification, QA, structure prediction, and retrieval.\n\nFBI Framework: Meta-evaluation system assessing evaluator robustness (Doddapaneni et al., 2024)\n\n\n\n6.2.2 Primary Dataset Sources (AI4Bharat)\n\nSangraha: 251B-token pretraining dataset across 22 Indic languages—largest cleaned Indic corpus.\n\nIndicGLUE: Benchmark for NLU across 11 Indic languages.\n\nNaamapadam: The largest named-entity-annotated dataset for Indic languages.\n\nIndicNLG Benchmark: Natural language generation tasks across Indic languages.\n\nMILU: Multi-domain Indic understanding benchmark.\n\nIndicNLP Corpora: 8.9B-token dataset; cultural topics across major languages.\n\nThese sources were great as they ranged from data sets that can be used to pre-train a model to evaluation frameworks and the like. Many of them were millions to billions of data points, which is great. They had many of the Indian languages covered, with manually collected and transcribed data.\nI moved into looking at additional data sources, besides the large AI4Bharat corpus.\n\n\n6.2.3 Additional Data Sources Consulted\nThese datasets supplement cultural categories not fully represented in AI4Bharat:\n\nReddit India Communities: r/india, r/AskAnIndian, r/hinduism, r/Christianity (via PRAW).\n\nWorld Values Survey (India subset): moral, political, and social values insights.\n\nPew India Survey: interfaith relations, religion, nationalism, caste, and social norms.\n\nCommon Crawl (India-filtered): festival mentions, cultural knowledge, news.\n\nReddit would be useful for finding out what topics are relevant today in the India region (r/India). The data would also be more conversational. However, after further exploration, using the API for Reddit would not be allowed, as they have a policy stating that their data cannot be used for training an AI model.\nThe World Values Survey is a survey that has collected data on the values, beliefs, and attitudes of people in various countries. The Wave 7 has an India survey that aims to see how these beliefs change over time, and the data is free to use and access. (World Values Survey Association, n.d.)\nThe Pew India Survey Data set is a large survey that included 29,999 Indian adults about “religious beliefs and practices, religious identity, nationalism, and tolerance in Indian society. The survey was administered face-to-face from Nov. 17, 2019, to March 23, 2020.” (Sahgal & Evans, 2021)\nCommon Crawl is a web scraping application that maintains a large corpus of petabytes of data, regularly collected, which contains web page data. The idea with the data set was to filter by region; however, that is not how data collection would work. Alternatively, you would need to be on a different AWS server to access region-specific data. It is a bit more complex and was ultimately ruled out. (Common Crawl, n.d.).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: Foundations & Data Collection</span>"
    ]
  },
  {
    "objectID": "Week1.html#cultural-buckets-dataset-categorization",
    "href": "Week1.html#cultural-buckets-dataset-categorization",
    "title": "2  Week 1: Foundations & Data Collection",
    "section": "6.3 3. Cultural Buckets & Dataset Categorization",
    "text": "6.3 3. Cultural Buckets & Dataset Categorization\nUsing the cultural definition and dataset sources described above, I asked AI to create a cultural bucket table, placing each dataset into the dominant cultural dimension(s) it represents. I also asked it to search for additional data sets that would be good additions to each bucket for further exploration.\n\n6.3.1 Cultural Category Table\n\n\n\n\n\n\n\n\n\nCategory \nDescription / Focus \nExisting Datasets \nGaps / Suggested Additions \n\n\nSymbolic \nMyths, icons, symbols, imagery in religion or national identity. \nSangraha, IndicNLP, Reddit (r/hinduism, r/India), Pew Survey (sections on national pride). \nAdd: Indian mythological texts (Mahabharata, Ramayana excerpts from Project Gutenberg); Emblem datasets from Indian Heritage Data Portal. \n\n\nLanguage \nLinguistic diversity, idioms, semantics, syntax. \nIndicBERT, IndicGLUE, MILU, Sangraha, IndicNLP Corpora, Naamapadam. \nAdd: Language–dialect corpora (e.g., Bodo, Manipuri from Bhashini initiative). \n\n\nNorms \nBehavioral expectations, etiquette, social order. \nReddit (r/AskAnIndian), Pew Survey, Common Crawl (filtered by “how to behave,” “should one…”). \nAdd: Indian Etiquette Corpora from newspapers or sociology textbooks; extract rule-like sentences from WikiHow-India subset. \n\n\nRituals \nReligious or cultural ceremonies, lifecycle events, festivals. \nCommon Crawl (with “Diwali,” “Puja,” etc.), Pew Survey, Sangraha texts, Reddit (r/hinduism). \nAdd: Digital Library of India (ritual manuals, cultural ethnographies). \n\n\nValues \nMoral or philosophical ideals (karma, tolerance, family, respect). \nPew Survey, World Values Survey India, Reddit (moral debates), MILU for culturally specific QA. \nAdd: OpenSubtitles India (for implicit moral contexts in dialogues). \n\n\nArtifacts \nTangible cultural products — art, clothing, architecture, tools. \nCommon Crawl (filtered “heritage,” “textiles,” “temple architecture”), Sangraha. \nAdd: Indian Heritage Portal or Archaeological Survey of India archives. \n\n\nReligious \nSpiritual texts, beliefs, rituals, interfaith dialogues. \nPew India Survey, Sangraha, IndicBERT, Reddit (r/Hinduism, r/Christianity), Common Crawl. \nAdd: Sacred Books of the East (digitized translations), Digital Library of India scripture scans. \n\n\n\nThese buckets were cross-referenced with the 5,000+ cultural examples targeted for the fine-tuning dataset.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: Foundations & Data Collection</span>"
    ]
  },
  {
    "objectID": "Week1.html#fine-tuning-process-how-the-data-fits",
    "href": "Week1.html#fine-tuning-process-how-the-data-fits",
    "title": "2  Week 1: Foundations & Data Collection",
    "section": "6.4 4. Fine-Tuning Process & How the Data Fits",
    "text": "6.4 4. Fine-Tuning Process & How the Data Fits\n(High-level outline used for Week 1 documentation)\nAdditionally, to begin preparing for the fine-tuning process, I conducted preliminary research on the data standards and types required for fine-tuning. What type of data is best for he collected (and categorized) data feeds into the fine-tuning pipeline is as follows:\n\n\n\n\n\n\n\nStage \nTask - relevant to India datasets\n\n\n(1) Base Fine-Tuning \nGeneral Indian language understanding. \n\n\n(2) Instruction Fine-Tuning \nConversational, context-aware behavior. \n\n\n(3) Cultural Alignment \nTeach norms, values, and religious sensibility. \n\n\n(4) Ethical / Alignment Fine-Tuning \nEnsure responses are Helpful, Honest, Harmless (HHH). \n\n\n\n\n6.4.1 Step 1: Data Curation\n\nPull data from AI4Bharat, World Values Survey, Pew, and other data sources,\n\nOrganize by cultural category.\n\n\n\n6.4.2 Step 2: Preprocessing\n\nClean text: remove noise, normalize, and keep originality.\n\nConvert to instruction-response pairs for LLaMA-based fine-tuning. This would only be applicable if we do Supervised Learning.\n\nThis will be a major adjustment to the data as most data does not come in this format.\n\nTag metadata:\n\nregion\ntone\nlanguage\n\n\n\n\n6.4.3 Step 3: Dataset Construction\n\nBuild a 5,000+ example fine-tuning dataset.\n\nEnsure balance across categories (symbolic, norms, rituals, values, etc.).\n\nEnsure access across languages (Hindi, Tamil, Telugu, Malayalam, Bengali, etc.).\n\n\n\n6.4.4 Step 4: Fine-Tuning Setup\n\nSelect model: Possibly LLaMA-3 8B (best fit for Titan GPU).\n\n\n\n6.4.5 Step 5: Evaluation Plan Preview\n\nUse the Cultural Sensitivity Index (CSI) developed in Week 1.\n\nCompare:\n\nRAG baseline (Original ConLingo)\n\nBase LLaMA-3 (Example model)\n\nFine-tuned LLaMA-3 (Example model)\n\n\nConduct both automated testing and human evaluation.\n\nThe Week 1 work ensures that the training data is valid, representative, culturally sensitive, and ready for preprocessing in Week 2 and fine-tuning in Week 3.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: Foundations & Data Collection</span>"
    ]
  },
  {
    "objectID": "Week2.html#suwilanji-deliverables",
    "href": "Week2.html#suwilanji-deliverables",
    "title": "3  Week 2: Data Cleaning & Pipeline Setup",
    "section": "3.4 Suwilanji Deliverables",
    "text": "3.4 Suwilanji Deliverables\n\n3.4.1 Overview of the Week 2 Deliverable\nWeek 2’s Deliverable was to solidify the training datasets for the fine-tuning process and then begin the cleaning process. In addition, ensuring that any unethical data was removed.\n\n\n3.4.2 Week 2 Deliverables:\n\n2,500 raw examples collected (cumulative with Week 1: 3,000 total)\nInitial cleaning applied\nSource distribution documented\nBias check on collected data\n\n\n\n3.4.3 What was Accomplished:\n\n3.4.3.1 1. Reaching out to the Reddit API\nThis week, I focused on identifying quality data and what it entails. I reached out to Reddit to obtain its API key for more relevant data, as I await its response to see if it would still be possible to use its data. (Filled in a form).\n\n\n3.4.3.2 2. Finalize Data Sources\nI collected data from these 7 sources, all of which perform a specific role or fit into the buckets from last week. These data sets were collected primarily from Kaggle and the rest HuggingFace, or webpages.\n\nIndicQuestionGeneration (hindu) (Source: AI4Bharat HuggingFace)\nRegional Indian Superstitions & Beliefs (Source: Kaggle)\nWorld Values Survey India (Source: WVS website)\nTop Spotify Podcast Episodes (Updated Daily) India (Source: Kaggle)\nPew India Survey Dataset (Source: Pew Research Center)\nHinglish TOP Dataset (Source: Kaggle)\nIndia News Headlines Dataset (Source: Kaggle)\n\n\nThe IndicQuestionGeneration dataset has 1,078,297 rows. It is part of the IndicNLG Benchmark that consists of one of AI4Bharat’s models. It is a large multilingual collection designed for Natural Language Generation (NLG) tasks and is in 11 Indic languages. It is formatted in Question Generation format, which means that when given a context and an answer, it aims to generate a question that leads to that answer. This was derived from the Stanford Question Answering Dataset (SQuAD). There are ~98,000 examples per language, totaling around 1.08 million examples across all 11 languages. This dataset would be useful for training and evaluating models that generate comprehension questions in Indian languages. This dataset would be easy to use because it is in a question-answer pair format.\nThe Regional Indian Superstitions & Beliefs dataset captures regional superstitions and beliefs from all 28 states and 8 union territories of India, showcasing the fascinating and often lesser-known cultural fabric that shapes daily life across the country. It is split into 500+ training entries and 100+ testing entries, each representing a specific superstition or folk belief. It is a data set for developing AI assistants that understand regional nuances.\nThe World Values Survey – India, Wave 7 (2017–2022), is a nationally representative dataset that captures the evolving social, political, economic, and cultural values of Indian society. Conducted through face-to-face interviews across multiple regions and languages, it surveyed approximately 2,400 respondents aged 18 and above. The dataset comprises nearly 400 variables, encompassing priorities in life (family, work, religion, and politics), child-rearing values, trust in institutions, gender roles, democracy, corruption, globalization, and religious beliefs.\nThe Top Spotify Podcast Episodes (Updated Daily) India contains a daily snapshot of Spotify’s top 200 podcast episodes for every country. It also includes detailed information about podcast episodes and shows from the Spotify API. Daily data collection began on 2025-10-11; additionally, some data is available from 2024-09-02 to 2024-10-23. This dataset will help take a different angle on culture by comparingpodcast popularity to understand cultural influences on podcast consumption. India is one of the regions.\nThe Pew India Survey Dataset is the raw data from a large-scale, face-to-face survey of 29,999 Indian adults (ages 18 and older) conducted by the Pew Research Center across 26 states and three union territories between November 17, 2019, and March 23, 2020. Conducted in 17 languages using computer-assisted personal interviews (CAPI), the sample was designed with an oversampling strategy to capture a broad view of religious demographics, including interviews with 22,975 Hindus, 3,336 Muslims, 1,782 Sikhs, 1,011 Christians, 719 Buddhists, and 109 Jains. This dataset provides researchers with detailed information on a wide range of social attitudes and beliefs in India, specifically covering topics such as religious identity, beliefs, practices, nationalism, and tolerance in Indian society.\nThe Hinglish TOP Dataset comprises a large (10K) human-annotated code-switched semantic parsing dataset, which contains 10,000 real examples of people speaking in Hinglish commands. Humans have manually labeled or “tagged” the computer-readable meaning for each one. Additionally, the 170K generated utterances using the CST5 augmentation technique introduced in the paper were utilized, which significantly expanded the dataset. To achieve this, researchers employed a special trick called CST5 to create 170,000 new, synthetic (computer-generated) Hinglish commands based on the existing data. Queries are derived from TOPv2, a multi-domain task-oriented semantic parsing dataset.\nThe India News Headlines Dataset is a comprehensive historical archive of notable events in the Indian subcontinent from 2001 to Q2 2023, recorded in real-time by journalists in India. It contains approximately 3.8 million events published by The Times of India. There are 3,876,557 rows in the CSV. Due to the heavy daily volume (avg. 600 articles) over multiple years, this data offers a deep insight into Indian society, its priorities, events, issues, and talking points, and how they have unfolded over time.\n\nThese seven sources will be used to train our data. However, because these sets combined are millions of data points, we will only use 6,000 of them to start. I will also be manually validating them as much as I can for any toxicity.\nIn this instance, toxicity is defined as data that may be considered harmful or detrimental. Sexual, indecent behaviour, curse words, and the like.\nAn Excel sheet with the following categories ofdata was collected and shared with the team.\n\n\n3.4.3.3 3. Distribution of Data\nAfter deciding on these datasets, due to the large amount of data in each, I had to narrow down the dataset sizes to ensure we have 6,000 high-quality examples to train our model. We can gather these examples by numbering each data point and doing a random sample, with the no. of examples as the goal.\nThe breakdown of the datasets and their approximate number of examples are:\n\n\n\nSubset\nNo. of Examples\n\n\n\n\nIndicQuestionGeneration\n1000\n\n\nRegional Indian Superstitions & Beliefs\n660\n\n\nWorld Values Survey India\n1000\n\n\nTop Spotify Podcast Episodes (Updated Daily) India\n500\n\n\nPew India Survey Dataset\n1000\n\n\nHinglish TOP Dataset\n1000\n\n\nIndia News Headlines Dataset\n840\n\n\nTotal:\n6,000\n\n\n\nData Pie\nThis shows the distribution of our data set piece:\n\n\n\nPie chart showing the distribution of data sets.\n\n\nThe bottleneck in this process is that the examples must be handpicked.\n\n\n3.4.3.4 4. Cleaning Script\nThe initial cleaning script will be better created. I integrated Claude into its development, so I can understand what automating that part looks like and the different functions. I will then create my own based on the final recommendation from Moses on how he wants the data to be structured.\n\n# Fine-Tuning Pipeline for ConLingo 2.0\n# 10/30/25\n# ETL Pipeline\n\n# Datasets included:\n# - India News Headlines Datasets 2001-01-01 ; End Date: 2023-06-30 (for example)\n\n\"\"\"\nData Cleaning & Preprocessing Pipeline for Fine-tuning\n\nSteps:\n1. Load datasets\n2. Clean text (remove URLs, excessive punctuation, spam)\n3. Deduplicate (cosine similarity &gt; 0.9)\n4. Flag toxic content (Detoxify)\n\"\"\"\n\nimport os\nimport re\nimport string\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom detoxify import Detoxify\nfrom tqdm import tqdm\n\n# ---------------------------\n# CONFIGURATION\n# ---------------------------\nDATASETS = {\n    \"IndicQuestionGeneration_hi\": \"path/to/IndicQuestionGeneration.csv\",\n    \"RegionalIndianSuperstitions\": \"path/to/RegionalIndianSuperstitions.csv\",\n    \"WorldValuesSurveyIndia\": \"path/to/WorldValuesSurveyIndia.csv\",\n    \"TopSpotifyPodcastEpisodes\": \"path/to/TopSpotifyPodcastEpisodes.csv\",\n    \"IndiaPewSurvey\": \"path/to/IndiaPewSurvey.csv\",\n    \"HinglishTOP\": \"path/to/HinglishTOP.csv\",\n    \"IndiaNewsHeadlines\": \"path/to/IndiaNewsHeadlines.csv\"\n}\n\nTEXT_COLUMN = \"text\"   # Change this if your datasets have a different column name\n\n# ---------------------------\n# TEXT CLEANING FUNCTIONS\n# ---------------------------\ndef clean_text(text: str) -&gt; str:\n    \"\"\"Remove URLs, excessive punctuation, and obvious spam.\"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # remove URLs\n    text = re.sub(r\"\\s+\", \" \", text)  # collapse whitespace\n    text = re.sub(rf\"[{re.escape(string.punctuation)}]\", \" \", text)  # remove punctuation\n    text = re.sub(r\"\\b(buy now|click here|free money|subscribe)\\b\", \"\", text, flags=re.I)\n    return text.strip()\n\n# ---------------------------\n# DEDUPLICATION\n# ---------------------------\ndef deduplicate_texts(df: pd.DataFrame, column: str, threshold=0.9) -&gt; pd.DataFrame:\n    \"\"\"Remove near-duplicates using cosine similarity.\"\"\"\n    texts = df[column].fillna(\"\").tolist()\n    vectorizer = TfidfVectorizer().fit_transform(texts)\n    similarity = cosine_similarity(vectorizer)\n    to_drop = set()\n    for i in range(len(texts)):\n        for j in range(i + 1, len(texts)):\n            if similarity[i, j] &gt; threshold:\n                to_drop.add(j)\n    return df.drop(df.index[list(to_drop)])\n\n# ---------------------------\n# TOXICITY FLAGGING\n# ---------------------------\ndef flag_toxicity(df: pd.DataFrame, column: str) -&gt; pd.DataFrame:\n    \"\"\"Add a column with toxicity scores using Detoxify.\"\"\"\n    model = Detoxify('original')\n    toxicity_scores = []\n    for text in tqdm(df[column], desc=\"Toxicity Scoring\"):\n        score = model.predict(str(text))\n        toxicity_scores.append(score[\"toxicity\"])\n    df[\"toxicity_score\"] = toxicity_scores\n    df[\"is_toxic_flag\"] = df[\"toxicity_score\"] &gt; 0.5  # manual review threshold\n    return df\n\n# ---------------------------\n# PIPELINE EXECUTION\n# ---------------------------\ndef process_dataset(name, path):\n    print(f\"\\nProcessing {name}...\")\n    df = pd.read_csv(path)\n    if TEXT_COLUMN not in df.columns:\n        raise ValueError(f\"Column '{TEXT_COLUMN}' not found in {path}\")\n\n    # Step 1: Clean\n    df[TEXT_COLUMN] = df[TEXT_COLUMN].apply(clean_text)\n\n    # Step 2: Deduplicate\n    df = deduplicate_texts(df, TEXT_COLUMN)\n\n    # Step 3: Flag toxicity\n    df = flag_toxicity(df, TEXT_COLUMN)\n\n    # Save processed version\n    out_path = f\"cleaned_{name}.csv\"\n    df.to_csv(out_path, index=False)\n    print(f\"Saved cleaned dataset to {out_path}\")\n\n# ---------------------------\n# MAIN SCRIPT\n# ---------------------------\nif __name__ == \"__main__\":\n    for name, path in DATASETS.items():\n        if os.path.exists(path):\n            process_dataset(name, path)\n        else:\n            print(f\" Skipping {name}: file not found at {path}\")\n\n\nThis script will aid in: Removing URLs, excessive punctuation, and obvious spam. Deduplicate or Near-duplicate detection (cosine similarity &gt; 0.9) and lastly flag any toxic content using Detoxify classifier (which will require additional manual review).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2: Data Cleaning & Pipeline Setup</span>"
    ]
  },
  {
    "objectID": "Week2.html#reaching-out-to-the-reddit-api",
    "href": "Week2.html#reaching-out-to-the-reddit-api",
    "title": "3  Week 2: Data Cleaning & Pipeline Setup",
    "section": "6.1 1. Reaching out to the Reddit API",
    "text": "6.1 1. Reaching out to the Reddit API\nThis week, I focused on identifying quality data and what it entails. I reached out to Reddit to obtain its API key for more relevant data, as I await its response to see if it would still be possible to use its data. (Filled in a form).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2: Data Cleaning & Pipeline Setup</span>"
    ]
  },
  {
    "objectID": "Week2.html#finalize-data-sources",
    "href": "Week2.html#finalize-data-sources",
    "title": "3  Week 2: Data Cleaning & Pipeline Setup",
    "section": "6.2 2. Finalize Data Sources",
    "text": "6.2 2. Finalize Data Sources\nI collected data from these 7 sources, all of which perform a specific role or fit into the buckets from last week. These data sets were collected primarily from Kaggle and the rest HuggingFace, or webpages.\n\nIndicQuestionGeneration (hindu) (Source: AI4Bharat HuggingFace)\nRegional Indian Superstitions & Beliefs (Source: Kaggle)\nWorld Values Survey India (Source: WVS website)\nTop Spotify Podcast Episodes (Updated Daily) India (Source: Kaggle)\nPew India Survey Dataset (Source: Pew Research Center)\nHinglish TOP Dataset (Source: Kaggle)\nIndia News Headlines Dataset (Source: Kaggle)\n\n\nThe IndicQuestionGeneration dataset has 1,078,297 rows. It is part of the IndicNLG Benchmark that consists of one of AI4Bharat’s models. It is a large multilingual collection designed for Natural Language Generation (NLG) tasks and is in 11 Indic languages. It is formatted in Question Generation format, which means that when given a context and an answer, it aims to generate a question that leads to that answer. This was derived from the Stanford Question Answering Dataset (SQuAD). There are ~98,000 examples per language, totaling around 1.08 million examples across all 11 languages. This dataset would be useful for training and evaluating models that generate comprehension questions in Indian languages. This dataset would be easy to use because it is in a question-answer pair format.\nThe Regional Indian Superstitions & Beliefs dataset captures regional superstitions and beliefs from all 28 states and 8 union territories of India, showcasing the fascinating and often lesser-known cultural fabric that shapes daily life across the country. It is split into 500+ training entries and 100+ testing entries, each representing a specific superstition or folk belief. It is a data set for developing AI assistants that understand regional nuances.\nThe World Values Survey – India, Wave 7 (2017–2022), is a nationally representative dataset that captures the evolving social, political, economic, and cultural values of Indian society. Conducted through face-to-face interviews across multiple regions and languages, it surveyed approximately 2,400 respondents aged 18 and above. The dataset comprises nearly 400 variables, encompassing priorities in life (family, work, religion, and politics), child-rearing values, trust in institutions, gender roles, democracy, corruption, globalization, and religious beliefs.\nThe Top Spotify Podcast Episodes (Updated Daily) India contains a daily snapshot of Spotify’s top 200 podcast episodes for every country. It also includes detailed information about podcast episodes and shows from the Spotify API. Daily data collection began on 2025-10-11; additionally, some data is available from 2024-09-02 to 2024-10-23. This dataset will help take a different angle on culture by comparingpodcast popularity to understand cultural influences on podcast consumption. India is one of the regions.\nThe Pew India Survey Dataset is the raw data from a large-scale, face-to-face survey of 29,999 Indian adults (ages 18 and older) conducted by the Pew Research Center across 26 states and three union territories between November 17, 2019, and March 23, 2020. Conducted in 17 languages using computer-assisted personal interviews (CAPI), the sample was designed with an oversampling strategy to capture a broad view of religious demographics, including interviews with 22,975 Hindus, 3,336 Muslims, 1,782 Sikhs, 1,011 Christians, 719 Buddhists, and 109 Jains. This dataset provides researchers with detailed information on a wide range of social attitudes and beliefs in India, specifically covering topics such as religious identity, beliefs, practices, nationalism, and tolerance in Indian society.\nThe Hinglish TOP Dataset comprises a large (10K) human-annotated code-switched semantic parsing dataset, which contains 10,000 real examples of people speaking in Hinglish commands. Humans have manually labeled or “tagged” the computer-readable meaning for each one. Additionally, the 170K generated utterances using the CST5 augmentation technique introduced in the paper were utilized, which significantly expanded the dataset. To achieve this, researchers employed a special trick called CST5 to create 170,000 new, synthetic (computer-generated) Hinglish commands based on the existing data. Queries are derived from TOPv2, a multi-domain task-oriented semantic parsing dataset.\nThe India News Headlines Dataset is a comprehensive historical archive of notable events in the Indian subcontinent from 2001 to Q2 2023, recorded in real-time by journalists in India. It contains approximately 3.8 million events published by The Times of India. There are 3,876,557 rows in the CSV. Due to the heavy daily volume (avg. 600 articles) over multiple years, this data offers a deep insight into Indian society, its priorities, events, issues, and talking points, and how they have unfolded over time.\n\nThese seven sources will be used to train our data. However, because these sets combined are millions of data points, we will only use 6,000 of them to start. I will also be manually validating them as much as I can for any toxicity.\nIn this instance, toxicity is defined as data that may be considered harmful or detrimental. Sexual, indecent behaviour, curse words, and the like.\nAn Excel sheet with the following categories ofdata was collected and shared with the team.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2: Data Cleaning & Pipeline Setup</span>"
    ]
  },
  {
    "objectID": "Week2.html#distribution-of-data",
    "href": "Week2.html#distribution-of-data",
    "title": "3  Week 2: Data Cleaning & Pipeline Setup",
    "section": "6.3 3. Distribution of Data",
    "text": "6.3 3. Distribution of Data\nAfter deciding on these datasets, due to the large amount of data in each, I had to narrow down the dataset sizes to ensure we have 6,000 high-quality examples to train our model. We can gather these examples by numbering each data point and doing a random sample, with the no. of examples as the goal.\nThe breakdown of the datasets and their approximate number of examples are:\n\n\n\nSubset\nNo. of Examples\n\n\n\n\nIndicQuestionGeneration\n1000\n\n\nRegional Indian Superstitions & Beliefs\n660\n\n\nWorld Values Survey India\n1000\n\n\nTop Spotify Podcast Episodes (Updated Daily) India\n500\n\n\nPew India Survey Dataset\n1000\n\n\nHinglish TOP Dataset\n1000\n\n\nIndia News Headlines Dataset\n840\n\n\nTotal:\n6,000\n\n\n\nData Pie\nThis shows the distribution of our data set piece:\n\n\n\nPie chart showing the distribution of data sets.\n\n\nThe bottleneck in this process is that the examples must be handpicked.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2: Data Cleaning & Pipeline Setup</span>"
    ]
  },
  {
    "objectID": "Week2.html#cleaning-script",
    "href": "Week2.html#cleaning-script",
    "title": "3  Week 2: Data Cleaning & Pipeline Setup",
    "section": "3.5 4. Cleaning Script",
    "text": "3.5 4. Cleaning Script\nThe initial cleaning script will be better created. I integrated Claude into its development, so I can understand what automating that part looks like and the different functions. I will then create my own based on the final recommendation from Moses on how he wants the data to be structured.\n\n# Fine-Tuning Pipeline for ConLingo 2.0\n# 10/30/25\n# ETL Pipeline\n\n# Datasets included:\n# - India News Headlines Datasets 2001-01-01 ; End Date: 2023-06-30 (for example)\n\n\"\"\"\nData Cleaning & Preprocessing Pipeline for Fine-tuning\n\nSteps:\n1. Load datasets\n2. Clean text (remove URLs, excessive punctuation, spam)\n3. Deduplicate (cosine similarity &gt; 0.9)\n4. Flag toxic content (Detoxify)\n\"\"\"\n\nimport os\nimport re\nimport string\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom detoxify import Detoxify\nfrom tqdm import tqdm\n\n# ---------------------------\n# CONFIGURATION\n# ---------------------------\nDATASETS = {\n    \"IndicQuestionGeneration_hi\": \"path/to/IndicQuestionGeneration.csv\",\n    \"RegionalIndianSuperstitions\": \"path/to/RegionalIndianSuperstitions.csv\",\n    \"WorldValuesSurveyIndia\": \"path/to/WorldValuesSurveyIndia.csv\",\n    \"TopSpotifyPodcastEpisodes\": \"path/to/TopSpotifyPodcastEpisodes.csv\",\n    \"IndiaPewSurvey\": \"path/to/IndiaPewSurvey.csv\",\n    \"HinglishTOP\": \"path/to/HinglishTOP.csv\",\n    \"IndiaNewsHeadlines\": \"path/to/IndiaNewsHeadlines.csv\"\n}\n\nTEXT_COLUMN = \"text\"   # Change this if your datasets have a different column name\n\n# ---------------------------\n# TEXT CLEANING FUNCTIONS\n# ---------------------------\ndef clean_text(text: str) -&gt; str:\n    \"\"\"Remove URLs, excessive punctuation, and obvious spam.\"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # remove URLs\n    text = re.sub(r\"\\s+\", \" \", text)  # collapse whitespace\n    text = re.sub(rf\"[{re.escape(string.punctuation)}]\", \" \", text)  # remove punctuation\n    text = re.sub(r\"\\b(buy now|click here|free money|subscribe)\\b\", \"\", text, flags=re.I)\n    return text.strip()\n\n# ---------------------------\n# DEDUPLICATION\n# ---------------------------\ndef deduplicate_texts(df: pd.DataFrame, column: str, threshold=0.9) -&gt; pd.DataFrame:\n    \"\"\"Remove near-duplicates using cosine similarity.\"\"\"\n    texts = df[column].fillna(\"\").tolist()\n    vectorizer = TfidfVectorizer().fit_transform(texts)\n    similarity = cosine_similarity(vectorizer)\n    to_drop = set()\n    for i in range(len(texts)):\n        for j in range(i + 1, len(texts)):\n            if similarity[i, j] &gt; threshold:\n                to_drop.add(j)\n    return df.drop(df.index[list(to_drop)])\n\n# ---------------------------\n# TOXICITY FLAGGING\n# ---------------------------\ndef flag_toxicity(df: pd.DataFrame, column: str) -&gt; pd.DataFrame:\n    \"\"\"Add a column with toxicity scores using Detoxify.\"\"\"\n    model = Detoxify('original')\n    toxicity_scores = []\n    for text in tqdm(df[column], desc=\"Toxicity Scoring\"):\n        score = model.predict(str(text))\n        toxicity_scores.append(score[\"toxicity\"])\n    df[\"toxicity_score\"] = toxicity_scores\n    df[\"is_toxic_flag\"] = df[\"toxicity_score\"] &gt; 0.5  # manual review threshold\n    return df\n\n# ---------------------------\n# PIPELINE EXECUTION\n# ---------------------------\ndef process_dataset(name, path):\n    print(f\"\\nProcessing {name}...\")\n    df = pd.read_csv(path)\n    if TEXT_COLUMN not in df.columns:\n        raise ValueError(f\"Column '{TEXT_COLUMN}' not found in {path}\")\n\n    # Step 1: Clean\n    df[TEXT_COLUMN] = df[TEXT_COLUMN].apply(clean_text)\n\n    # Step 2: Deduplicate\n    df = deduplicate_texts(df, TEXT_COLUMN)\n\n    # Step 3: Flag toxicity\n    df = flag_toxicity(df, TEXT_COLUMN)\n\n    # Save processed version\n    out_path = f\"cleaned_{name}.csv\"\n    df.to_csv(out_path, index=False)\n    print(f\"Saved cleaned dataset to {out_path}\")\n\n# ---------------------------\n# MAIN SCRIPT\n# ---------------------------\nif __name__ == \"__main__\":\n    for name, path in DATASETS.items():\n        if os.path.exists(path):\n            process_dataset(name, path)\n        else:\n            print(f\" Skipping {name}: file not found at {path}\")\n\n\nThis script will aid in: Removing URLs, excessive punctuation, and obvious spam. Deduplicate or Near-duplicate detection (cosine similarity &gt; 0.9) and lastly flag any toxic content using Detoxify classifier (which will require additional manual review).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2: Data Cleaning & Pipeline Setup</span>"
    ]
  },
  {
    "objectID": "Week3.html#suwilanji-deliverables",
    "href": "Week3.html#suwilanji-deliverables",
    "title": "4  Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning",
    "section": "4.4 Suwilanji Deliverables",
    "text": "4.4 Suwilanji Deliverables\n\n4.4.1 Overview of the Week 3 Deliverable\nThis week, we had a pivot in deliverables. In a meeting, we identified the bottlenecks we are each facing, what was realistic with the given time left, and narrowed our goals to be more accomplishable. In addition, my deliverable was to format the paper for the arXiv.\n\n4.4.1.1 Week 3 Deliverables:\n\nRestructure deliverables moving forward.\n\nTeam meetings\nProvide better clarity on the original ConLingo.\n\nOutline research paper according to arXiv guidelines\n\n\n\n4.4.1.2 What Was Accomplished:\n\n\n\n4.4.2 1. Deliverable Pivot\nDue to this restructuring, I had to pause my data collection strategy process, as the decisions we made would impact whether my data would be helpful or not.\nMeeting Agenda:\n\nWe defined what the existing ConLingo model is:\n\nAn AI assistant that helps you bridge the cultural gap between civilizations (starting with India).\n\nRevamped our deliverables due to two significant reasons:\n\n(1) Initial deliverables were overambitious as the reality of our time was realized.\n(2) We felt that because of this, we did not have SMART goals, emphasis on the A for Acheivable, and thus we redesigned the deliverables to be SMART goals for weeks 3-4.\n\nThe seven categories of culture that I created were updated and further categorized into Non-Material and Material.\n\nNon-Material Culture:​\n\nValues & Beliefs: what is considered good, desirable, and accurate.​\nNorms & Customs: routine practices, rituals, eg, Greetings, holiday celebrations.​\nLanguage: A system of words, symbols, and non-verbal cues.​\nReligion & Spirituality: morality and the supernatural​.\nArts and Literature: Music, dance, visual arts, stories, poetry, etc.​\nSocial Organization: family units (nuclear vs extended), social classes, and hierarchies based on wealth, age, and occupation​.\n\nMaterial Culture:​\n\nArtifacts & Technology: Eating utensils, transportation methods, and housing style​.\nGovernment & Economic Systems: The structure  used to provide for everyday needs, distribute power, and manage resources​.\n\n\nLastly, we had to make a final decision on whether to use Supervised Fine-Tuning (SFT) or Unsupervised Fine-Tuning (UFT) for our fine-tuning process. This decision is key because it affects how the data is formatted. If it is SFT that has to be labeled, the question-answer (QA) pairs are a must. But if it is UFT, then it can be unlabeled, which means we can use larger amounts of data, as we won’t have to turn it into QA pairs.\n\nWe had the following constraints to consider:\n\nHardware: NVIDIA A30 GPU (24GB VRAM) on Titan​\nTimeline: 3 Weeks​ (less than)\nTeam size: 4 people​\nGoal: Beat the RAG implementation in having Indian culturally nuanced responses\n\nSupervised Fine-Tuning (SFT) is a process of taking a pre-trained language model and further training it on a smaller, task-specific dataset with labeled examples. Its goal is to adjust the weights of the pre-trained model so that it performs better on our specific task without losing its general knowledge acquired during pre-training. (GeeksforGeeks, 2025)\nUnsupervised finetuning: This method doesn’t require labeled data. Instead, the LLM is exposed to a large corpus of unlabeled text from the target domain. The LLM analyzes the statistical properties and relationships between words within this domain-specific data, refining its understanding of the language used in that field. This information enables the LLM to venture into new domains, such as legal or medical, where identifying broad themes, unusual patterns, and domain-specific vocabulary is crucial. Unsupervised finetuning is commonly used for tasks such as language modeling, where the model learns to predict the next word in a sequence based on its context. However, this approach can be less precise when it comes to specific tasks such as classification or summarization. (Padgaonkar & Ghoshal, 2024)\nDue to time, team size, and computer resources constraints the only realistic option would be to use Supervised Fine Tuning where the primary focus would be to get excellent high quality Q & A pairs somewhere around 1000-1500 of them and do some fine tuning iterations on Titan and since training is in the form factor of hours in terms of the time taken, if the model is not performing as well it is easy to tweak parameters easily. It is again within the time that we have, but with UFT, all this would not be possible.\n\nBased on the datasets I collected, this was selected and then transformed by an AI-powered data cleaning pipeline created by Moses. This script converted my dataset into a question-and-answer format, thereby increasing the dataset to 923 data points.\n\nIndian Superstition & Beliefs (923)​\n\n\n\n\n4.4.3 2. Outline of Research Paper\nI switched to creating the outline of our research paper. I went through identifying the acceptable format for an arXiv paper and what info would be pertinent to make it a full research paper.\n\nConLingo 2.0: Fine-Tuning for Cultural Contextualization  \n\nBy: Rohan Aby, Moses Madale, Suwilanji Mwanza, and William Richards \n\n\nAbstract \n\nSummary of the paper in one paragraph. Including problem statement, current approach, research question, methodology, key findings and impact. \n\nTable of Contents \n \n\nI. Introduction - Suwilanji  \n\n1.1 Problem Statement \n\n1.2 Project Context \n\n1.3 Research Question \n\n1.4 Contributions \n\n1.5. Roadmap \n\n\nII. Related Work - Suwilanji \n\n2.1 Retrieval Augmented Generation (RAG) \n\n2.2 Fine-Tuning for Domain Adaptation \n\n2.3 Cultural AI and NLP \n\n2.4 Biblical Contextualization \n\n2.5 Gap Analysis \n \n\nIII. Methodology - Moses \n\n3.1 Data Collection \n\n3.1.1 Data Sources \n\n3.1.2 Collection Process \n\n3.1.3 Data Annotation & Metadata \n\n3.1.4 Ethical Considerations \n\n3.1.5 Dataset Statistics \n\n3.2 Model Architecture and Training \n\n3.2.1 Model Selection \n\n3.2.2 Environment Set Up \n\n3.2.3 Fine-Tuning Approach \n\n3.2.4 Training Details and Monitoring \n\n3.3. Baseline Systems \n\n3.3.1 RAG Baseline (Current ConLingo) \n\n3.3.2 Additional Baselines \n\n3.4 Evaluation Framework \n\n3.4.1 Cultural Sensitivity Index (CSI) \n\n3.4.2 Automated Metrics \n\n3.4.3 Cost & Efficiency Metrics \n\n3.4.4 Human Evaluation \n\n3.5 Dataset Testing \n\n\nIV. Results - Rohan \n\n4.1 Quantitative Results \n\n4.1.1 Overall Performance \n\n4.1.2 Category-Wise Performance \n\n4.1.3 Regional Analysis \n\n4.1.4 Statistical Significance \n\n4.2 Human Evaluation Results \n\n4.2.1 Human CSI Scores \n\n4.2.2 Qualitative Feedback from Indian testers? \n\n4.3 Qualitative Analysis:  \n\n4.4 Error Analysis \n\n4.4.1 Error Categories (What caused failure) \n\n4.4.2 Deep Dive Into Failure Cases \n\n\nV. Conclusion – Rohan + Moses \n\n5.1 Evaluation Summary - Rohan \n\n5.2 Why These Results? - Rohan \n\n5.2.1 If Fine-Tuning Wins \n\n5.2.2 If RAG Wins \n\n5.2.3 If Context-Dependent \n\n5.3 Implications for Project - Moses \n\n5.3.1 Deployment Recommendations \n\n5.3.2 Scalability Considerations \n\n5.4 Limitations - Moses \n\n5.4.1 Dataset Limitations \n\n5.4.2 Evaluation Limitations \n\n5.4.3 Model Limitations \n\n5.4.4 Methodological Limitations \n\n5.5 Future Applications \n\n5.6 Key Takeaways \n \n\nReferences \n\n\nAppendices",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span>"
    ]
  },
  {
    "objectID": "Week3.html#deliverable-pivot",
    "href": "Week3.html#deliverable-pivot",
    "title": "4  Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning",
    "section": "4.5 1. Deliverable Pivot",
    "text": "4.5 1. Deliverable Pivot\nDue to this restructuring, I had to pause my data collection strategy process, as the decisions we made would impact whether my data would be helpful or not.\nMeeting Agenda:\n\nWe defined what the existing ConLingo model is:\n\nAn AI assistant that helps you bridge the cultural gap between civilizations (starting with India).\n\nRevamped our deliverables due to two significant reasons:\n\n(1) Initial deliverables were overambitious as the reality of our time was realized.\n(2) We felt that because of this, we did not have SMART goals, emphasis on the A for Acheivable, and thus we redesigned the deliverables to be SMART goals for weeks 3-4.\n\nThe seven categories of culture that I created were updated and further categorized into Non-Material and Material.\n\nNon-Material Culture:​\n\nValues & Beliefs: what is considered good, desirable, and accurate.​\nNorms & Customs: routine practices, rituals, eg, Greetings, holiday celebrations.​\nLanguage: A system of words, symbols, and non-verbal cues.​\nReligion & Spirituality: morality and the supernatural​.\nArts and Literature: Music, dance, visual arts, stories, poetry, etc.​\nSocial Organization: family units (nuclear vs extended), social classes, and hierarchies based on wealth, age, and occupation​.\n\nMaterial Culture:​\n\nArtifacts & Technology: Eating utensils, transportation methods, and housing style​.\nGovernment & Economic Systems: The structure  used to provide for everyday needs, distribute power, and manage resources​.\n\n\nLastly, we had to make a final decision on whether to use Supervised Fine-Tuning (SFT) or Unsupervised Fine-Tuning (UFT) for our fine-tuning process. This decision is key because it affects how the data is formatted. If it is SFT that has to be labeled, the question-answer (QA) pairs are a must. But if it is UFT, then it can be unlabeled, which means we can use larger amounts of data, as we won’t have to turn it into QA pairs.\n\nWe had the following constraints to consider:\n\nHardware: NVIDIA A30 GPU (24GB VRAM) on Titan​\nTimeline: 3 Weeks​ (less than)\nTeam size: 4 people​\nGoal: Beat the RAG implementation in having Indian culturally nuanced responses\n\nSupervised Fine-Tuning (SFT) is a process of taking a pre-trained language model and further training it on a smaller, task-specific dataset with labeled examples. Its goal is to adjust the weights of the pre-trained model so that it performs better on our specific task without losing its general knowledge acquired during pre-training. (GeeksforGeeks, 2025)\nUnsupervised finetuning: This method doesn’t require labeled data. Instead, the LLM is exposed to a large corpus of unlabeled text from the target domain. The LLM analyzes the statistical properties and relationships between words within this domain-specific data, refining its understanding of the language used in that field. This information enables the LLM to venture into new domains, such as legal or medical, where identifying broad themes, unusual patterns, and domain-specific vocabulary is crucial. Unsupervised finetuning is commonly used for tasks such as language modeling, where the model learns to predict the next word in a sequence based on its context. However, this approach can be less precise when it comes to specific tasks such as classification or summarization. (Padgaonkar & Ghoshal, 2024)\nDue to time, team size, and computer resources constraints the only realistic option would be to use Supervised Fine Tuning where the primary focus would be to get excellent high quality Q & A pairs somewhere around 1000-1500 of them and do some fine tuning iterations on Titan and since training is in the form factor of hours in terms of the time taken, if the model is not performing as well it is easy to tweak parameters easily. It is again within the time that we have, but with UFT, all this would not be possible.\n\nBased on the datasets I collected, this was selected and then transformed by an AI-powered data cleaning pipeline created by Moses. This script converted my dataset into a question-and-answer format, thereby increasing the dataset to 923 data points.\n\nIndian Superstition & Beliefs (923)​",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span>"
    ]
  },
  {
    "objectID": "Week3.html#outline-of-research-paper",
    "href": "Week3.html#outline-of-research-paper",
    "title": "4  Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning",
    "section": "4.6 2. Outline of Research Paper",
    "text": "4.6 2. Outline of Research Paper\nI switched to creating the outline of our research paper. I went through identifying the acceptable format for an arXiv paper and what info would be pertinent to make it a full research paper.\n\nConLingo 2.0: Fine-Tuning for Cultural Contextualization  \n\nBy: Rohan Aby, Moses Madale, Suwilanji Mwanza, and William Richards \n\n\nAbstract \n\nSummary of the paper in one paragraph. Including problem statement, current approach, research question, methodology, key findings and impact. \n\nTable of Contents \n \n\nI. Introduction - Suwilanji  \n\n1.1 Problem Statement \n\n1.2 Project Context \n\n1.3 Research Question \n\n1.4 Contributions \n\n1.5. Roadmap \n\n\nII. Related Work - Suwilanji \n\n2.1 Retrieval Augmented Generation (RAG) \n\n2.2 Fine-Tuning for Domain Adaptation \n\n2.3 Cultural AI and NLP \n\n2.4 Biblical Contextualization \n\n2.5 Gap Analysis \n \n\nIII. Methodology - Moses \n\n3.1 Data Collection \n\n3.1.1 Data Sources \n\n3.1.2 Collection Process \n\n3.1.3 Data Annotation & Metadata \n\n3.1.4 Ethical Considerations \n\n3.1.5 Dataset Statistics \n\n3.2 Model Architecture and Training \n\n3.2.1 Model Selection \n\n3.2.2 Environment Set Up \n\n3.2.3 Fine-Tuning Approach \n\n3.2.4 Training Details and Monitoring \n\n3.3. Baseline Systems \n\n3.3.1 RAG Baseline (Current ConLingo) \n\n3.3.2 Additional Baselines \n\n3.4 Evaluation Framework \n\n3.4.1 Cultural Sensitivity Index (CSI) \n\n3.4.2 Automated Metrics \n\n3.4.3 Cost & Efficiency Metrics \n\n3.4.4 Human Evaluation \n\n3.5 Dataset Testing \n\n\nIV. Results - Rohan \n\n4.1 Quantitative Results \n\n4.1.1 Overall Performance \n\n4.1.2 Category-Wise Performance \n\n4.1.3 Regional Analysis \n\n4.1.4 Statistical Significance \n\n4.2 Human Evaluation Results \n\n4.2.1 Human CSI Scores \n\n4.2.2 Qualitative Feedback from Indian testers? \n\n4.3 Qualitative Analysis:  \n\n4.4 Error Analysis \n\n4.4.1 Error Categories (What caused failure) \n\n4.4.2 Deep Dive Into Failure Cases \n\n\nV. Conclusion – Rohan + Moses \n\n5.1 Evaluation Summary - Rohan \n\n5.2 Why These Results? - Rohan \n\n5.2.1 If Fine-Tuning Wins \n\n5.2.2 If RAG Wins \n\n5.2.3 If Context-Dependent \n\n5.3 Implications for Project - Moses \n\n5.3.1 Deployment Recommendations \n\n5.3.2 Scalability Considerations \n\n5.4 Limitations - Moses \n\n5.4.1 Dataset Limitations \n\n5.4.2 Evaluation Limitations \n\n5.4.3 Model Limitations \n\n5.4.4 Methodological Limitations \n\n5.5 Future Applications \n\n5.6 Key Takeaways \n \n\nReferences \n\n\nAppendices",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span>"
    ]
  },
  {
    "objectID": "Week4.html",
    "href": "Week4.html",
    "title": "5  Week 4: Data Searching, Data Vetting, Dataset Collection, Data Cleaning",
    "section": "",
    "text": "5.1 Overview\nIn Week 4, the team fine-tuned multiple models on Titan using various datasets, tested them with 20 selected questions, and analyzed performance across metrics like tone, empathy, and accuracy. Rohan led the performance analysis, while Moses and William handled model training, and Suwilanji created visualizations, investigated dataset issues, and began drafting the research paper and Week 4 presentation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span>"
    ]
  },
  {
    "objectID": "Week4.html#rohan-aby-deliverables",
    "href": "Week4.html#rohan-aby-deliverables",
    "title": "5  Week 4: Data Searching, Data Vetting, Dataset Collection, Data Cleaning",
    "section": "5.2 Rohan Aby Deliverables",
    "text": "5.2 Rohan Aby Deliverables\n\n5.2.1 CSI Scoring of superstitions and constitution dataset\nRohan was tasked with scoring the output from the superstitions and constitution dataset. The same twenty questions that were used for the other models were selected. The results showed us that the constitution model performed better. It had a score of 92%. The superstitions had a score of 79%. This is contrary to what we expected as we felt that the data that was used to train the superstitions model was more reliable than the constitution dataset. The results can be seen in the image below.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span>"
    ]
  },
  {
    "objectID": "Week4.html#william-richards-deliverables",
    "href": "Week4.html#william-richards-deliverables",
    "title": "5  Week 4: Data Searching, Data Vetting, Dataset Collection, Data Cleaning",
    "section": "5.3 William Richards Deliverables",
    "text": "5.3 William Richards Deliverables\n\n5.3.1 Individual Fine-Tuned Model Testing\nWills tasks for this deliverable was to alter Moses’ fine-tuning program for the Superstition and Constitution datasets to create fine-tuned models for each dataset:\n\n5.3.1.1 Superstitions Dataset:\n# Superstitions Fine-Tuning Model Bash Script\n\n#!/bin/bash\n#SBATCH --job-name=super_finetune\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=64G\n#SBATCH --gres=gpu:1\n#SBATCH --time=6:00:00\n#SBATCH --output=logs/super_finetune_%j.out\n#SBATCH --error=logs/super_finetune_%j.err\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=starwill16@gmail.com\n\nmodule purge\nmodule load Python/3.12.3-GCCcore-13.3.0\nmodule load CUDA/12.3.0\n\nsource /home/gcsc563_01/CSC463/conlingo/llama_env/bin/activate\n\nexport HF_HOME=/home/gcsc563_01/.cache/huggingface\nexport HUGGINGFACE_HUB_CACHE=/home/gcsc563_01/.cache/huggingface\n\ncd /home/gcsc563_01/conlingo\n\npython scripts/training/finetune_indian_superstitions.py\n\ndeactivate\n# Superstitions Fine-Tuning Model Python Script\n\n#!/usr/bin/env python3\n\"\"\"\nLoRA Fine-Tuning for the Indian Superstitions Data\nFine-tunes LLaMA-3 8B on Indian Superstitions Q&A pairs\n\"\"\"\n\nimport torch\nimport json\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling\n)\nfrom peft import LoraConfig, get_peft_model\nfrom datasets import Dataset\nimport os\nfrom sklearn.model_selection import train_test_split\n\nprint(\"=\"*60)\nprint(\"Superstitions LoRA Fine-Tuning Pipeline\")\nprint(\"=\"*60)\n\n# Paths\nDATA_PATH = \"/home/gcsc563_01/conlingo/data/raw/superstition_qa.jsonl\"\nOUTPUT_DIR = \"/home/gcsc563_01/conlingo/models/finetuned/superstition\"\nMODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\n# Create output directory\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"\\n1. Loading data from: {DATA_PATH}\")\n\n# Load JSONL data\ndata = []\nwith open(DATA_PATH, 'r', encoding='utf-8') as f:\n    for line in f:\n        data.append(json.loads(line))\n\nprint(f\"Total examples: {len(data)}\")\n\n# Extract question-answer pairs\nexamples = []\nfor item in data:\n    examples.append({\n        \"question\": item[\"question\"],\n        \"answer\": item[\"answer\"]\n    })\n\n# Split into train/validation (90/10)\ntrain_data, val_data = train_test_split(examples, test_size=0.1, random_state=42)\n\nprint(f\"Training examples: {len(train_data)}\")\nprint(f\"Validation examples: {len(val_data)}\")\n\nprint(f\"\\n2. Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nprint(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n\nprint(f\"\\n3. Preparing datasets...\")\n\ndef format_instruction(example):\n    \"\"\"Format question-answer pair for training\"\"\"\n    text = f\"### Question:\\n{example['question']}\\n\\n### Answer:\\n{example['answer']}\"\n    return text\n\ndef tokenize_function(example):\n    \"\"\"Tokenize examples with padding and truncation\"\"\"\n    text = format_instruction(example)\n    \n    tokenized = tokenizer(\n        text,\n        truncation=True,\n        max_length=512,\n        padding=\"max_length\",\n        return_tensors=None\n    )\n    \n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized\n\n# Convert to HuggingFace Dataset format\ntrain_dataset = Dataset.from_list(train_data)\nval_dataset = Dataset.from_list(val_data)\n\n# Tokenize\nprint(\"Tokenizing training data...\")\ntrain_dataset = train_dataset.map(\n    tokenize_function,\n    remove_columns=train_dataset.column_names\n)\n\nprint(\"Tokenizing validation data...\")\nval_dataset = val_dataset.map(\n    tokenize_function,\n    remove_columns=val_dataset.column_names\n)\n\nprint(f\"Training dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(val_dataset)}\")\n\nprint(f\"\\n4. Loading base model...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\n\nmodel.gradient_checkpointing_enable()\n\nprint(f\"Model loaded: {model.__class__.__name__}\")\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\nprint(f\"\\n5. Configuring LoRA...\")\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\n\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\n\nprint(f\"LoRA configured successfully\")\nprint(f\"Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.4f}%)\")\nprint(f\"Total parameters: {total_params:,}\")\n\nprint(f\"\\n6. Setting up training arguments...\")\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=16,\n    learning_rate=2e-4,\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=50,\n    logging_steps=10,\n    save_steps=50,\n    eval_steps=50,\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    fp16=True,\n    report_to=\"none\",\n    save_total_limit=2,\n    remove_unused_columns=False,\n    gradient_checkpointing=True\n)\n\nprint(\"Training configuration:\")\nprint(f\"  Epochs: {training_args.num_train_epochs}\")\nprint(f\"  Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  Learning rate: {training_args.learning_rate}\")\n\nprint(f\"\\n7. Initializing trainer...\")\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator\n)\n\nprint(\"Trainer initialized successfully!\")\n\nprint(f\"\\n8. Starting training...\")\nprint(\"=\"*60)\n\ntrainer.train()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Training complete!\")\nprint(\"=\"*60)\n\nprint(f\"\\n9. Saving final model...\")\nmodel.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\ntokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n\nprint(f\"Model saved to: {OUTPUT_DIR}/final_model\")\n\nprint(f\"\\n10. Final evaluation metrics:\")\neval_results = trainer.evaluate()\nfor key, value in eval_results.items():\n    print(f\"  {key}: {value:.4f}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Fine-tuning pipeline complete!\")\nprint(\"=\"*60)\n\n\n5.3.1.2 Constitutions Dataset:\n# Constitutions Fine-Tuning Model Bash Script\n\n#!/bin/bash\n#SBATCH --job-name=constitution_finetune\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=64G\n#SBATCH --gres=gpu:1\n#SBATCH --time=6:00:00\n#SBATCH --output=logs/const_finetune_%j.out\n#SBATCH --error=logs/const_finetune_%j.err\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=starwill16@gmail.com\n\nmodule purge\nmodule load Python/3.12.3-GCCcore-13.3.0\nmodule load CUDA/12.3.0\n\nsource /home/gcsc563_01/CSC463/conlingo/llama_env/bin/activate\n\nexport HF_HOME=/home/gcsc563_01/.cache/huggingface\nexport HUGGINGFACE_HUB_CACHE=/home/gcsc563_01/.cache/huggingface\n\ncd /home/gcsc563_01/conlingo\n\npython scripts/training/finetune_indian_constitutions.py\n\ndeactivate\n# Constitutions Fine-Tuning Model Python Script\n\n#!/usr/bin/env python3\n\"\"\"\nLoRA Fine-Tuning for the Indian Constitutions Data\nFine-tunes LLaMA-3 8B on Indian Constitutions Q&A pairs\n\"\"\"\n\nimport torch\nimport json\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling\n)\nfrom peft import LoraConfig, get_peft_model\nfrom datasets import Dataset\nimport os\nfrom sklearn.model_selection import train_test_split\n\nprint(\"=\"*60)\nprint(\"Constitutions LoRA Fine-Tuning Pipeline\")\nprint(\"=\"*60)\n\n# Paths\nDATA_PATH = \"/home/gcsc563_01/conlingo/data/raw/constitution_qa.jsonl\"\nOUTPUT_DIR = \"/home/gcsc563_01/conlingo/models/finetuned/constitution\"\nMODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\n# Create output directory\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"\\n1. Loading data from: {DATA_PATH}\")\n\n# Load JSONL data\ndata = []\nwith open(DATA_PATH, 'r', encoding='utf-8') as f:\n    for line in f:\n        data.append(json.loads(line))\n\nprint(f\"Total examples: {len(data)}\")\n\n# Extract question-answer pairs\nexamples = []\nfor item in data:\n    examples.append({\n        \"question\": item[\"question\"],\n        \"answer\": item[\"answer\"]\n    })\n\n# Split into train/validation (90/10)\ntrain_data, val_data = train_test_split(examples, test_size=0.1, random_state=42)\n\nprint(f\"Training examples: {len(train_data)}\")\nprint(f\"Validation examples: {len(val_data)}\")\n\nprint(f\"\\n2. Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nprint(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n\nprint(f\"\\n3. Preparing datasets...\")\n\ndef format_instruction(example):\n    \"\"\"Format question-answer pair for training\"\"\"\n    text = f\"### Question:\\n{example['question']}\\n\\n### Answer:\\n{example['answer']}\"\n    return text\n\ndef tokenize_function(example):\n    \"\"\"Tokenize examples with padding and truncation\"\"\"\n    text = format_instruction(example)\n    \n    tokenized = tokenizer(\n        text,\n        truncation=True,\n        max_length=512,\n        padding=\"max_length\",\n        return_tensors=None\n    )\n    \n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized\n\n# Convert to HuggingFace Dataset format\ntrain_dataset = Dataset.from_list(train_data)\nval_dataset = Dataset.from_list(val_data)\n\n# Tokenize\nprint(\"Tokenizing training data...\")\ntrain_dataset = train_dataset.map(\n    tokenize_function,\n    remove_columns=train_dataset.column_names\n)\n\nprint(\"Tokenizing validation data...\")\nval_dataset = val_dataset.map(\n    tokenize_function,\n    remove_columns=val_dataset.column_names\n)\n\nprint(f\"Training dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(val_dataset)}\")\n\nprint(f\"\\n4. Loading base model...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\n\nmodel.gradient_checkpointing_enable()\n\nprint(f\"Model loaded: {model.__class__.__name__}\")\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\nprint(f\"\\n5. Configuring LoRA...\")\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\n\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\n\nprint(f\"LoRA configured successfully\")\nprint(f\"Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.4f}%)\")\nprint(f\"Total parameters: {total_params:,}\")\n\nprint(f\"\\n6. Setting up training arguments...\")\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=16,\n    learning_rate=2e-4,\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=50,\n    logging_steps=10,\n    save_steps=50,\n    eval_steps=50,\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    fp16=True,\n    report_to=\"none\",\n    save_total_limit=2,\n    remove_unused_columns=False,\n    gradient_checkpointing=True\n)\n\nprint(\"Training configuration:\")\nprint(f\"  Epochs: {training_args.num_train_epochs}\")\nprint(f\"  Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  Learning rate: {training_args.learning_rate}\")\n\nprint(f\"\\n7. Initializing trainer...\")\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator\n)\n\nprint(\"Trainer initialized successfully!\")\n\nprint(f\"\\n8. Starting training...\")\nprint(\"=\"*60)\n\ntrainer.train()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Training complete!\")\nprint(\"=\"*60)\n\nprint(f\"\\n9. Saving final model...\")\nmodel.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\ntokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n\nprint(f\"Model saved to: {OUTPUT_DIR}/final_model\")\n\nprint(f\"\\n10. Final evaluation metrics:\")\neval_results = trainer.evaluate()\nfor key, value in eval_results.items():\n    print(f\"  {key}: {value:.4f}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Fine-tuning pipeline complete!\")\nprint(\"=\"*60)\n\n\n\n\n5.3.2 Question Testing\nOnce the fine-tuned models for the two datasets where created, the next was to test each model against the questions provided by Rohan.\n\n5.3.2.1 Superstitions Dataset:\n# Superstitions Model Testing Bash Script\n\n#!/bin/bash\n#SBATCH --job-name=test_super_model\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=32G\n#SBATCH --gres=gpu:1\n#SBATCH --time=1:00:00\n#SBATCH --output=logs/test_super_%j.out\n#SBATCH --error=logs/test_super_%j.err\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=starwill16@gmail.com\n\nmodule purge\nmodule load Python/3.12.3-GCCcore-13.3.0\nmodule load CUDA/12.3.0\n\nsource /home/gcsc563_01/CSC463/conlingo/llama_env/bin/activate\n\nexport HF_HOME=/home/gcsc563_01/.cache/huggingface\nexport HUGGINGFACE_HUB_CACHE=/home/gcsc563_01/.cache/huggingface\n\ncd /home/gcsc563_01/conlingo\n\npython scripts/training/run_superstitions_model.py\n\ndeactivate\n# Superstitions Model Testing Python Script\n\n#!/usr/bin/env python3\n\"\"\"\nTest the fine-tuned TED Talks model\nLoads the model and runs sample inference\n\"\"\"\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\nprint(\"=\"*60)\nprint(\"Superstitions Model Inference Test\")\nprint(\"=\"*60)\n\n# Paths\nBASE_MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nFINETUNED_MODEL_PATH = \"/home/gcsc563_01/conlingo/models/finetuned/superstition/final_model\"\n\nprint(f\"\\n1. Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(FINETUNED_MODEL_PATH)\nprint(f\"✅ Tokenizer loaded\")\n\nprint(f\"\\n2. Loading base model...\")\nbase_model = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\nprint(f\"✅ Base model loaded\")\n\nprint(f\"\\n3. Loading fine-tuned LoRA adapters...\")\nmodel = PeftModel.from_pretrained(base_model, FINETUNED_MODEL_PATH)\nprint(f\"✅ Fine-tuned model loaded\")\n\nprint(f\"\\n4. Running test inference...\")\nprint(\"-\"*60)\n\n# Test questions\ntest_questions = [\n    \"How can Diwali and Christmas both symbolize the victory of light over darkness in their respective traditions?\",\n    \"What themes of renewal in Diwali resonate with the idea of rebirth in Christian theology?\",\n    \"How might Indian Christians use Diwali to express biblical messages of hope?\",\n    \"How can Christian schools acknowledge Diwali without compromising faith boundaries?\",\n    \"What sensitivities should pastors consider when mentioning Hindu deities in Christmas homilies?\",\n    \"How can Christian youth groups create shared Diwali–Christmas community service projects?\",\n    \"How can churches ensure caste-neutral seating and participation during worship?\",\n    \"What examples of caste inclusion can be found in the life of Jesus?\",\n    \"What are sensitive ways to discuss “the least of these” without reinforcing caste bias?\",\n    \"Why might some Christians still use caste surnames, and how should this be discussed?\",\n    \"What is a sensitive way to discuss Jesus’ association with marginalized groups?\",\n    \"What is an inclusive way to discuss Krishna and Christ comparisons in academia?\",\n    \"How can Hindu concepts of karma be reconciled with Christian grace in conversation?\",\n    \"How can Christian missions emphasize shared moral principles rather than conversion?\",\n    \"How does food during Christmas vary regionally across India?\",\n    \"How does regional folklore shape Christian storytelling traditions?\",\n    \"How do Indian Christian elders respond to youth questioning of denominational traditions?\",\n    \"How can faith-based education evolve to reach younger, tech-savvy Christians?\",\n    \"How can liturgy adapt to youth participation without losing sacredness?\",\n    \"How do young Christians view service and mission in a modern Indian context?\"\n]\n\nfor i, question in enumerate(test_questions, 1):\n    print(f\"\\n[Test {i}]\")\n    print(f\"Question: {question}\")\n    \n    # Format prompt\n    prompt = f\"### Question:\\n{question}\\n\\n### Answer:\\n\"\n    \n    # Tokenize\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    # Generate\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            temperature=0.7,\n            do_sample=True,\n            top_p=0.9,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    # Decode\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Extract only the answer part (after \"### Answer:\")\n    answer = response.split(\"### Answer:\")[-1].strip()\n    \n    print(f\"Answer: {answer}\")\n    print(\"-\"*60)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Inference test complete!\")\nprint(\"=\"*60)\n\n\n\n5.3.2.2 Constitutions Dataset:\n# Constitutions Model Testing Bash Script\n\n#!/bin/bash\n#SBATCH --job-name=test_const_model\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=32G\n#SBATCH --gres=gpu:1\n#SBATCH --time=1:00:00\n#SBATCH --output=logs/test_const_%j.out\n#SBATCH --error=logs/test_const_%j.err\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=starwill16@gmail.com\n\nmodule purge\nmodule load Python/3.12.3-GCCcore-13.3.0\nmodule load CUDA/12.3.0\n\nsource /home/gcsc563_01/CSC463/conlingo/llama_env/bin/activate\n\nexport HF_HOME=/home/gcsc563_01/.cache/huggingface\nexport HUGGINGFACE_HUB_CACHE=/home/gcsc563_01/.cache/huggingface\n\ncd /home/gcsc563_01/conlingo\n\npython scripts/training/run_constitutions_model.py\n\ndeactivate\n# Constitutions Model Testing Python Script\n\n#!/usr/bin/env python3\n\"\"\"\nTest the fine-tuned Constitutions model\nLoads the model and runs sample inference\n\"\"\"\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\nprint(\"=\"*60)\nprint(\"Constitutions Model Inference Test\")\nprint(\"=\"*60)\n\n# Paths\nBASE_MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nFINETUNED_MODEL_PATH = \"/home/gcsc563_01/conlingo/models/finetuned/constitution/final_model\"\n\nprint(f\"\\n1. Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(FINETUNED_MODEL_PATH)\nprint(f\"✅ Tokenizer loaded\")\n\nprint(f\"\\n2. Loading base model...\")\nbase_model = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\nprint(f\"✅ Base model loaded\")\n\nprint(f\"\\n3. Loading fine-tuned LoRA adapters...\")\nmodel = PeftModel.from_pretrained(base_model, FINETUNED_MODEL_PATH)\nprint(f\"✅ Fine-tuned model loaded\")\n\nprint(f\"\\n4. Running test inference...\")\nprint(\"-\"*60)\n\n# Test questions\ntest_questions = [\n    \"How can Diwali and Christmas both symbolize the victory of light over darkness in their respective traditions?\",\n    \"What themes of renewal in Diwali resonate with the idea of rebirth in Christian theology?\",\n    \"How might Indian Christians use Diwali to express biblical messages of hope?\",\n    \"How can Christian schools acknowledge Diwali without compromising faith boundaries?\",\n    \"What sensitivities should pastors consider when mentioning Hindu deities in Christmas homilies?\",\n    \"How can Christian youth groups create shared Diwali–Christmas community service projects?\",\n    \"How can churches ensure caste-neutral seating and participation during worship?\",\n    \"What examples of caste inclusion can be found in the life of Jesus?\",\n    \"What are sensitive ways to discuss “the least of these” without reinforcing caste bias?\",\n    \"Why might some Christians still use caste surnames, and how should this be discussed?\",\n    \"What is a sensitive way to discuss Jesus’ association with marginalized groups?\",\n    \"What is an inclusive way to discuss Krishna and Christ comparisons in academia?\",\n    \"How can Hindu concepts of karma be reconciled with Christian grace in conversation?\",\n    \"How can Christian missions emphasize shared moral principles rather than conversion?\",\n    \"How does food during Christmas vary regionally across India?\",\n    \"How does regional folklore shape Christian storytelling traditions?\",\n    \"How do Indian Christian elders respond to youth questioning of denominational traditions?\",\n    \"How can faith-based education evolve to reach younger, tech-savvy Christians?\",\n    \"How can liturgy adapt to youth participation without losing sacredness?\",\n    \"How do young Christians view service and mission in a modern Indian context?\"\n]\n\nfor i, question in enumerate(test_questions, 1):\n    print(f\"\\n[Test {i}]\")\n    print(f\"Question: {question}\")\n    \n    # Format prompt\n    prompt = f\"### Question:\\n{question}\\n\\n### Answer:\\n\"\n    \n    # Tokenize\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    # Generate\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            temperature=0.7,\n            do_sample=True,\n            top_p=0.9,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    # Decode\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Extract only the answer part (after \"### Answer:\")\n    answer = response.split(\"### Answer:\")[-1].strip()\n    \n    print(f\"Answer: {answer}\")\n    print(\"-\"*60)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Inference test complete!\")\nprint(\"=\"*60)\n\n\n\n\n5.3.3 High Five Testing\nAfter testing the other 2 models, Wills final requirement was to combine all datasets into one, creating what I called the “High Five” dataset, fine-tuning a model for it, and testing it against Rohan’s questions one more time.\n\n5.3.3.1 Fine-Tuning Model:\n# High Five Fine-Tuning Model Bash Script\n\n#!/bin/bash\n#SBATCH --job-name=high_five_finetune\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=64G\n#SBATCH --gres=gpu:1\n#SBATCH --time=6:00:00\n#SBATCH --output=logs/high_finetune_%j.out\n#SBATCH --error=logs/high_finetune_%j.err\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=starwill16@gmail.com\n\nmodule purge\nmodule load Python/3.12.3-GCCcore-13.3.0\nmodule load CUDA/12.3.0\n\nsource /home/gcsc563_01/CSC463/conlingo/llama_env/bin/activate\n\nexport HF_HOME=/home/gcsc563_01/.cache/huggingface\nexport HUGGINGFACE_HUB_CACHE=/home/gcsc563_01/.cache/huggingface\n\ncd /home/gcsc563_01/conlingo\n\npython scripts/training/finetune_high_five.py\ndeactivate\n# High Five Fine-Tuning Model Python Script\n\n#!/usr/bin/env python3\n\"\"\"\nLoRA Fine-Tuning for all 5 Datasets\nFine-tunes LLaMA-3 8B on all 5 Datasets' Q&A pairs\n\"\"\"\n\nimport torch\nimport json\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling\n)\nfrom peft import LoraConfig, get_peft_model\nfrom datasets import Dataset\nimport os\nfrom sklearn.model_selection import train_test_split\n\nprint(\"=\"*60)\nprint(\"High Five LoRA Fine-Tuning Pipeline\")\nprint(\"=\"*60)\n\n# Paths\nDATA_PATH = \"/home/gcsc563_01/conlingo/data/raw/highfive_qa.jsonl\"\nOUTPUT_DIR = \"/home/gcsc563_01/conlingo/models/finetuned/high_five\"\nMODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\n# Create output directory\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"\\n1. Loading data from: {DATA_PATH}\")\n\n# Load JSONL data\ndata = []\nwith open(DATA_PATH, 'r', encoding='utf-8') as f:\n    for line in f:\n        data.append(json.loads(line))\n\nprint(f\"Total examples: {len(data)}\")\n\n# Extract question-answer pairs\nexamples = []\nfor item in data:\n    examples.append({\n        \"question\": item[\"question\"],\n        \"answer\": item[\"answer\"]\n    })\n\n# Split into train/validation (90/10)\ntrain_data, val_data = train_test_split(examples, test_size=0.1, random_state=42)\n\nprint(f\"Training examples: {len(train_data)}\")\nprint(f\"Validation examples: {len(val_data)}\")\n\nprint(f\"\\n2. Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nprint(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n\nprint(f\"\\n3. Preparing datasets...\")\n\ndef format_instruction(example):\n    \"\"\"Format question-answer pair for training\"\"\"\n    text = f\"### Question:\\n{example['question']}\\n\\n### Answer:\\n{example['answer']}\"\n    return text\n\ndef tokenize_function(example):\n    \"\"\"Tokenize examples with padding and truncation\"\"\"\n    text = format_instruction(example)\n    \n    tokenized = tokenizer(\n        text,\n        truncation=True,\n        max_length=512,\n        padding=\"max_length\",\n        return_tensors=None\n    )\n    \n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized\n\n# Convert to HuggingFace Dataset format\ntrain_dataset = Dataset.from_list(train_data)\nval_dataset = Dataset.from_list(val_data)\n\n# Tokenize\nprint(\"Tokenizing training data...\")\ntrain_dataset = train_dataset.map(\n    tokenize_function,\n    remove_columns=train_dataset.column_names\n)\n\nprint(\"Tokenizing validation data...\")\nval_dataset = val_dataset.map(\n    tokenize_function,\n    remove_columns=val_dataset.column_names\n)\n\nprint(f\"Training dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(val_dataset)}\")\n\nprint(f\"\\n4. Loading base model...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\n\nmodel.gradient_checkpointing_enable()\n\nprint(f\"Model loaded: {model.__class__.__name__}\")\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\nprint(f\"\\n5. Configuring LoRA...\")\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\n\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\n\nprint(f\"LoRA configured successfully\")\nprint(f\"Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.4f}%)\")\nprint(f\"Total parameters: {total_params:,}\")\n\nprint(f\"\\n6. Setting up training arguments...\")\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=16,\n    learning_rate=2e-4,\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=50,\n    logging_steps=10,\n    save_steps=50,\n    eval_steps=50,\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    fp16=True,\n    report_to=\"none\",\n    save_total_limit=2,\n    remove_unused_columns=False,\n    gradient_checkpointing=True\n)\n\nprint(\"Training configuration:\")\nprint(f\"  Epochs: {training_args.num_train_epochs}\")\nprint(f\"  Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  Learning rate: {training_args.learning_rate}\")\n\nprint(f\"\\n7. Initializing trainer...\")\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator\n)\n\nprint(\"Trainer initialized successfully!\")\n\nprint(f\"\\n8. Starting training...\")\nprint(\"=\"*60)\n\ntrainer.train()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Training complete!\")\nprint(\"=\"*60)\n\nprint(f\"\\n9. Saving final model...\")\nmodel.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\ntokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n\nprint(f\"Model saved to: {OUTPUT_DIR}/final_model\")\n\nprint(f\"\\n10. Final evaluation metrics:\")\neval_results = trainer.evaluate()\nfor key, value in eval_results.items():\n    print(f\"  {key}: {value:.4f}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Fine-tuning pipeline complete!\")\nprint(\"=\"*60)\n\n\n5.3.3.2 Model Testing:\n# High Five Model Testing Bash Script\n\n#!/bin/bash\n#SBATCH --job-name=test_high_five_model\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=32G\n#SBATCH --gres=gpu:1\n#SBATCH --time=1:00:00\n#SBATCH --output=logs/test_high_five_%j.out\n#SBATCH --error=logs/test_high_five_%j.err\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=starwill16@gmail.com\n\nmodule purge\nmodule load Python/3.12.3-GCCcore-13.3.0\nmodule load CUDA/12.3.0\n\nsource /home/gcsc563_01/CSC463/conlingo/llama_env/bin/activate\n\nexport HF_HOME=/home/gcsc563_01/.cache/huggingface\nexport HUGGINGFACE_HUB_CACHE=/home/gcsc563_01/.cache/huggingface\n\ncd /home/gcsc563_01/conlingo\n\npython scripts/training/run_high_five_model.py\n\ndeactivate\n# High Five Model Testing Python Script\n\n#!/usr/bin/env python3\n\"\"\"\nTest the fine-tuned High Five model\nLoads the model and runs sample inference\n\"\"\"\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\nprint(\"=\"*60)\nprint(\"High Five Model Inference Test\")\nprint(\"=\"*60)\n\n# Paths\nBASE_MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nFINETUNED_MODEL_PATH = \"/home/gcsc563_01/conlingo/models/finetuned/high_five/final_model\"\n\nprint(f\"\\n1. Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(FINETUNED_MODEL_PATH)\nprint(f\"✅ Tokenizer loaded\")\n\nprint(f\"\\n2. Loading base model...\")\nbase_model = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\nprint(f\"✅ Base model loaded\")\n\nprint(f\"\\n3. Loading fine-tuned LoRA adapters...\")\nmodel = PeftModel.from_pretrained(base_model, FINETUNED_MODEL_PATH)\nprint(f\"✅ Fine-tuned model loaded\")\n\nprint(f\"\\n4. Running test inference...\")\nprint(\"-\"*60)\n\n# Test questions\ntest_questions = [\n    \"How can Diwali and Christmas both symbolize the victory of light over darkness in their respective traditions?\",\n    \"What themes of renewal in Diwali resonate with the idea of rebirth in Christian theology?\",\n    \"How might Indian Christians use Diwali to express biblical messages of hope?\",\n    \"How can Christian schools acknowledge Diwali without compromising faith boundaries?\",\n    \"What sensitivities should pastors consider when mentioning Hindu deities in Christmas homilies?\",\n    \"How can Christian youth groups create shared Diwali–Christmas community service projects?\",\n    \"How can churches ensure caste-neutral seating and participation during worship?\",\n    \"What examples of caste inclusion can be found in the life of Jesus?\",\n    \"What are sensitive ways to discuss “the least of these” without reinforcing caste bias?\",\n    \"Why might some Christians still use caste surnames, and how should this be discussed?\",\n    \"What is a sensitive way to discuss Jesus’ association with marginalized groups?\",\n    \"What is an inclusive way to discuss Krishna and Christ comparisons in academia?\",\n    \"How can Hindu concepts of karma be reconciled with Christian grace in conversation?\",\n    \"How can Christian missions emphasize shared moral principles rather than conversion?\",\n    \"How does food during Christmas vary regionally across India?\",\n    \"How does regional folklore shape Christian storytelling traditions?\",\n    \"How do Indian Christian elders respond to youth questioning of denominational traditions?\",\n    \"How can faith-based education evolve to reach younger, tech-savvy Christians?\",\n    \"How can liturgy adapt to youth participation without losing sacredness?\",\n    \"How do young Christians view service and mission in a modern Indian context?\"\n]\n\n\nfor i, question in enumerate(test_questions, 1):\n    print(f\"\\n[Test {i}]\")\n    print(f\"Question: {question}\")\n    \n    # Format prompt\n    prompt = f\"### Question:\\n{question}\\n\\n### Answer:\\n\"\n    \n    # Tokenize\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    # Generate\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            temperature=0.7,\n            do_sample=True,\n            top_p=0.9,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    # Decode\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Extract only the answer part (after \"### Answer:\")\n    answer = response.split(\"### Answer:\")[-1].strip()\n    \n    print(f\"Answer: {answer}\")\n    print(\"-\"*60)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Inference test complete!\")\nprint(\"=\"*60)\nNow, after me and Moses’ analyses and fine-tuning process, evrey model is set for grading and comparison with the original ConLingo model.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span>"
    ]
  },
  {
    "objectID": "Week4.html#individual-fine-tuned-model-testing",
    "href": "Week4.html#individual-fine-tuned-model-testing",
    "title": "5  Week 4: Data Searching, Data Vetting, Dataset Collection, Data Cleaning",
    "section": "5.4 Individual Fine-Tuned Model Testing",
    "text": "5.4 Individual Fine-Tuned Model Testing\nMy tasks for this deliverable was to alter Moses’ fine-tuning program for the Superstition and Constitution datasets to create fine-tuned models for each dataset:\n\n5.4.1 Superstitions Dataset:\n# Superstitions Fine-Tuning Model Bash Script\n\n#!/bin/bash\n#SBATCH --job-name=super_finetune\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=64G\n#SBATCH --gres=gpu:1\n#SBATCH --time=6:00:00\n#SBATCH --output=logs/super_finetune_%j.out\n#SBATCH --error=logs/super_finetune_%j.err\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=starwill16@gmail.com\n\nmodule purge\nmodule load Python/3.12.3-GCCcore-13.3.0\nmodule load CUDA/12.3.0\n\nsource /home/gcsc563_01/CSC463/conlingo/llama_env/bin/activate\n\nexport HF_HOME=/home/gcsc563_01/.cache/huggingface\nexport HUGGINGFACE_HUB_CACHE=/home/gcsc563_01/.cache/huggingface\n\ncd /home/gcsc563_01/conlingo\n\npython scripts/training/finetune_indian_superstitions.py\n\ndeactivate\n# Superstitions Fine-Tuning Model Python Script\n\n#!/usr/bin/env python3\n\"\"\"\nLoRA Fine-Tuning for the Indian Superstitions Data\nFine-tunes LLaMA-3 8B on Indian Superstitions Q&A pairs\n\"\"\"\n\nimport torch\nimport json\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling\n)\nfrom peft import LoraConfig, get_peft_model\nfrom datasets import Dataset\nimport os\nfrom sklearn.model_selection import train_test_split\n\nprint(\"=\"*60)\nprint(\"Superstitions LoRA Fine-Tuning Pipeline\")\nprint(\"=\"*60)\n\n# Paths\nDATA_PATH = \"/home/gcsc563_01/conlingo/data/raw/superstition_qa.jsonl\"\nOUTPUT_DIR = \"/home/gcsc563_01/conlingo/models/finetuned/superstition\"\nMODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\n# Create output directory\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"\\n1. Loading data from: {DATA_PATH}\")\n\n# Load JSONL data\ndata = []\nwith open(DATA_PATH, 'r', encoding='utf-8') as f:\n    for line in f:\n        data.append(json.loads(line))\n\nprint(f\"Total examples: {len(data)}\")\n\n# Extract question-answer pairs\nexamples = []\nfor item in data:\n    examples.append({\n        \"question\": item[\"question\"],\n        \"answer\": item[\"answer\"]\n    })\n\n# Split into train/validation (90/10)\ntrain_data, val_data = train_test_split(examples, test_size=0.1, random_state=42)\n\nprint(f\"Training examples: {len(train_data)}\")\nprint(f\"Validation examples: {len(val_data)}\")\n\nprint(f\"\\n2. Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nprint(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n\nprint(f\"\\n3. Preparing datasets...\")\n\ndef format_instruction(example):\n    \"\"\"Format question-answer pair for training\"\"\"\n    text = f\"### Question:\\n{example['question']}\\n\\n### Answer:\\n{example['answer']}\"\n    return text\n\ndef tokenize_function(example):\n    \"\"\"Tokenize examples with padding and truncation\"\"\"\n    text = format_instruction(example)\n    \n    tokenized = tokenizer(\n        text,\n        truncation=True,\n        max_length=512,\n        padding=\"max_length\",\n        return_tensors=None\n    )\n    \n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized\n\n# Convert to HuggingFace Dataset format\ntrain_dataset = Dataset.from_list(train_data)\nval_dataset = Dataset.from_list(val_data)\n\n# Tokenize\nprint(\"Tokenizing training data...\")\ntrain_dataset = train_dataset.map(\n    tokenize_function,\n    remove_columns=train_dataset.column_names\n)\n\nprint(\"Tokenizing validation data...\")\nval_dataset = val_dataset.map(\n    tokenize_function,\n    remove_columns=val_dataset.column_names\n)\n\nprint(f\"Training dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(val_dataset)}\")\n\nprint(f\"\\n4. Loading base model...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\n\nmodel.gradient_checkpointing_enable()\n\nprint(f\"Model loaded: {model.__class__.__name__}\")\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\nprint(f\"\\n5. Configuring LoRA...\")\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\n\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\n\nprint(f\"LoRA configured successfully\")\nprint(f\"Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.4f}%)\")\nprint(f\"Total parameters: {total_params:,}\")\n\nprint(f\"\\n6. Setting up training arguments...\")\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=16,\n    learning_rate=2e-4,\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=50,\n    logging_steps=10,\n    save_steps=50,\n    eval_steps=50,\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    fp16=True,\n    report_to=\"none\",\n    save_total_limit=2,\n    remove_unused_columns=False,\n    gradient_checkpointing=True\n)\n\nprint(\"Training configuration:\")\nprint(f\"  Epochs: {training_args.num_train_epochs}\")\nprint(f\"  Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  Learning rate: {training_args.learning_rate}\")\n\nprint(f\"\\n7. Initializing trainer...\")\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator\n)\n\nprint(\"Trainer initialized successfully!\")\n\nprint(f\"\\n8. Starting training...\")\nprint(\"=\"*60)\n\ntrainer.train()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Training complete!\")\nprint(\"=\"*60)\n\nprint(f\"\\n9. Saving final model...\")\nmodel.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\ntokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n\nprint(f\"Model saved to: {OUTPUT_DIR}/final_model\")\n\nprint(f\"\\n10. Final evaluation metrics:\")\neval_results = trainer.evaluate()\nfor key, value in eval_results.items():\n    print(f\"  {key}: {value:.4f}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Fine-tuning pipeline complete!\")\nprint(\"=\"*60)\n\n\n5.4.2 Constitutions Dataset:\n# Constitutions Fine-Tuning Model Bash Script\n\n#!/bin/bash\n#SBATCH --job-name=constitution_finetune\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=64G\n#SBATCH --gres=gpu:1\n#SBATCH --time=6:00:00\n#SBATCH --output=logs/const_finetune_%j.out\n#SBATCH --error=logs/const_finetune_%j.err\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=starwill16@gmail.com\n\nmodule purge\nmodule load Python/3.12.3-GCCcore-13.3.0\nmodule load CUDA/12.3.0\n\nsource /home/gcsc563_01/CSC463/conlingo/llama_env/bin/activate\n\nexport HF_HOME=/home/gcsc563_01/.cache/huggingface\nexport HUGGINGFACE_HUB_CACHE=/home/gcsc563_01/.cache/huggingface\n\ncd /home/gcsc563_01/conlingo\n\npython scripts/training/finetune_indian_constitutions.py\n\ndeactivate\n# Constitutions Fine-Tuning Model Python Script\n\n#!/usr/bin/env python3\n\"\"\"\nLoRA Fine-Tuning for the Indian Constitutions Data\nFine-tunes LLaMA-3 8B on Indian Constitutions Q&A pairs\n\"\"\"\n\nimport torch\nimport json\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling\n)\nfrom peft import LoraConfig, get_peft_model\nfrom datasets import Dataset\nimport os\nfrom sklearn.model_selection import train_test_split\n\nprint(\"=\"*60)\nprint(\"Constitutions LoRA Fine-Tuning Pipeline\")\nprint(\"=\"*60)\n\n# Paths\nDATA_PATH = \"/home/gcsc563_01/conlingo/data/raw/constitution_qa.jsonl\"\nOUTPUT_DIR = \"/home/gcsc563_01/conlingo/models/finetuned/constitution\"\nMODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\n# Create output directory\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"\\n1. Loading data from: {DATA_PATH}\")\n\n# Load JSONL data\ndata = []\nwith open(DATA_PATH, 'r', encoding='utf-8') as f:\n    for line in f:\n        data.append(json.loads(line))\n\nprint(f\"Total examples: {len(data)}\")\n\n# Extract question-answer pairs\nexamples = []\nfor item in data:\n    examples.append({\n        \"question\": item[\"question\"],\n        \"answer\": item[\"answer\"]\n    })\n\n# Split into train/validation (90/10)\ntrain_data, val_data = train_test_split(examples, test_size=0.1, random_state=42)\n\nprint(f\"Training examples: {len(train_data)}\")\nprint(f\"Validation examples: {len(val_data)}\")\n\nprint(f\"\\n2. Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nprint(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n\nprint(f\"\\n3. Preparing datasets...\")\n\ndef format_instruction(example):\n    \"\"\"Format question-answer pair for training\"\"\"\n    text = f\"### Question:\\n{example['question']}\\n\\n### Answer:\\n{example['answer']}\"\n    return text\n\ndef tokenize_function(example):\n    \"\"\"Tokenize examples with padding and truncation\"\"\"\n    text = format_instruction(example)\n    \n    tokenized = tokenizer(\n        text,\n        truncation=True,\n        max_length=512,\n        padding=\"max_length\",\n        return_tensors=None\n    )\n    \n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized\n\n# Convert to HuggingFace Dataset format\ntrain_dataset = Dataset.from_list(train_data)\nval_dataset = Dataset.from_list(val_data)\n\n# Tokenize\nprint(\"Tokenizing training data...\")\ntrain_dataset = train_dataset.map(\n    tokenize_function,\n    remove_columns=train_dataset.column_names\n)\n\nprint(\"Tokenizing validation data...\")\nval_dataset = val_dataset.map(\n    tokenize_function,\n    remove_columns=val_dataset.column_names\n)\n\nprint(f\"Training dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(val_dataset)}\")\n\nprint(f\"\\n4. Loading base model...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\n\nmodel.gradient_checkpointing_enable()\n\nprint(f\"Model loaded: {model.__class__.__name__}\")\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\nprint(f\"\\n5. Configuring LoRA...\")\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\n\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\n\nprint(f\"LoRA configured successfully\")\nprint(f\"Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.4f}%)\")\nprint(f\"Total parameters: {total_params:,}\")\n\nprint(f\"\\n6. Setting up training arguments...\")\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=16,\n    learning_rate=2e-4,\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=50,\n    logging_steps=10,\n    save_steps=50,\n    eval_steps=50,\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    fp16=True,\n    report_to=\"none\",\n    save_total_limit=2,\n    remove_unused_columns=False,\n    gradient_checkpointing=True\n)\n\nprint(\"Training configuration:\")\nprint(f\"  Epochs: {training_args.num_train_epochs}\")\nprint(f\"  Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  Learning rate: {training_args.learning_rate}\")\n\nprint(f\"\\n7. Initializing trainer...\")\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator\n)\n\nprint(\"Trainer initialized successfully!\")\n\nprint(f\"\\n8. Starting training...\")\nprint(\"=\"*60)\n\ntrainer.train()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Training complete!\")\nprint(\"=\"*60)\n\nprint(f\"\\n9. Saving final model...\")\nmodel.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\ntokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n\nprint(f\"Model saved to: {OUTPUT_DIR}/final_model\")\n\nprint(f\"\\n10. Final evaluation metrics:\")\neval_results = trainer.evaluate()\nfor key, value in eval_results.items():\n    print(f\"  {key}: {value:.4f}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Fine-tuning pipeline complete!\")\nprint(\"=\"*60)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span>"
    ]
  },
  {
    "objectID": "Week4.html#question-testing",
    "href": "Week4.html#question-testing",
    "title": "5  Week 4: Data Searching, Data Vetting, Dataset Collection, Data Cleaning",
    "section": "5.5 Question Testing",
    "text": "5.5 Question Testing\nOnce the fine-tuned models for the two datasets where created, I now had the ability to test each model against the questions provided by Rohan.\n\n5.5.1 Superstitions Dataset:\n# Superstitions Model Testing Bash Script\n\n#!/bin/bash\n#SBATCH --job-name=test_super_model\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=32G\n#SBATCH --gres=gpu:1\n#SBATCH --time=1:00:00\n#SBATCH --output=logs/test_super_%j.out\n#SBATCH --error=logs/test_super_%j.err\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=starwill16@gmail.com\n\nmodule purge\nmodule load Python/3.12.3-GCCcore-13.3.0\nmodule load CUDA/12.3.0\n\nsource /home/gcsc563_01/CSC463/conlingo/llama_env/bin/activate\n\nexport HF_HOME=/home/gcsc563_01/.cache/huggingface\nexport HUGGINGFACE_HUB_CACHE=/home/gcsc563_01/.cache/huggingface\n\ncd /home/gcsc563_01/conlingo\n\npython scripts/training/run_superstitions_model.py\n\ndeactivate\n# Superstitions Model Testing Python Script\n\n#!/usr/bin/env python3\n\"\"\"\nTest the fine-tuned TED Talks model\nLoads the model and runs sample inference\n\"\"\"\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\nprint(\"=\"*60)\nprint(\"Superstitions Model Inference Test\")\nprint(\"=\"*60)\n\n# Paths\nBASE_MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nFINETUNED_MODEL_PATH = \"/home/gcsc563_01/conlingo/models/finetuned/superstition/final_model\"\n\nprint(f\"\\n1. Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(FINETUNED_MODEL_PATH)\nprint(f\"✅ Tokenizer loaded\")\n\nprint(f\"\\n2. Loading base model...\")\nbase_model = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\nprint(f\"✅ Base model loaded\")\n\nprint(f\"\\n3. Loading fine-tuned LoRA adapters...\")\nmodel = PeftModel.from_pretrained(base_model, FINETUNED_MODEL_PATH)\nprint(f\"✅ Fine-tuned model loaded\")\n\nprint(f\"\\n4. Running test inference...\")\nprint(\"-\"*60)\n\n# Test questions\ntest_questions = [\n    \"How can Diwali and Christmas both symbolize the victory of light over darkness in their respective traditions?\",\n    \"What themes of renewal in Diwali resonate with the idea of rebirth in Christian theology?\",\n    \"How might Indian Christians use Diwali to express biblical messages of hope?\",\n    \"How can Christian schools acknowledge Diwali without compromising faith boundaries?\",\n    \"What sensitivities should pastors consider when mentioning Hindu deities in Christmas homilies?\",\n    \"How can Christian youth groups create shared Diwali–Christmas community service projects?\",\n    \"How can churches ensure caste-neutral seating and participation during worship?\",\n    \"What examples of caste inclusion can be found in the life of Jesus?\",\n    \"What are sensitive ways to discuss “the least of these” without reinforcing caste bias?\",\n    \"Why might some Christians still use caste surnames, and how should this be discussed?\",\n    \"What is a sensitive way to discuss Jesus’ association with marginalized groups?\",\n    \"What is an inclusive way to discuss Krishna and Christ comparisons in academia?\",\n    \"How can Hindu concepts of karma be reconciled with Christian grace in conversation?\",\n    \"How can Christian missions emphasize shared moral principles rather than conversion?\",\n    \"How does food during Christmas vary regionally across India?\",\n    \"How does regional folklore shape Christian storytelling traditions?\",\n    \"How do Indian Christian elders respond to youth questioning of denominational traditions?\",\n    \"How can faith-based education evolve to reach younger, tech-savvy Christians?\",\n    \"How can liturgy adapt to youth participation without losing sacredness?\",\n    \"How do young Christians view service and mission in a modern Indian context?\"\n]\n\nfor i, question in enumerate(test_questions, 1):\n    print(f\"\\n[Test {i}]\")\n    print(f\"Question: {question}\")\n    \n    # Format prompt\n    prompt = f\"### Question:\\n{question}\\n\\n### Answer:\\n\"\n    \n    # Tokenize\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    # Generate\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            temperature=0.7,\n            do_sample=True,\n            top_p=0.9,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    # Decode\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Extract only the answer part (after \"### Answer:\")\n    answer = response.split(\"### Answer:\")[-1].strip()\n    \n    print(f\"Answer: {answer}\")\n    print(\"-\"*60)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Inference test complete!\")\nprint(\"=\"*60)\n\n\n\n5.5.2 Constitutions Dataset:\n# Constitutions Model Testing Bash Script\n\n#!/bin/bash\n#SBATCH --job-name=test_const_model\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=32G\n#SBATCH --gres=gpu:1\n#SBATCH --time=1:00:00\n#SBATCH --output=logs/test_const_%j.out\n#SBATCH --error=logs/test_const_%j.err\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=starwill16@gmail.com\n\nmodule purge\nmodule load Python/3.12.3-GCCcore-13.3.0\nmodule load CUDA/12.3.0\n\nsource /home/gcsc563_01/CSC463/conlingo/llama_env/bin/activate\n\nexport HF_HOME=/home/gcsc563_01/.cache/huggingface\nexport HUGGINGFACE_HUB_CACHE=/home/gcsc563_01/.cache/huggingface\n\ncd /home/gcsc563_01/conlingo\n\npython scripts/training/run_constitutions_model.py\n\ndeactivate\n# Constitutions Model Testing Python Script\n\n#!/usr/bin/env python3\n\"\"\"\nTest the fine-tuned Constitutions model\nLoads the model and runs sample inference\n\"\"\"\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\nprint(\"=\"*60)\nprint(\"Constitutions Model Inference Test\")\nprint(\"=\"*60)\n\n# Paths\nBASE_MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nFINETUNED_MODEL_PATH = \"/home/gcsc563_01/conlingo/models/finetuned/constitution/final_model\"\n\nprint(f\"\\n1. Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(FINETUNED_MODEL_PATH)\nprint(f\"✅ Tokenizer loaded\")\n\nprint(f\"\\n2. Loading base model...\")\nbase_model = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\nprint(f\"✅ Base model loaded\")\n\nprint(f\"\\n3. Loading fine-tuned LoRA adapters...\")\nmodel = PeftModel.from_pretrained(base_model, FINETUNED_MODEL_PATH)\nprint(f\"✅ Fine-tuned model loaded\")\n\nprint(f\"\\n4. Running test inference...\")\nprint(\"-\"*60)\n\n# Test questions\ntest_questions = [\n    \"How can Diwali and Christmas both symbolize the victory of light over darkness in their respective traditions?\",\n    \"What themes of renewal in Diwali resonate with the idea of rebirth in Christian theology?\",\n    \"How might Indian Christians use Diwali to express biblical messages of hope?\",\n    \"How can Christian schools acknowledge Diwali without compromising faith boundaries?\",\n    \"What sensitivities should pastors consider when mentioning Hindu deities in Christmas homilies?\",\n    \"How can Christian youth groups create shared Diwali–Christmas community service projects?\",\n    \"How can churches ensure caste-neutral seating and participation during worship?\",\n    \"What examples of caste inclusion can be found in the life of Jesus?\",\n    \"What are sensitive ways to discuss “the least of these” without reinforcing caste bias?\",\n    \"Why might some Christians still use caste surnames, and how should this be discussed?\",\n    \"What is a sensitive way to discuss Jesus’ association with marginalized groups?\",\n    \"What is an inclusive way to discuss Krishna and Christ comparisons in academia?\",\n    \"How can Hindu concepts of karma be reconciled with Christian grace in conversation?\",\n    \"How can Christian missions emphasize shared moral principles rather than conversion?\",\n    \"How does food during Christmas vary regionally across India?\",\n    \"How does regional folklore shape Christian storytelling traditions?\",\n    \"How do Indian Christian elders respond to youth questioning of denominational traditions?\",\n    \"How can faith-based education evolve to reach younger, tech-savvy Christians?\",\n    \"How can liturgy adapt to youth participation without losing sacredness?\",\n    \"How do young Christians view service and mission in a modern Indian context?\"\n]\n\nfor i, question in enumerate(test_questions, 1):\n    print(f\"\\n[Test {i}]\")\n    print(f\"Question: {question}\")\n    \n    # Format prompt\n    prompt = f\"### Question:\\n{question}\\n\\n### Answer:\\n\"\n    \n    # Tokenize\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    # Generate\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            temperature=0.7,\n            do_sample=True,\n            top_p=0.9,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    # Decode\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Extract only the answer part (after \"### Answer:\")\n    answer = response.split(\"### Answer:\")[-1].strip()\n    \n    print(f\"Answer: {answer}\")\n    print(\"-\"*60)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Inference test complete!\")\nprint(\"=\"*60)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span>"
    ]
  },
  {
    "objectID": "Week4.html#high-five-testing",
    "href": "Week4.html#high-five-testing",
    "title": "5  Week 4: Data Searching, Data Vetting, Dataset Collection, Data Cleaning",
    "section": "5.6 High Five Testing",
    "text": "5.6 High Five Testing\nAfter testing the other 2 models, my final requirement was to combine all datasets into one, creating what I called the “High Five” dataset, fine-tuning a model for it, and testing it against Rohan’s questions one more time.\n\n5.6.1 Fine-Tuning Model:\n# High Five Fine-Tuning Model Bash Script\n\n#!/bin/bash\n#SBATCH --job-name=high_five_finetune\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=64G\n#SBATCH --gres=gpu:1\n#SBATCH --time=6:00:00\n#SBATCH --output=logs/high_finetune_%j.out\n#SBATCH --error=logs/high_finetune_%j.err\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=starwill16@gmail.com\n\nmodule purge\nmodule load Python/3.12.3-GCCcore-13.3.0\nmodule load CUDA/12.3.0\n\nsource /home/gcsc563_01/CSC463/conlingo/llama_env/bin/activate\n\nexport HF_HOME=/home/gcsc563_01/.cache/huggingface\nexport HUGGINGFACE_HUB_CACHE=/home/gcsc563_01/.cache/huggingface\n\ncd /home/gcsc563_01/conlingo\n\npython scripts/training/finetune_high_five.py\ndeactivate\n# High Five Fine-Tuning Model Python Script\n\n#!/usr/bin/env python3\n\"\"\"\nLoRA Fine-Tuning for all 5 Datasets\nFine-tunes LLaMA-3 8B on all 5 Datasets' Q&A pairs\n\"\"\"\n\nimport torch\nimport json\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling\n)\nfrom peft import LoraConfig, get_peft_model\nfrom datasets import Dataset\nimport os\nfrom sklearn.model_selection import train_test_split\n\nprint(\"=\"*60)\nprint(\"High Five LoRA Fine-Tuning Pipeline\")\nprint(\"=\"*60)\n\n# Paths\nDATA_PATH = \"/home/gcsc563_01/conlingo/data/raw/highfive_qa.jsonl\"\nOUTPUT_DIR = \"/home/gcsc563_01/conlingo/models/finetuned/high_five\"\nMODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\n# Create output directory\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"\\n1. Loading data from: {DATA_PATH}\")\n\n# Load JSONL data\ndata = []\nwith open(DATA_PATH, 'r', encoding='utf-8') as f:\n    for line in f:\n        data.append(json.loads(line))\n\nprint(f\"Total examples: {len(data)}\")\n\n# Extract question-answer pairs\nexamples = []\nfor item in data:\n    examples.append({\n        \"question\": item[\"question\"],\n        \"answer\": item[\"answer\"]\n    })\n\n# Split into train/validation (90/10)\ntrain_data, val_data = train_test_split(examples, test_size=0.1, random_state=42)\n\nprint(f\"Training examples: {len(train_data)}\")\nprint(f\"Validation examples: {len(val_data)}\")\n\nprint(f\"\\n2. Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nprint(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n\nprint(f\"\\n3. Preparing datasets...\")\n\ndef format_instruction(example):\n    \"\"\"Format question-answer pair for training\"\"\"\n    text = f\"### Question:\\n{example['question']}\\n\\n### Answer:\\n{example['answer']}\"\n    return text\n\ndef tokenize_function(example):\n    \"\"\"Tokenize examples with padding and truncation\"\"\"\n    text = format_instruction(example)\n    \n    tokenized = tokenizer(\n        text,\n        truncation=True,\n        max_length=512,\n        padding=\"max_length\",\n        return_tensors=None\n    )\n    \n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized\n\n# Convert to HuggingFace Dataset format\ntrain_dataset = Dataset.from_list(train_data)\nval_dataset = Dataset.from_list(val_data)\n\n# Tokenize\nprint(\"Tokenizing training data...\")\ntrain_dataset = train_dataset.map(\n    tokenize_function,\n    remove_columns=train_dataset.column_names\n)\n\nprint(\"Tokenizing validation data...\")\nval_dataset = val_dataset.map(\n    tokenize_function,\n    remove_columns=val_dataset.column_names\n)\n\nprint(f\"Training dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(val_dataset)}\")\n\nprint(f\"\\n4. Loading base model...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\n\nmodel.gradient_checkpointing_enable()\n\nprint(f\"Model loaded: {model.__class__.__name__}\")\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\nprint(f\"\\n5. Configuring LoRA...\")\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\n\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\n\nprint(f\"LoRA configured successfully\")\nprint(f\"Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.4f}%)\")\nprint(f\"Total parameters: {total_params:,}\")\n\nprint(f\"\\n6. Setting up training arguments...\")\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=16,\n    learning_rate=2e-4,\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=50,\n    logging_steps=10,\n    save_steps=50,\n    eval_steps=50,\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    fp16=True,\n    report_to=\"none\",\n    save_total_limit=2,\n    remove_unused_columns=False,\n    gradient_checkpointing=True\n)\n\nprint(\"Training configuration:\")\nprint(f\"  Epochs: {training_args.num_train_epochs}\")\nprint(f\"  Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  Learning rate: {training_args.learning_rate}\")\n\nprint(f\"\\n7. Initializing trainer...\")\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator\n)\n\nprint(\"Trainer initialized successfully!\")\n\nprint(f\"\\n8. Starting training...\")\nprint(\"=\"*60)\n\ntrainer.train()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Training complete!\")\nprint(\"=\"*60)\n\nprint(f\"\\n9. Saving final model...\")\nmodel.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\ntokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n\nprint(f\"Model saved to: {OUTPUT_DIR}/final_model\")\n\nprint(f\"\\n10. Final evaluation metrics:\")\neval_results = trainer.evaluate()\nfor key, value in eval_results.items():\n    print(f\"  {key}: {value:.4f}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Fine-tuning pipeline complete!\")\nprint(\"=\"*60)\n\n\n5.6.2 Model Testing:\n# High Five Model Testing Bash Script\n\n#!/bin/bash\n#SBATCH --job-name=test_high_five_model\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=32G\n#SBATCH --gres=gpu:1\n#SBATCH --time=1:00:00\n#SBATCH --output=logs/test_high_five_%j.out\n#SBATCH --error=logs/test_high_five_%j.err\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=starwill16@gmail.com\n\nmodule purge\nmodule load Python/3.12.3-GCCcore-13.3.0\nmodule load CUDA/12.3.0\n\nsource /home/gcsc563_01/CSC463/conlingo/llama_env/bin/activate\n\nexport HF_HOME=/home/gcsc563_01/.cache/huggingface\nexport HUGGINGFACE_HUB_CACHE=/home/gcsc563_01/.cache/huggingface\n\ncd /home/gcsc563_01/conlingo\n\npython scripts/training/run_high_five_model.py\n\ndeactivate\n# High Five Model Testing Python Script\n\n#!/usr/bin/env python3\n\"\"\"\nTest the fine-tuned High Five model\nLoads the model and runs sample inference\n\"\"\"\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\nprint(\"=\"*60)\nprint(\"High Five Model Inference Test\")\nprint(\"=\"*60)\n\n# Paths\nBASE_MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nFINETUNED_MODEL_PATH = \"/home/gcsc563_01/conlingo/models/finetuned/high_five/final_model\"\n\nprint(f\"\\n1. Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(FINETUNED_MODEL_PATH)\nprint(f\"✅ Tokenizer loaded\")\n\nprint(f\"\\n2. Loading base model...\")\nbase_model = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\nprint(f\"✅ Base model loaded\")\n\nprint(f\"\\n3. Loading fine-tuned LoRA adapters...\")\nmodel = PeftModel.from_pretrained(base_model, FINETUNED_MODEL_PATH)\nprint(f\"✅ Fine-tuned model loaded\")\n\nprint(f\"\\n4. Running test inference...\")\nprint(\"-\"*60)\n\n# Test questions\ntest_questions = [\n    \"How can Diwali and Christmas both symbolize the victory of light over darkness in their respective traditions?\",\n    \"What themes of renewal in Diwali resonate with the idea of rebirth in Christian theology?\",\n    \"How might Indian Christians use Diwali to express biblical messages of hope?\",\n    \"How can Christian schools acknowledge Diwali without compromising faith boundaries?\",\n    \"What sensitivities should pastors consider when mentioning Hindu deities in Christmas homilies?\",\n    \"How can Christian youth groups create shared Diwali–Christmas community service projects?\",\n    \"How can churches ensure caste-neutral seating and participation during worship?\",\n    \"What examples of caste inclusion can be found in the life of Jesus?\",\n    \"What are sensitive ways to discuss “the least of these” without reinforcing caste bias?\",\n    \"Why might some Christians still use caste surnames, and how should this be discussed?\",\n    \"What is a sensitive way to discuss Jesus’ association with marginalized groups?\",\n    \"What is an inclusive way to discuss Krishna and Christ comparisons in academia?\",\n    \"How can Hindu concepts of karma be reconciled with Christian grace in conversation?\",\n    \"How can Christian missions emphasize shared moral principles rather than conversion?\",\n    \"How does food during Christmas vary regionally across India?\",\n    \"How does regional folklore shape Christian storytelling traditions?\",\n    \"How do Indian Christian elders respond to youth questioning of denominational traditions?\",\n    \"How can faith-based education evolve to reach younger, tech-savvy Christians?\",\n    \"How can liturgy adapt to youth participation without losing sacredness?\",\n    \"How do young Christians view service and mission in a modern Indian context?\"\n]\n\n\nfor i, question in enumerate(test_questions, 1):\n    print(f\"\\n[Test {i}]\")\n    print(f\"Question: {question}\")\n    \n    # Format prompt\n    prompt = f\"### Question:\\n{question}\\n\\n### Answer:\\n\"\n    \n    # Tokenize\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    # Generate\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            temperature=0.7,\n            do_sample=True,\n            top_p=0.9,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    # Decode\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Extract only the answer part (after \"### Answer:\")\n    answer = response.split(\"### Answer:\")[-1].strip()\n    \n    print(f\"Answer: {answer}\")\n    print(\"-\"*60)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Inference test complete!\")\nprint(\"=\"*60)\nNow, after me and Moses’ analyses and fine-tuning process, evrey model is set for grading and comparison with the original ConLingo model.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span>"
    ]
  },
  {
    "objectID": "Week4.html#william-richards-deliverables-model-fine-tuning-and-testing",
    "href": "Week4.html#william-richards-deliverables-model-fine-tuning-and-testing",
    "title": "5  Week 4: Data Searching, Data Vetting, Dataset Collection, Data Cleaning",
    "section": "5.3 William Richards Deliverables: Model Fine-tuning and Testing",
    "text": "5.3 William Richards Deliverables: Model Fine-tuning and Testing\n\n5.3.1 Individual Fine-Tuned Model Testing\nMy tasks for this deliverable was to alter Moses’ fine-tuning program for the Superstition and Constitution datasets to create fine-tuned models for each dataset:\n\n5.3.1.1 Superstitions Dataset:\n# Superstitions Fine-Tuning Model Bash Script\n\n#!/bin/bash\n#SBATCH --job-name=super_finetune\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=64G\n#SBATCH --gres=gpu:1\n#SBATCH --time=6:00:00\n#SBATCH --output=logs/super_finetune_%j.out\n#SBATCH --error=logs/super_finetune_%j.err\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=starwill16@gmail.com\n\nmodule purge\nmodule load Python/3.12.3-GCCcore-13.3.0\nmodule load CUDA/12.3.0\n\nsource /home/gcsc563_01/CSC463/conlingo/llama_env/bin/activate\n\nexport HF_HOME=/home/gcsc563_01/.cache/huggingface\nexport HUGGINGFACE_HUB_CACHE=/home/gcsc563_01/.cache/huggingface\n\ncd /home/gcsc563_01/conlingo\n\npython scripts/training/finetune_indian_superstitions.py\n\ndeactivate\n# Superstitions Fine-Tuning Model Python Script\n\n#!/usr/bin/env python3\n\"\"\"\nLoRA Fine-Tuning for the Indian Superstitions Data\nFine-tunes LLaMA-3 8B on Indian Superstitions Q&A pairs\n\"\"\"\n\nimport torch\nimport json\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling\n)\nfrom peft import LoraConfig, get_peft_model\nfrom datasets import Dataset\nimport os\nfrom sklearn.model_selection import train_test_split\n\nprint(\"=\"*60)\nprint(\"Superstitions LoRA Fine-Tuning Pipeline\")\nprint(\"=\"*60)\n\n# Paths\nDATA_PATH = \"/home/gcsc563_01/conlingo/data/raw/superstition_qa.jsonl\"\nOUTPUT_DIR = \"/home/gcsc563_01/conlingo/models/finetuned/superstition\"\nMODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\n# Create output directory\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"\\n1. Loading data from: {DATA_PATH}\")\n\n# Load JSONL data\ndata = []\nwith open(DATA_PATH, 'r', encoding='utf-8') as f:\n    for line in f:\n        data.append(json.loads(line))\n\nprint(f\"Total examples: {len(data)}\")\n\n# Extract question-answer pairs\nexamples = []\nfor item in data:\n    examples.append({\n        \"question\": item[\"question\"],\n        \"answer\": item[\"answer\"]\n    })\n\n# Split into train/validation (90/10)\ntrain_data, val_data = train_test_split(examples, test_size=0.1, random_state=42)\n\nprint(f\"Training examples: {len(train_data)}\")\nprint(f\"Validation examples: {len(val_data)}\")\n\nprint(f\"\\n2. Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nprint(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n\nprint(f\"\\n3. Preparing datasets...\")\n\ndef format_instruction(example):\n    \"\"\"Format question-answer pair for training\"\"\"\n    text = f\"### Question:\\n{example['question']}\\n\\n### Answer:\\n{example['answer']}\"\n    return text\n\ndef tokenize_function(example):\n    \"\"\"Tokenize examples with padding and truncation\"\"\"\n    text = format_instruction(example)\n    \n    tokenized = tokenizer(\n        text,\n        truncation=True,\n        max_length=512,\n        padding=\"max_length\",\n        return_tensors=None\n    )\n    \n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized\n\n# Convert to HuggingFace Dataset format\ntrain_dataset = Dataset.from_list(train_data)\nval_dataset = Dataset.from_list(val_data)\n\n# Tokenize\nprint(\"Tokenizing training data...\")\ntrain_dataset = train_dataset.map(\n    tokenize_function,\n    remove_columns=train_dataset.column_names\n)\n\nprint(\"Tokenizing validation data...\")\nval_dataset = val_dataset.map(\n    tokenize_function,\n    remove_columns=val_dataset.column_names\n)\n\nprint(f\"Training dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(val_dataset)}\")\n\nprint(f\"\\n4. Loading base model...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\n\nmodel.gradient_checkpointing_enable()\n\nprint(f\"Model loaded: {model.__class__.__name__}\")\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\nprint(f\"\\n5. Configuring LoRA...\")\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\n\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\n\nprint(f\"LoRA configured successfully\")\nprint(f\"Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.4f}%)\")\nprint(f\"Total parameters: {total_params:,}\")\n\nprint(f\"\\n6. Setting up training arguments...\")\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=16,\n    learning_rate=2e-4,\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=50,\n    logging_steps=10,\n    save_steps=50,\n    eval_steps=50,\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    fp16=True,\n    report_to=\"none\",\n    save_total_limit=2,\n    remove_unused_columns=False,\n    gradient_checkpointing=True\n)\n\nprint(\"Training configuration:\")\nprint(f\"  Epochs: {training_args.num_train_epochs}\")\nprint(f\"  Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  Learning rate: {training_args.learning_rate}\")\n\nprint(f\"\\n7. Initializing trainer...\")\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator\n)\n\nprint(\"Trainer initialized successfully!\")\n\nprint(f\"\\n8. Starting training...\")\nprint(\"=\"*60)\n\ntrainer.train()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Training complete!\")\nprint(\"=\"*60)\n\nprint(f\"\\n9. Saving final model...\")\nmodel.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\ntokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n\nprint(f\"Model saved to: {OUTPUT_DIR}/final_model\")\n\nprint(f\"\\n10. Final evaluation metrics:\")\neval_results = trainer.evaluate()\nfor key, value in eval_results.items():\n    print(f\"  {key}: {value:.4f}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Fine-tuning pipeline complete!\")\nprint(\"=\"*60)\n\n\n5.3.1.2 Constitutions Dataset:\n# Constitutions Fine-Tuning Model Bash Script\n\n#!/bin/bash\n#SBATCH --job-name=constitution_finetune\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=64G\n#SBATCH --gres=gpu:1\n#SBATCH --time=6:00:00\n#SBATCH --output=logs/const_finetune_%j.out\n#SBATCH --error=logs/const_finetune_%j.err\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=starwill16@gmail.com\n\nmodule purge\nmodule load Python/3.12.3-GCCcore-13.3.0\nmodule load CUDA/12.3.0\n\nsource /home/gcsc563_01/CSC463/conlingo/llama_env/bin/activate\n\nexport HF_HOME=/home/gcsc563_01/.cache/huggingface\nexport HUGGINGFACE_HUB_CACHE=/home/gcsc563_01/.cache/huggingface\n\ncd /home/gcsc563_01/conlingo\n\npython scripts/training/finetune_indian_constitutions.py\n\ndeactivate\n# Constitutions Fine-Tuning Model Python Script\n\n#!/usr/bin/env python3\n\"\"\"\nLoRA Fine-Tuning for the Indian Constitutions Data\nFine-tunes LLaMA-3 8B on Indian Constitutions Q&A pairs\n\"\"\"\n\nimport torch\nimport json\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling\n)\nfrom peft import LoraConfig, get_peft_model\nfrom datasets import Dataset\nimport os\nfrom sklearn.model_selection import train_test_split\n\nprint(\"=\"*60)\nprint(\"Constitutions LoRA Fine-Tuning Pipeline\")\nprint(\"=\"*60)\n\n# Paths\nDATA_PATH = \"/home/gcsc563_01/conlingo/data/raw/constitution_qa.jsonl\"\nOUTPUT_DIR = \"/home/gcsc563_01/conlingo/models/finetuned/constitution\"\nMODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\n# Create output directory\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"\\n1. Loading data from: {DATA_PATH}\")\n\n# Load JSONL data\ndata = []\nwith open(DATA_PATH, 'r', encoding='utf-8') as f:\n    for line in f:\n        data.append(json.loads(line))\n\nprint(f\"Total examples: {len(data)}\")\n\n# Extract question-answer pairs\nexamples = []\nfor item in data:\n    examples.append({\n        \"question\": item[\"question\"],\n        \"answer\": item[\"answer\"]\n    })\n\n# Split into train/validation (90/10)\ntrain_data, val_data = train_test_split(examples, test_size=0.1, random_state=42)\n\nprint(f\"Training examples: {len(train_data)}\")\nprint(f\"Validation examples: {len(val_data)}\")\n\nprint(f\"\\n2. Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nprint(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n\nprint(f\"\\n3. Preparing datasets...\")\n\ndef format_instruction(example):\n    \"\"\"Format question-answer pair for training\"\"\"\n    text = f\"### Question:\\n{example['question']}\\n\\n### Answer:\\n{example['answer']}\"\n    return text\n\ndef tokenize_function(example):\n    \"\"\"Tokenize examples with padding and truncation\"\"\"\n    text = format_instruction(example)\n    \n    tokenized = tokenizer(\n        text,\n        truncation=True,\n        max_length=512,\n        padding=\"max_length\",\n        return_tensors=None\n    )\n    \n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized\n\n# Convert to HuggingFace Dataset format\ntrain_dataset = Dataset.from_list(train_data)\nval_dataset = Dataset.from_list(val_data)\n\n# Tokenize\nprint(\"Tokenizing training data...\")\ntrain_dataset = train_dataset.map(\n    tokenize_function,\n    remove_columns=train_dataset.column_names\n)\n\nprint(\"Tokenizing validation data...\")\nval_dataset = val_dataset.map(\n    tokenize_function,\n    remove_columns=val_dataset.column_names\n)\n\nprint(f\"Training dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(val_dataset)}\")\n\nprint(f\"\\n4. Loading base model...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\n\nmodel.gradient_checkpointing_enable()\n\nprint(f\"Model loaded: {model.__class__.__name__}\")\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\nprint(f\"\\n5. Configuring LoRA...\")\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\n\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\n\nprint(f\"LoRA configured successfully\")\nprint(f\"Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.4f}%)\")\nprint(f\"Total parameters: {total_params:,}\")\n\nprint(f\"\\n6. Setting up training arguments...\")\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=16,\n    learning_rate=2e-4,\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=50,\n    logging_steps=10,\n    save_steps=50,\n    eval_steps=50,\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    fp16=True,\n    report_to=\"none\",\n    save_total_limit=2,\n    remove_unused_columns=False,\n    gradient_checkpointing=True\n)\n\nprint(\"Training configuration:\")\nprint(f\"  Epochs: {training_args.num_train_epochs}\")\nprint(f\"  Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  Learning rate: {training_args.learning_rate}\")\n\nprint(f\"\\n7. Initializing trainer...\")\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator\n)\n\nprint(\"Trainer initialized successfully!\")\n\nprint(f\"\\n8. Starting training...\")\nprint(\"=\"*60)\n\ntrainer.train()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Training complete!\")\nprint(\"=\"*60)\n\nprint(f\"\\n9. Saving final model...\")\nmodel.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\ntokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n\nprint(f\"Model saved to: {OUTPUT_DIR}/final_model\")\n\nprint(f\"\\n10. Final evaluation metrics:\")\neval_results = trainer.evaluate()\nfor key, value in eval_results.items():\n    print(f\"  {key}: {value:.4f}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Fine-tuning pipeline complete!\")\nprint(\"=\"*60)\n\n\n\n\n5.3.2 Question Testing\nOnce the fine-tuned models for the two datasets where created, I now had the ability to test each model against the questions provided by Rohan.\n\n5.3.2.1 Superstitions Dataset:\n# Superstitions Model Testing Bash Script\n\n#!/bin/bash\n#SBATCH --job-name=test_super_model\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=32G\n#SBATCH --gres=gpu:1\n#SBATCH --time=1:00:00\n#SBATCH --output=logs/test_super_%j.out\n#SBATCH --error=logs/test_super_%j.err\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=starwill16@gmail.com\n\nmodule purge\nmodule load Python/3.12.3-GCCcore-13.3.0\nmodule load CUDA/12.3.0\n\nsource /home/gcsc563_01/CSC463/conlingo/llama_env/bin/activate\n\nexport HF_HOME=/home/gcsc563_01/.cache/huggingface\nexport HUGGINGFACE_HUB_CACHE=/home/gcsc563_01/.cache/huggingface\n\ncd /home/gcsc563_01/conlingo\n\npython scripts/training/run_superstitions_model.py\n\ndeactivate\n# Superstitions Model Testing Python Script\n\n#!/usr/bin/env python3\n\"\"\"\nTest the fine-tuned TED Talks model\nLoads the model and runs sample inference\n\"\"\"\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\nprint(\"=\"*60)\nprint(\"Superstitions Model Inference Test\")\nprint(\"=\"*60)\n\n# Paths\nBASE_MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nFINETUNED_MODEL_PATH = \"/home/gcsc563_01/conlingo/models/finetuned/superstition/final_model\"\n\nprint(f\"\\n1. Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(FINETUNED_MODEL_PATH)\nprint(f\"✅ Tokenizer loaded\")\n\nprint(f\"\\n2. Loading base model...\")\nbase_model = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\nprint(f\"✅ Base model loaded\")\n\nprint(f\"\\n3. Loading fine-tuned LoRA adapters...\")\nmodel = PeftModel.from_pretrained(base_model, FINETUNED_MODEL_PATH)\nprint(f\"✅ Fine-tuned model loaded\")\n\nprint(f\"\\n4. Running test inference...\")\nprint(\"-\"*60)\n\n# Test questions\ntest_questions = [\n    \"How can Diwali and Christmas both symbolize the victory of light over darkness in their respective traditions?\",\n    \"What themes of renewal in Diwali resonate with the idea of rebirth in Christian theology?\",\n    \"How might Indian Christians use Diwali to express biblical messages of hope?\",\n    \"How can Christian schools acknowledge Diwali without compromising faith boundaries?\",\n    \"What sensitivities should pastors consider when mentioning Hindu deities in Christmas homilies?\",\n    \"How can Christian youth groups create shared Diwali–Christmas community service projects?\",\n    \"How can churches ensure caste-neutral seating and participation during worship?\",\n    \"What examples of caste inclusion can be found in the life of Jesus?\",\n    \"What are sensitive ways to discuss “the least of these” without reinforcing caste bias?\",\n    \"Why might some Christians still use caste surnames, and how should this be discussed?\",\n    \"What is a sensitive way to discuss Jesus’ association with marginalized groups?\",\n    \"What is an inclusive way to discuss Krishna and Christ comparisons in academia?\",\n    \"How can Hindu concepts of karma be reconciled with Christian grace in conversation?\",\n    \"How can Christian missions emphasize shared moral principles rather than conversion?\",\n    \"How does food during Christmas vary regionally across India?\",\n    \"How does regional folklore shape Christian storytelling traditions?\",\n    \"How do Indian Christian elders respond to youth questioning of denominational traditions?\",\n    \"How can faith-based education evolve to reach younger, tech-savvy Christians?\",\n    \"How can liturgy adapt to youth participation without losing sacredness?\",\n    \"How do young Christians view service and mission in a modern Indian context?\"\n]\n\nfor i, question in enumerate(test_questions, 1):\n    print(f\"\\n[Test {i}]\")\n    print(f\"Question: {question}\")\n    \n    # Format prompt\n    prompt = f\"### Question:\\n{question}\\n\\n### Answer:\\n\"\n    \n    # Tokenize\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    # Generate\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            temperature=0.7,\n            do_sample=True,\n            top_p=0.9,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    # Decode\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Extract only the answer part (after \"### Answer:\")\n    answer = response.split(\"### Answer:\")[-1].strip()\n    \n    print(f\"Answer: {answer}\")\n    print(\"-\"*60)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Inference test complete!\")\nprint(\"=\"*60)\n\n\n\n5.3.2.2 Constitutions Dataset:\n# Constitutions Model Testing Bash Script\n\n#!/bin/bash\n#SBATCH --job-name=test_const_model\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=32G\n#SBATCH --gres=gpu:1\n#SBATCH --time=1:00:00\n#SBATCH --output=logs/test_const_%j.out\n#SBATCH --error=logs/test_const_%j.err\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=starwill16@gmail.com\n\nmodule purge\nmodule load Python/3.12.3-GCCcore-13.3.0\nmodule load CUDA/12.3.0\n\nsource /home/gcsc563_01/CSC463/conlingo/llama_env/bin/activate\n\nexport HF_HOME=/home/gcsc563_01/.cache/huggingface\nexport HUGGINGFACE_HUB_CACHE=/home/gcsc563_01/.cache/huggingface\n\ncd /home/gcsc563_01/conlingo\n\npython scripts/training/run_constitutions_model.py\n\ndeactivate\n# Constitutions Model Testing Python Script\n\n#!/usr/bin/env python3\n\"\"\"\nTest the fine-tuned Constitutions model\nLoads the model and runs sample inference\n\"\"\"\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\nprint(\"=\"*60)\nprint(\"Constitutions Model Inference Test\")\nprint(\"=\"*60)\n\n# Paths\nBASE_MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nFINETUNED_MODEL_PATH = \"/home/gcsc563_01/conlingo/models/finetuned/constitution/final_model\"\n\nprint(f\"\\n1. Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(FINETUNED_MODEL_PATH)\nprint(f\"✅ Tokenizer loaded\")\n\nprint(f\"\\n2. Loading base model...\")\nbase_model = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\nprint(f\"✅ Base model loaded\")\n\nprint(f\"\\n3. Loading fine-tuned LoRA adapters...\")\nmodel = PeftModel.from_pretrained(base_model, FINETUNED_MODEL_PATH)\nprint(f\"✅ Fine-tuned model loaded\")\n\nprint(f\"\\n4. Running test inference...\")\nprint(\"-\"*60)\n\n# Test questions\ntest_questions = [\n    \"How can Diwali and Christmas both symbolize the victory of light over darkness in their respective traditions?\",\n    \"What themes of renewal in Diwali resonate with the idea of rebirth in Christian theology?\",\n    \"How might Indian Christians use Diwali to express biblical messages of hope?\",\n    \"How can Christian schools acknowledge Diwali without compromising faith boundaries?\",\n    \"What sensitivities should pastors consider when mentioning Hindu deities in Christmas homilies?\",\n    \"How can Christian youth groups create shared Diwali–Christmas community service projects?\",\n    \"How can churches ensure caste-neutral seating and participation during worship?\",\n    \"What examples of caste inclusion can be found in the life of Jesus?\",\n    \"What are sensitive ways to discuss “the least of these” without reinforcing caste bias?\",\n    \"Why might some Christians still use caste surnames, and how should this be discussed?\",\n    \"What is a sensitive way to discuss Jesus’ association with marginalized groups?\",\n    \"What is an inclusive way to discuss Krishna and Christ comparisons in academia?\",\n    \"How can Hindu concepts of karma be reconciled with Christian grace in conversation?\",\n    \"How can Christian missions emphasize shared moral principles rather than conversion?\",\n    \"How does food during Christmas vary regionally across India?\",\n    \"How does regional folklore shape Christian storytelling traditions?\",\n    \"How do Indian Christian elders respond to youth questioning of denominational traditions?\",\n    \"How can faith-based education evolve to reach younger, tech-savvy Christians?\",\n    \"How can liturgy adapt to youth participation without losing sacredness?\",\n    \"How do young Christians view service and mission in a modern Indian context?\"\n]\n\nfor i, question in enumerate(test_questions, 1):\n    print(f\"\\n[Test {i}]\")\n    print(f\"Question: {question}\")\n    \n    # Format prompt\n    prompt = f\"### Question:\\n{question}\\n\\n### Answer:\\n\"\n    \n    # Tokenize\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    # Generate\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            temperature=0.7,\n            do_sample=True,\n            top_p=0.9,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    # Decode\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Extract only the answer part (after \"### Answer:\")\n    answer = response.split(\"### Answer:\")[-1].strip()\n    \n    print(f\"Answer: {answer}\")\n    print(\"-\"*60)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Inference test complete!\")\nprint(\"=\"*60)\n\n\n\n\n5.3.3 High Five Testing\nAfter testing the other 2 models, my final requirement was to combine all datasets into one, creating what I called the “High Five” dataset, fine-tuning a model for it, and testing it against Rohan’s questions one more time.\n\n5.3.3.1 Fine-Tuning Model:\n# High Five Fine-Tuning Model Bash Script\n\n#!/bin/bash\n#SBATCH --job-name=high_five_finetune\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=64G\n#SBATCH --gres=gpu:1\n#SBATCH --time=6:00:00\n#SBATCH --output=logs/high_finetune_%j.out\n#SBATCH --error=logs/high_finetune_%j.err\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=starwill16@gmail.com\n\nmodule purge\nmodule load Python/3.12.3-GCCcore-13.3.0\nmodule load CUDA/12.3.0\n\nsource /home/gcsc563_01/CSC463/conlingo/llama_env/bin/activate\n\nexport HF_HOME=/home/gcsc563_01/.cache/huggingface\nexport HUGGINGFACE_HUB_CACHE=/home/gcsc563_01/.cache/huggingface\n\ncd /home/gcsc563_01/conlingo\n\npython scripts/training/finetune_high_five.py\ndeactivate\n# High Five Fine-Tuning Model Python Script\n\n#!/usr/bin/env python3\n\"\"\"\nLoRA Fine-Tuning for all 5 Datasets\nFine-tunes LLaMA-3 8B on all 5 Datasets' Q&A pairs\n\"\"\"\n\nimport torch\nimport json\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling\n)\nfrom peft import LoraConfig, get_peft_model\nfrom datasets import Dataset\nimport os\nfrom sklearn.model_selection import train_test_split\n\nprint(\"=\"*60)\nprint(\"High Five LoRA Fine-Tuning Pipeline\")\nprint(\"=\"*60)\n\n# Paths\nDATA_PATH = \"/home/gcsc563_01/conlingo/data/raw/highfive_qa.jsonl\"\nOUTPUT_DIR = \"/home/gcsc563_01/conlingo/models/finetuned/high_five\"\nMODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\n# Create output directory\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"\\n1. Loading data from: {DATA_PATH}\")\n\n# Load JSONL data\ndata = []\nwith open(DATA_PATH, 'r', encoding='utf-8') as f:\n    for line in f:\n        data.append(json.loads(line))\n\nprint(f\"Total examples: {len(data)}\")\n\n# Extract question-answer pairs\nexamples = []\nfor item in data:\n    examples.append({\n        \"question\": item[\"question\"],\n        \"answer\": item[\"answer\"]\n    })\n\n# Split into train/validation (90/10)\ntrain_data, val_data = train_test_split(examples, test_size=0.1, random_state=42)\n\nprint(f\"Training examples: {len(train_data)}\")\nprint(f\"Validation examples: {len(val_data)}\")\n\nprint(f\"\\n2. Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nprint(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n\nprint(f\"\\n3. Preparing datasets...\")\n\ndef format_instruction(example):\n    \"\"\"Format question-answer pair for training\"\"\"\n    text = f\"### Question:\\n{example['question']}\\n\\n### Answer:\\n{example['answer']}\"\n    return text\n\ndef tokenize_function(example):\n    \"\"\"Tokenize examples with padding and truncation\"\"\"\n    text = format_instruction(example)\n    \n    tokenized = tokenizer(\n        text,\n        truncation=True,\n        max_length=512,\n        padding=\"max_length\",\n        return_tensors=None\n    )\n    \n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized\n\n# Convert to HuggingFace Dataset format\ntrain_dataset = Dataset.from_list(train_data)\nval_dataset = Dataset.from_list(val_data)\n\n# Tokenize\nprint(\"Tokenizing training data...\")\ntrain_dataset = train_dataset.map(\n    tokenize_function,\n    remove_columns=train_dataset.column_names\n)\n\nprint(\"Tokenizing validation data...\")\nval_dataset = val_dataset.map(\n    tokenize_function,\n    remove_columns=val_dataset.column_names\n)\n\nprint(f\"Training dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(val_dataset)}\")\n\nprint(f\"\\n4. Loading base model...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\n\nmodel.gradient_checkpointing_enable()\n\nprint(f\"Model loaded: {model.__class__.__name__}\")\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\nprint(f\"\\n5. Configuring LoRA...\")\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\n\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\n\nprint(f\"LoRA configured successfully\")\nprint(f\"Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.4f}%)\")\nprint(f\"Total parameters: {total_params:,}\")\n\nprint(f\"\\n6. Setting up training arguments...\")\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=16,\n    learning_rate=2e-4,\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=50,\n    logging_steps=10,\n    save_steps=50,\n    eval_steps=50,\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    fp16=True,\n    report_to=\"none\",\n    save_total_limit=2,\n    remove_unused_columns=False,\n    gradient_checkpointing=True\n)\n\nprint(\"Training configuration:\")\nprint(f\"  Epochs: {training_args.num_train_epochs}\")\nprint(f\"  Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  Learning rate: {training_args.learning_rate}\")\n\nprint(f\"\\n7. Initializing trainer...\")\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator\n)\n\nprint(\"Trainer initialized successfully!\")\n\nprint(f\"\\n8. Starting training...\")\nprint(\"=\"*60)\n\ntrainer.train()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Training complete!\")\nprint(\"=\"*60)\n\nprint(f\"\\n9. Saving final model...\")\nmodel.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\ntokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n\nprint(f\"Model saved to: {OUTPUT_DIR}/final_model\")\n\nprint(f\"\\n10. Final evaluation metrics:\")\neval_results = trainer.evaluate()\nfor key, value in eval_results.items():\n    print(f\"  {key}: {value:.4f}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Fine-tuning pipeline complete!\")\nprint(\"=\"*60)\n\n\n5.3.3.2 Model Testing:\n# High Five Model Testing Bash Script\n\n#!/bin/bash\n#SBATCH --job-name=test_high_five_model\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=32G\n#SBATCH --gres=gpu:1\n#SBATCH --time=1:00:00\n#SBATCH --output=logs/test_high_five_%j.out\n#SBATCH --error=logs/test_high_five_%j.err\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=starwill16@gmail.com\n\nmodule purge\nmodule load Python/3.12.3-GCCcore-13.3.0\nmodule load CUDA/12.3.0\n\nsource /home/gcsc563_01/CSC463/conlingo/llama_env/bin/activate\n\nexport HF_HOME=/home/gcsc563_01/.cache/huggingface\nexport HUGGINGFACE_HUB_CACHE=/home/gcsc563_01/.cache/huggingface\n\ncd /home/gcsc563_01/conlingo\n\npython scripts/training/run_high_five_model.py\n\ndeactivate\n# High Five Model Testing Python Script\n\n#!/usr/bin/env python3\n\"\"\"\nTest the fine-tuned High Five model\nLoads the model and runs sample inference\n\"\"\"\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\nprint(\"=\"*60)\nprint(\"High Five Model Inference Test\")\nprint(\"=\"*60)\n\n# Paths\nBASE_MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nFINETUNED_MODEL_PATH = \"/home/gcsc563_01/conlingo/models/finetuned/high_five/final_model\"\n\nprint(f\"\\n1. Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(FINETUNED_MODEL_PATH)\nprint(f\"✅ Tokenizer loaded\")\n\nprint(f\"\\n2. Loading base model...\")\nbase_model = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\nprint(f\"✅ Base model loaded\")\n\nprint(f\"\\n3. Loading fine-tuned LoRA adapters...\")\nmodel = PeftModel.from_pretrained(base_model, FINETUNED_MODEL_PATH)\nprint(f\"✅ Fine-tuned model loaded\")\n\nprint(f\"\\n4. Running test inference...\")\nprint(\"-\"*60)\n\n# Test questions\ntest_questions = [\n    \"How can Diwali and Christmas both symbolize the victory of light over darkness in their respective traditions?\",\n    \"What themes of renewal in Diwali resonate with the idea of rebirth in Christian theology?\",\n    \"How might Indian Christians use Diwali to express biblical messages of hope?\",\n    \"How can Christian schools acknowledge Diwali without compromising faith boundaries?\",\n    \"What sensitivities should pastors consider when mentioning Hindu deities in Christmas homilies?\",\n    \"How can Christian youth groups create shared Diwali–Christmas community service projects?\",\n    \"How can churches ensure caste-neutral seating and participation during worship?\",\n    \"What examples of caste inclusion can be found in the life of Jesus?\",\n    \"What are sensitive ways to discuss “the least of these” without reinforcing caste bias?\",\n    \"Why might some Christians still use caste surnames, and how should this be discussed?\",\n    \"What is a sensitive way to discuss Jesus’ association with marginalized groups?\",\n    \"What is an inclusive way to discuss Krishna and Christ comparisons in academia?\",\n    \"How can Hindu concepts of karma be reconciled with Christian grace in conversation?\",\n    \"How can Christian missions emphasize shared moral principles rather than conversion?\",\n    \"How does food during Christmas vary regionally across India?\",\n    \"How does regional folklore shape Christian storytelling traditions?\",\n    \"How do Indian Christian elders respond to youth questioning of denominational traditions?\",\n    \"How can faith-based education evolve to reach younger, tech-savvy Christians?\",\n    \"How can liturgy adapt to youth participation without losing sacredness?\",\n    \"How do young Christians view service and mission in a modern Indian context?\"\n]\n\n\nfor i, question in enumerate(test_questions, 1):\n    print(f\"\\n[Test {i}]\")\n    print(f\"Question: {question}\")\n    \n    # Format prompt\n    prompt = f\"### Question:\\n{question}\\n\\n### Answer:\\n\"\n    \n    # Tokenize\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    # Generate\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            temperature=0.7,\n            do_sample=True,\n            top_p=0.9,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    # Decode\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Extract only the answer part (after \"### Answer:\")\n    answer = response.split(\"### Answer:\")[-1].strip()\n    \n    print(f\"Answer: {answer}\")\n    print(\"-\"*60)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Inference test complete!\")\nprint(\"=\"*60)\nNow, after me and Moses’ analyses and fine-tuning process, evrey model is set for grading and comparison with the original ConLingo model.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span>"
    ]
  },
  {
    "objectID": "Week4.html#suwilanji-deliverables",
    "href": "Week4.html#suwilanji-deliverables",
    "title": "5  Week 4: Data Searching, Data Vetting, Dataset Collection, Data Cleaning",
    "section": "5.4 Suwilanji Deliverables",
    "text": "5.4 Suwilanji Deliverables\n\n5.4.1 Overview of the Week 4 Deliverable\nThis week, my deliverable was to provide visualizations of the different models created for each dataset and then visualize their performance, which was evaluated by Rohan. Additionally, examine the datasets that contributed to a mode’s negative performance, if any, alongside Rohan.\n\n\n5.4.2 Week 4 Deliverables:\n\nCreate comprehensive visualizations about the performance of each of the 7 models, as well as the 2 combined models.\nExamine the datasets that contributed to the model’s poor performance and propose potential reasons for this.\nCreate the full presentation that will be used as the Week 4 presentation.\n\n\n5.4.2.1 What Was Accomplished:\n\n5.4.2.1.1 1. Model Visualizations\nIn accordance with the CSI scoring that Rohan provided earlier, he used it to evaluate the model’s performance. This model scores an AI’s response based on accuracy, tone, context, and empathy. This all contributes to the final CSI score.\nThe two models’ results, which were ready for me to visualize this week, were the Constitution Model and the Superstitions Model. I used Excel for this process.\nRohan graded the model’s response on a scale of 20 questions that he also created earlier. Each question received a number for it to be easily plotted:\n\nHow can Diwali and Christmas both symbolize the victory of light over darkness in their respective traditions? \nWhat themes of renewal in Diwali resonate with the idea of rebirth in Christian theology? \nHow might Indian Christians use Diwali to express biblical messages of hope? \nHow can Christian schools acknowledge Diwali without compromising faith boundaries? \nWhat sensitivities should pastors consider when mentioning Hindu deities in Christmas homilies?\nHow can Christian youth groups create shared Diwali–Christmas community service projects? \nHow can churches ensure caste-neutral seating and participation during worship? \nWhat examples of caste inclusion can be found in the life of Jesus? \nWhat are sensitive ways to discuss “the least of these” without reinforcing caste bias? \nWhy might some Christians still use caste surnames, and how should this be discussed? \nWhat is a sensitive way to discuss Jesus’ association with marginalized groups? \nWhat is an inclusive way to discuss Krishna and Christ comparisons in academia? \nHow can Hindu concepts of karma be reconciled with Christian grace in conversation?\nHow can Christian missions emphasize shared moral principles rather than conversion?\nHow does food during Christmas vary regionally across India?  \nHow does regional folklore shape Christian storytelling traditions? \nHow do Indian Christian elders respond to youth questioning of denominational traditions? \nHow can faith-based education evolve to reach younger, tech-savvy Christians? \nHow can liturgy adapt to youth participation without losing sacredness? \nHow do young Christians view service and mission in a modern Indian context? \n\nAccuracy\n\nIn this graph, the Constitution’s model performs better on average, with 5 being a perfect score, with more than half of its answers achieving a perfect score.\nTone\n\nIn this graph, the Constitutions model performs better than the Superstitions data. The Superstitions model has an above average response.\nContext\n\nThis graph shows that the Constitution model has a better understanding of the context of the question and thus answers better.\nEmpathy\n\nThe empathy graph as well has Constitutuions model doing much better than the other model with near perfect scores for each question.\nCSI Score\n\nIt is not surprising that the overall model shows the cons\n\n\n5.4.2.1.2 2. Negative Performing Models\nLimited grading (only Rohan)\nQuestions do not reflect data and this model defaults to base knowledge (Llama model)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span>"
    ]
  },
  {
    "objectID": "Week4.html#model-visualizations",
    "href": "Week4.html#model-visualizations",
    "title": "5  Week 4: Data Searching, Data Vetting, Dataset Collection, Data Cleaning",
    "section": "8.1 1. Model Visualizations",
    "text": "8.1 1. Model Visualizations\nIn accordance with the CSI scoring that Rohan provided earlier, he used it to evaluate the model’s performance. This model scores an AI’s response based on accuracy, tone, context, and empathy. This all contributes to the final CSI score.\nThe two models’ results, which were ready for me to visualize this week, were the Constitution Model and the Superstitions Model. I used Excel for this process.\nRohan graded the model’s response on a scale of 20 questions that he also created earlier. Each question received a number for it to be easily plotted:\n\nHow can Diwali and Christmas both symbolize the victory of light over darkness in their respective traditions? \nWhat themes of renewal in Diwali resonate with the idea of rebirth in Christian theology? \nHow might Indian Christians use Diwali to express biblical messages of hope? \nHow can Christian schools acknowledge Diwali without compromising faith boundaries? \nWhat sensitivities should pastors consider when mentioning Hindu deities in Christmas homilies?\nHow can Christian youth groups create shared Diwali–Christmas community service projects? \nHow can churches ensure caste-neutral seating and participation during worship? \nWhat examples of caste inclusion can be found in the life of Jesus? \nWhat are sensitive ways to discuss “the least of these” without reinforcing caste bias? \nWhy might some Christians still use caste surnames, and how should this be discussed? \nWhat is a sensitive way to discuss Jesus’ association with marginalized groups? \nWhat is an inclusive way to discuss Krishna and Christ comparisons in academia? \nHow can Hindu concepts of karma be reconciled with Christian grace in conversation?\nHow can Christian missions emphasize shared moral principles rather than conversion?\nHow does food during Christmas vary regionally across India?  \nHow does regional folklore shape Christian storytelling traditions? \nHow do Indian Christian elders respond to youth questioning of denominational traditions? \nHow can faith-based education evolve to reach younger, tech-savvy Christians? \nHow can liturgy adapt to youth participation without losing sacredness? \nHow do young Christians view service and mission in a modern Indian context? \n\nAccuracy\n\nIn this graph, the Constitution’s model performs better on average, with 5 being a perfect score, with more than half of its answers achieving a perfect score.\nTone\n\nIn this graph, the Constitutions model performs better than the Superstitions data. The Superstitions model has an above average response.\nContext\n\nThis graph shows that the Constitution model has a better understanding of the context of the question and thus answers better.\nEmpathy\n\nThe empathy graph as well has Constitutuions model doing much better than the other model with near perfect scores for each question.\nCSI Score\n\nIt is not surprising that the overall model shows the cons",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span>"
    ]
  },
  {
    "objectID": "Week4.html#negative-performing-models",
    "href": "Week4.html#negative-performing-models",
    "title": "5  Week 4: Data Searching, Data Vetting, Dataset Collection, Data Cleaning",
    "section": "5.5 2. Negative Performing Models",
    "text": "5.5 2. Negative Performing Models\nFirst and foremost, the fact that Rohan was the only one to grade both the model’s responses meant a heavy bias was present. Although he is from India, his notions of what he sees as accurate are unconsciously influencing his grading.\nAdditionally, the sample size for this model was too small. Only Rohan. Some areas to improve our perception of a negatively performing model include increasing our sample size to gain a better understanding of which model is truly underperforming.\nA major flaw in this grading is that since the models were trained with supervised fine-tuning, they are only able to provide answers based on the fact that the training data, such as that of supervised learning. However, the questions that we are asking the Constitution model do not utilize the knowledge base or dataset of the constitution data, yet the trained model performs better.\nWhat most likely happened is that, although the evaluation questions were unrelated to the SFT knowledge (religious questions versus the Constitution and superstition data), the Constitution-trained model was consistently judged stronger by Rohan. More plainly put, our questions unfortunately did not evaluate knowledge recall but rather cross-domain behavioral transfer. This concept falls under transfer learning, a machine learning technique in which a model trained on one task is repurposed as the foundation for a second task. (GeeksforGeeks, 2025)\nIn this case, the model was trained for the task of analyzing superstition data or constitutional data; however, during evaluation, the questions soliciting a religious answer posed a new task. What happened here is that the training data sets’ style and reasoning picked up during the training session transferred (cross-domain). SFT not only teaches content but also behavioral patterns.\nIt is possible that Rohan agreed more with the Constitution’s style that transferred to the responses being evaluated. I’d imagine this data to be more formal, neutral, and structured. In addition, it may have known how to phrase sensitive topics better, which transferred a style of the model being careful with wording, less biased, and more diplomatic, for example. The superstition’s data may have come across as less credible, as the data contains myths, folklore, and mixes belief and fiction. And thus the style transfer is less academic, more storytelling, and less objective.\nIn conclusion, what would have made this process more effective would have been to have a base model to test against these evaluation questions and truly determine how both models deviate from the base case.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span>"
    ]
  },
  {
    "objectID": "Week5.html",
    "href": "Week5.html",
    "title": "6  Week 5: Compare Fine-tuned model with RAG implementation and ChatGPT-5 & Research Paper",
    "section": "",
    "text": "6.1 Overview\nIn Week 5, the team focused on comparing the fine-tuned models with the RAG implementation and ChatGPT-5, including running a Turing test to evaluate whether the AI responses were indistinguishable from those of a Native Indian Expert. Rohan conducted the survey and integrated deliverables into the research paper, Moses fine-tuned the model using RAG data and prepared for testing, Suwilanji drafted the initial paper and organized participants for the Turing test, and William edited and finalized the research paper.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5: Compare Fine-tuned model with RAG implementation and ChatGPT-5 & Research Paper</span>"
    ]
  },
  {
    "objectID": "Week5.html#rohan-aby-deliverables",
    "href": "Week5.html#rohan-aby-deliverables",
    "title": "6  Week 5: Compare Fine-tuned model with RAG implementation and ChatGPT-5 & Research Paper",
    "section": "6.2 Rohan Aby Deliverables",
    "text": "6.2 Rohan Aby Deliverables\nRohan worked on recuriting multiple to do the longer survey that dealt with the extensive grading proecss. Only his mother completed the task. Then he recruited multiple people from churches in Tulsa, Kuwait and India to fill out the survey that Moses and William designed.\nRohan then worked on the research paper and this qurto book.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5: Compare Fine-tuned model with RAG implementation and ChatGPT-5 & Research Paper</span>"
    ]
  },
  {
    "objectID": "Week5.html#william-richards-deliverables",
    "href": "Week5.html#william-richards-deliverables",
    "title": "6  Week 5: Compare Fine-tuned model with RAG implementation and ChatGPT-5 & Research Paper",
    "section": "6.3 William Richards Deliverables",
    "text": "6.3 William Richards Deliverables\nThis quarto document will be quite short given me and Moses worked on this together and Will was more focused on reaching out to others.\nOur deliverable this week was to form the grading process into a yin and yang, where Rohan’s grading would be individualistic and personal, while ours would be clustered and technical.\nWe chose 3 different models and a short set of questions (courtesy of Rohan) and Moses implemented them into a google form for us to send out to anyone from India. A final grading process that emphasizes and utilizes the cultural diversity of ORU, Wills job was to get this to as many people as possible.\nThe google form requires one to have been an Indian citizen and decently knowledgeable of Indian culture, teseting to see which model Q/A pairs the community at large (via polling) decided was most accurate.\nBetween:\n\nConLingo Base Model (RAG Implementation)\nConLingo 2.0 Model\nChatGPT 5.1\n\nWill used his position on Student Association, he was able to send this survey out to as many people as possible. Along with Moses’, Suwilanji’s, and Rohan’s massive help, we were able to receive 50 responses in 24 hours!\nAfter analyzing it appears that many decided the RAG Implementation was just barely better than ConLingo 2.0 and ChatGPT 5.1. However despite the fact our project might not have been as impactful as we hoped, the journey itself is a fantastic groundwork with a new way to approach improving model’s such as these. By interviewing and surveying more personally and emotionally than simply grading with another AI model.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5: Compare Fine-tuned model with RAG implementation and ChatGPT-5 & Research Paper</span>"
    ]
  },
  {
    "objectID": "Week1.html#moses-deliverables-model-selection-and-environment-setup",
    "href": "Week1.html#moses-deliverables-model-selection-and-environment-setup",
    "title": "2  Week 1: Foundations & Data Collection",
    "section": "2.5 Moses Deliverables: Model Selection and Environment Setup",
    "text": "2.5 Moses Deliverables: Model Selection and Environment Setup\n\n2.5.1 Overview\nWeek 1 focused on establishing the technical foundation for fine-tuning a large language model (LLM) with Indian cultural awareness. The primary deliverables included researching and selecting an appropriate base model, configuring the Titan GPU environment for model training, conducting baseline tests, and documenting technical specifications for the project.\n\n\n2.5.2 Model Selection Process\n\n2.5.2.1 Evaluation Criteria\nThe model selection process evaluated five candidate models based on three critical criteria:\n\nCompatibility with Hardware: The model must fit within the 24.6 GB VRAM available on the NVIDIA A30 GPU\nTraining Feasibility: The model must be trainable within a reasonable timeframe (15-18 hours estimated)\nCultural Reasoning Capability: The model must have sufficient capacity to learn and generate culturally nuanced responses\n\n\n\n2.5.2.2 Candidate Models Comparison\n\n\n\n\n\n\n\n\n\nModel\nDescription\nWhy Consider It?\nWhy NOT Use It?\n\n\n\n\nGPT-2 (1.5B)\nOlder OpenAI model from 2019\nFree, well-documented, easy to train\nToo old - poor at complex cultural reasoning\n\n\nDistilGPT-2\nSmaller, faster version of GPT-2\nVery fast to train, low memory\nToo small - won’t capture cultural nuances\n\n\nIndicBERT\nModel trained on Indian languages\nMade for Indian content\nOnly understands, doesn’t generate well\n\n\nGPT-4o mini (API)\nOpenAI’s newest small model\nVery smart, easy to use via API\nCan’t truly fine-tune it for deep learning\n\n\nLLaMA-3 8B Instruct\nMeta’s powerful open-source model\nPerfect size, good at reasoning, you control it, fits in 24.6 GB GPU\nSELECTED - Best option for our setup\n\n\n\n\n\n2.5.2.3 Selection Rationale\nLLaMA-3 8B Instruct was selected as the optimal base model for the following reasons:\n\nParameter Count: With 8 billion parameters, the model provides sufficient capacity for learning cultural nuances without being prohibitively large\nHardware Compatibility: The model requires approximately 16-17 GB of GPU memory when loaded in half-precision (float16), well within the A30’s 24.6 GB capacity\nOpen-Source License: Meta’s permissive license allows full control over fine-tuning and deployment\nInstruction-Tuned: The “Instruct” variant has been pre-trained to follow instructions, providing a strong foundation for conversational applications\nCommunity Support: Extensive documentation and community resources facilitate troubleshooting and optimization\n\nKey Technical Specifications: - Model Parameters: 8,037,076,992 (8.03 billion) - Architecture: Decoder-only transformer - Precision: Half-precision (float16) for memory efficiency - License: Meta LLaMA 3 Community License\n\n\n\n2.5.3 Titan GPU Environment Setup\n\n2.5.3.1 Hardware Configuration\nThe Titan supercomputer provided the computational infrastructure for this project. The allocated resources included:\nGPU Specifications: - Model: NVIDIA A30 - Memory: 24.6 GB VRAM - Compute Capability: 8.0 - CUDA Version: 12.1\nSystem Specifications: - RAM: 32 GB minimum - Storage: 50 GB for model and checkpoints - CPU: 4 cores for data preprocessing\n\n\n2.5.3.2 Software Environment\nThe environment was configured with the following key components:\nCore Dependencies:\n# PyTorch with CUDA support\ntorch==2.5.1+cu121\n\n# Transformers ecosystem\ntransformers==4.36.0\naccelerate==0.25.0\npeft==0.7.0  # For LoRA fine-tuning\n\n# Data processing\ndatasets==2.15.0\npandas==2.1.0\nLoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning technique that adds small trainable adapter layers to the model while keeping the original weights frozen. This approach reduces memory requirements and training time while maintaining model quality.\n\n\n2.5.3.3 Installation Process\nThe environment setup involved creating a Python virtual environment and installing dependencies:\n# Create virtual environment\npython3 -m venv llama_env\nsource llama_env/bin/activate\n\n# Install PyTorch with CUDA 12.1\npip install torch torchvision torchaudio \\\n    --index-url https://download.pytorch.org/whl/cu121\n\n# Install Transformers and PEFT\npip install transformers accelerate peft bitsandbytes\nThe installation completed successfully, confirming GPU availability and proper CUDA configuration:\n\n\n\nTitan GPU environment setup confirmation\n\n\n\n\n\n2.5.4 Baseline Testing\n\n2.5.4.1 Model Loading Test\nInitial tests verified that the LLaMA-3 8B model could be successfully loaded onto the GPU:\nTest Results: - Tokenizer loading: Successful - Model loading time: ~55 seconds - GPU memory usage: 16.06 GB (65% of available VRAM) - GPU memory cached: 17.23 GB\n\n\n2.5.4.2 LoRA Configuration Test\nLoRA adapters were configured to enable efficient fine-tuning:\nLoRA Parameters: - Rank (r): 16 - Alpha: 32 - Target modules: q_proj, v_proj (query and value projection layers) - Dropout: 0.05 - Trainable parameters: 6,815,744 (0.08% of total parameters)\nThis configuration means only 0.08% of the model’s parameters need to be trained, dramatically reducing memory requirements and training time while maintaining effectiveness.\n\n\n\nBaseline test output showing model loading and LoRA setup\n\n\n\n\n2.5.4.3 Inference Test\nA simple inference test confirmed the model’s ability to generate culturally relevant responses:\nTest Prompt: “Explain the cultural significance of Diwali in India.”\nModel Response: The model successfully generated a coherent explanation of Diwali, describing it as the Festival of Lights celebrated over five days, typically in October or November. The response demonstrated the base model’s existing cultural knowledge and confirmed that the inference pipeline was functioning correctly.\n\n\n\n2.5.5 Technical Specifications Documentation\n\n2.5.5.1 Hardware Requirements\n\n\n\nComponent\nSpecification\n\n\n\n\nGPU Model\nNVIDIA A30\n\n\nGPU Memory\n24.6 GB VRAM\n\n\nGPU Compute\n8.0 capability\n\n\nTraining Time Estimate\n15-18 hours\n\n\n\n\n\n2.5.5.2 Model Architecture\n\n\n\nSpecification\nValue\n\n\n\n\nBase Model\nLLaMA-3 8B Instruct\n\n\nTotal Parameters\n8,037,076,992\n\n\nArchitecture\nDecoder-only transformer\n\n\nContext Window\n8,192 tokens\n\n\n\n\n\n2.5.5.3 Fine-Tuning Configuration\n\n\n\n\n\n\n\n\nParameter\nValue\nExplanation\n\n\n\n\nFine-tuning Method\nLoRA (Low-Rank Adaptation)\nParameter-efficient technique\n\n\nLoRA Rank (r)\n16\nControls adapter size\n\n\nLoRA Alpha\n32\nScaling factor for adapters\n\n\nTarget Modules\nq_proj, v_proj\nQuery and value projections\n\n\nTrainable Parameters\n6,815,744 (0.08%)\nOnly adapters are trained\n\n\nTotal Parameters\n8,043,892,736\nBase + adapter parameters\n\n\n\n\n\n\nTechnical specifications diagram\n\n\n\n\n2.5.5.4 System Configuration\nDirectory Structure:\n/home/mmadale/CSC463/conlingo/\n├── data/\n│   ├── raw/              # Original datasets\n│   ├── processed/        # Cleaned and formatted data\n│   └── test/             # Test sets\n├── models/\n│   ├── base/             # LLaMA-3 base model\n│   ├── finetuned/        # Fine-tuned models\n│   └── checkpoints/      # Training checkpoints\n├── scripts/\n│   ├── setup/            # Environment setup scripts\n│   ├── training/         # Training scripts\n│   └── evaluation/       # Evaluation scripts\n├── logs/                 # Training and job logs\n└── llama_env/            # Python virtual environment\n\n\n\n2.5.6 Key Achievements\nWeek 1 successfully established the technical foundation for the project:\n\nModel Selection: Identified and justified LLaMA-3 8B Instruct as the optimal base model\nEnvironment Configuration: Set up a fully functional GPU environment on Titan with all required dependencies\nBaseline Validation: Confirmed the model loads correctly, uses GPU resources efficiently, and can generate responses\nLoRA Integration: Configured parameter-efficient fine-tuning with only 0.08% of parameters trainable\nDocumentation: Created comprehensive technical specifications for reproducibility\n\n\n\n2.5.7 Challenges and Solutions\nChallenge 1: HuggingFace Authentication - Issue: LLaMA-3 requires accepting Meta’s license agreement through HuggingFace - Solution: Created HuggingFace account, accepted license terms, and configured authentication token\nChallenge 2: GPU Memory Management - Issue: Full-precision model (float32) would exceed 24.6 GB VRAM - Solution: Used half-precision (float16) loading, reducing memory footprint to ~16 GB\nChallenge 3: Training Efficiency - Issue: Fine-tuning all 8 billion parameters would be computationally expensive - Solution: Implemented LoRA adapters, training only 6.8 million parameters (0.08%)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: Foundations & Data Collection</span>"
    ]
  },
  {
    "objectID": "Week1.html#overview-1",
    "href": "Week1.html#overview-1",
    "title": "2  Week 1: Foundations & Data Collection",
    "section": "2.6 Overview",
    "text": "2.6 Overview\nWeek 1 focused on establishing the technical foundation for fine-tuning a large language model (LLM) with Indian cultural awareness. The primary deliverables included researching and selecting an appropriate base model, configuring the Titan GPU environment for model training, conducting baseline tests, and documenting technical specifications for the project.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: Foundations & Data Collection</span>"
    ]
  },
  {
    "objectID": "Week1.html#model-selection-process",
    "href": "Week1.html#model-selection-process",
    "title": "2  Week 1: Foundations & Data Collection",
    "section": "2.7 Model Selection Process",
    "text": "2.7 Model Selection Process\n\n2.7.1 Evaluation Criteria\nThe model selection process evaluated five candidate models based on three critical criteria:\n\nCompatibility with Hardware: The model must fit within the 24.6 GB VRAM available on the NVIDIA A30 GPU\nTraining Feasibility: The model must be trainable within a reasonable timeframe (15-18 hours estimated)\nCultural Reasoning Capability: The model must have sufficient capacity to learn and generate culturally nuanced responses\n\n\n\n2.7.2 Candidate Models Comparison\n\n\n\n\n\n\n\n\n\nModel\nDescription\nWhy Consider It?\nWhy NOT Use It?\n\n\n\n\nGPT-2 (1.5B)\nOlder OpenAI model from 2019\nFree, well-documented, easy to train\nToo old - poor at complex cultural reasoning\n\n\nDistilGPT-2\nSmaller, faster version of GPT-2\nVery fast to train, low memory\nToo small - won’t capture cultural nuances\n\n\nIndicBERT\nModel trained on Indian languages\nMade for Indian content\nOnly understands, doesn’t generate well\n\n\nGPT-4o mini (API)\nOpenAI’s newest small model\nVery smart, easy to use via API\nCan’t truly fine-tune it for deep learning\n\n\nLLaMA-3 8B Instruct\nMeta’s powerful open-source model\nPerfect size, good at reasoning, you control it, fits in 24.6 GB GPU\nSELECTED - Best option for our setup\n\n\n\n\n\n2.7.3 Selection Rationale\nLLaMA-3 8B Instruct was selected as the optimal base model for the following reasons:\n\nParameter Count: With 8 billion parameters, the model provides sufficient capacity for learning cultural nuances without being prohibitively large\nHardware Compatibility: The model requires approximately 16-17 GB of GPU memory when loaded in half-precision (float16), well within the A30’s 24.6 GB capacity\nOpen-Source License: Meta’s permissive license allows full control over fine-tuning and deployment\nInstruction-Tuned: The “Instruct” variant has been pre-trained to follow instructions, providing a strong foundation for conversational applications\nCommunity Support: Extensive documentation and community resources facilitate troubleshooting and optimization\n\nKey Technical Specifications: - Model Parameters: 8,037,076,992 (8.03 billion) - Architecture: Decoder-only transformer - Precision: Half-precision (float16) for memory efficiency - License: Meta LLaMA 3 Community License",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: Foundations & Data Collection</span>"
    ]
  },
  {
    "objectID": "Week1.html#titan-gpu-environment-setup",
    "href": "Week1.html#titan-gpu-environment-setup",
    "title": "2  Week 1: Foundations & Data Collection",
    "section": "2.6 Titan GPU Environment Setup",
    "text": "2.6 Titan GPU Environment Setup\n\n2.6.1 Hardware Configuration\nThe Titan supercomputer provided the computational infrastructure for this project. The allocated resources included:\nGPU Specifications: - Model: NVIDIA A30 - Memory: 24.6 GB VRAM - Compute Capability: 8.0 - CUDA Version: 12.1\nSystem Specifications: - RAM: 32 GB minimum - Storage: 50 GB for model and checkpoints - CPU: 4 cores for data preprocessing\n\n\n2.6.2 Software Environment\nThe environment was configured with the following key components:\nCore Dependencies:\n# PyTorch with CUDA support\ntorch==2.5.1+cu121\n\n# Transformers ecosystem\ntransformers==4.36.0\naccelerate==0.25.0\npeft==0.7.0  # For LoRA fine-tuning\n\n# Data processing\ndatasets==2.15.0\npandas==2.1.0\nLoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning technique that adds small trainable adapter layers to the model while keeping the original weights frozen. This approach reduces memory requirements and training time while maintaining model quality.\n\n\n2.6.3 Installation Process\nThe environment setup involved creating a Python virtual environment and installing dependencies:\n# Create virtual environment\npython3 -m venv llama_env\nsource llama_env/bin/activate\n\n# Install PyTorch with CUDA 12.1\npip install torch torchvision torchaudio \\\n    --index-url https://download.pytorch.org/whl/cu121\n\n# Install Transformers and PEFT\npip install transformers accelerate peft bitsandbytes\nThe installation completed successfully, confirming GPU availability and proper CUDA configuration:\n\n\n\nTitan GPU environment setup confirmation",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: Foundations & Data Collection</span>"
    ]
  },
  {
    "objectID": "Week1.html#baseline-testing",
    "href": "Week1.html#baseline-testing",
    "title": "2  Week 1: Foundations & Data Collection",
    "section": "2.6 Baseline Testing",
    "text": "2.6 Baseline Testing\n\n2.6.1 Model Loading Test\nInitial tests verified that the LLaMA-3 8B model could be successfully loaded onto the GPU:\nTest Results: - Tokenizer loading: Successful - Model loading time: ~55 seconds - GPU memory usage: 16.06 GB (65% of available VRAM) - GPU memory cached: 17.23 GB\n\n\n2.6.2 LoRA Configuration Test\nLoRA adapters were configured to enable efficient fine-tuning:\nLoRA Parameters: - Rank (r): 16 - Alpha: 32 - Target modules: q_proj, v_proj (query and value projection layers) - Dropout: 0.05 - Trainable parameters: 6,815,744 (0.08% of total parameters)\nThis configuration means only 0.08% of the model’s parameters need to be trained, dramatically reducing memory requirements and training time while maintaining effectiveness.\n\n\n\nBaseline test output showing model loading and LoRA setup\n\n\n\n\n2.6.3 Inference Test\nA simple inference test confirmed the model’s ability to generate culturally relevant responses:\nTest Prompt: “Explain the cultural significance of Diwali in India.”\nModel Response: The model successfully generated a coherent explanation of Diwali, describing it as the Festival of Lights celebrated over five days, typically in October or November. The response demonstrated the base model’s existing cultural knowledge and confirmed that the inference pipeline was functioning correctly.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: Foundations & Data Collection</span>"
    ]
  },
  {
    "objectID": "Week1.html#echnical-specifications-documentation",
    "href": "Week1.html#echnical-specifications-documentation",
    "title": "2  Week 1: Foundations & Data Collection",
    "section": "2.6 echnical Specifications Documentation",
    "text": "2.6 echnical Specifications Documentation\n\n2.6.1 Hardware Requirements\n\n\n\nComponent\nSpecification\n\n\n\n\nGPU Model\nNVIDIA A30\n\n\nGPU Memory\n24.6 GB VRAM\n\n\nGPU Compute\n8.0 capability\n\n\nTraining Time Estimate\n15-18 hours\n\n\n\n\n\n2.6.2 Model Architecture\n\n\n\nSpecification\nValue\n\n\n\n\nBase Model\nLLaMA-3 8B Instruct\n\n\nTotal Parameters\n8,037,076,992\n\n\nArchitecture\nDecoder-only transformer\n\n\nContext Window\n8,192 tokens\n\n\n\n\n\n2.6.3 Fine-Tuning Configuration\n\n\n\n\n\n\n\n\nParameter\nValue\nExplanation\n\n\n\n\nFine-tuning Method\nLoRA (Low-Rank Adaptation)\nParameter-efficient technique\n\n\nLoRA Rank (r)\n16\nControls adapter size\n\n\nLoRA Alpha\n32\nScaling factor for adapters\n\n\nTarget Modules\nq_proj, v_proj\nQuery and value projections\n\n\nTrainable Parameters\n6,815,744 (0.08%)\nOnly adapters are trained\n\n\nTotal Parameters\n8,043,892,736\nBase + adapter parameters\n\n\n\n\n\n\nTechnical specifications diagram\n\n\n\n\n2.6.4 System Configuration\nDirectory Structure:\n/home/mmadale/CSC463/conlingo/\n├── data/\n│   ├── raw/              # Original datasets\n│   ├── processed/        # Cleaned and formatted data\n│   └── test/             # Test sets\n├── models/\n│   ├── base/             # LLaMA-3 base model\n│   ├── finetuned/        # Fine-tuned models\n│   └── checkpoints/      # Training checkpoints\n├── scripts/\n│   ├── setup/            # Environment setup scripts\n│   ├── training/         # Training scripts\n│   └── evaluation/       # Evaluation scripts\n├── logs/                 # Training and job logs\n└── llama_env/            # Python virtual environment",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: Foundations & Data Collection</span>"
    ]
  },
  {
    "objectID": "Week1.html#key-achievements",
    "href": "Week1.html#key-achievements",
    "title": "2  Week 1: Foundations & Data Collection",
    "section": "2.6 Key Achievements",
    "text": "2.6 Key Achievements\nWeek 1 successfully established the technical foundation for the project:\n\nModel Selection: Identified and justified LLaMA-3 8B Instruct as the optimal base model\nEnvironment Configuration: Set up a fully functional GPU environment on Titan with all required dependencies\nBaseline Validation: Confirmed the model loads correctly, uses GPU resources efficiently, and can generate responses\nLoRA Integration: Configured parameter-efficient fine-tuning with only 0.08% of parameters trainable\nDocumentation: Created comprehensive technical specifications for reproducibility",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: Foundations & Data Collection</span>"
    ]
  },
  {
    "objectID": "Week1.html#challenges-and-solutions",
    "href": "Week1.html#challenges-and-solutions",
    "title": "2  Week 1: Foundations & Data Collection",
    "section": "2.7 Challenges and Solutions",
    "text": "2.7 Challenges and Solutions\nChallenge 1: HuggingFace Authentication - Issue: LLaMA-3 requires accepting Meta’s license agreement through HuggingFace - Solution: Created HuggingFace account, accepted license terms, and configured authentication token\nChallenge 2: GPU Memory Management - Issue: Full-precision model (float32) would exceed 24.6 GB VRAM - Solution: Used half-precision (float16) loading, reducing memory footprint to ~16 GB\nChallenge 3: Training Efficiency - Issue: Fine-tuning all 8 billion parameters would be computationally expensive - Solution: Implemented LoRA adapters, training only 6.8 million parameters (0.08%)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: Foundations & Data Collection</span>"
    ]
  },
  {
    "objectID": "Week2.html#moses-deliverablesfine-tuning-pipeline-development",
    "href": "Week2.html#moses-deliverablesfine-tuning-pipeline-development",
    "title": "3  Week 2: Data Cleaning & Pipeline Setup",
    "section": "6.5 Moses Deliverables:Fine-Tuning Pipeline Development",
    "text": "6.5 Moses Deliverables:Fine-Tuning Pipeline Development",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2: Data Cleaning & Pipeline Setup</span>"
    ]
  },
  {
    "objectID": "Week2.html#overview-1",
    "href": "Week2.html#overview-1",
    "title": "3  Week 2: Data Cleaning & Pipeline Setup",
    "section": "3.7 Overview",
    "text": "3.7 Overview\nWeek 2 focused on developing the complete fine-tuning pipeline for the LLaMA-3 8B model. The primary deliverables included creating data preprocessing scripts, implementing the LoRA fine-tuning workflow, training the first model on Indian superstition data, and validating the training process. This week transformed the configured environment from Week 1 into a functional training system.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2: Data Cleaning & Pipeline Setup</span>"
    ]
  },
  {
    "objectID": "Week2.html#fine-tuning-pipeline-architecture",
    "href": "Week2.html#fine-tuning-pipeline-architecture",
    "title": "3  Week 2: Data Cleaning & Pipeline Setup",
    "section": "3.8 Fine-Tuning Pipeline Architecture",
    "text": "3.8 Fine-Tuning Pipeline Architecture\n\n3.8.1 Pipeline Overview\nThe fine-tuning pipeline consists of two main stages: data preprocessing and model training. The architecture was designed to be modular, allowing for easy iteration on different datasets while maintaining consistent formatting and training procedures.\n\n\n\nFine-tuning pipeline architecture\n\n\nPipeline Stages:\n\nData Preprocessing (preprocess_data.py): Converts raw CSV data into instruction-response format suitable for LLaMA-3 fine-tuning\nModel Fine-Tuning (finetune_lora.py): Trains LoRA adapters on the preprocessed data while keeping base model weights frozen\n\n\n\n3.8.2 Data Flow\nRaw CSV (Indian Superstitions)\n    ↓\npreprocess_data.py\n    ↓\nJSON instruction-response pairs\n    ↓\nTrain/Validation Split (90/10)\n    ↓\nfinetune_lora.py\n    ↓\nFine-tuned Model with LoRA Adapters",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2: Data Cleaning & Pipeline Setup</span>"
    ]
  },
  {
    "objectID": "Week2.html#data-preprocessing",
    "href": "Week2.html#data-preprocessing",
    "title": "3  Week 2: Data Cleaning & Pipeline Setup",
    "section": "3.6 Data Preprocessing",
    "text": "3.6 Data Preprocessing\n\n3.6.1 Input Dataset\nThe preprocessing pipeline began with a dataset of Indian superstitions collected by team member Suwilanji from Kaggle. The raw dataset contained:\n\nTotal Rows: 660 entries\nKey Columns: superstition_name, description, region, category, origin_theory, modern_status, is_harmful, source, user_contributed\nFormat: CSV file\n\n\n\n3.6.2 Preprocessing Script Design\nThe preprocess_data.py script implements a systematic transformation from raw tabular data to instruction-response format suitable for LLM fine-tuning.\nKey Processing Steps:\n\nColumn Detection: Automatically identifies name and description columns\nData Cleaning: Filters out invalid entries (null values, insufficient length)\nUnicode Normalization: Converts Unicode quotation marks to ASCII equivalents\nQuestion Generation: Transforms superstition names into natural questions\nJSON Structuring: Creates instruction-response pairs with metadata\nTrain/Validation Split: Implements 90/10 split with random shuffling\n\nData Transformation Example:\n# Input (CSV row)\nsuperstition_name: \"Mirror breaks, 7 years bad luck\"\ndescription: \"Believed to bring misfortune if a mirror breaks.\"\n\n# Output (JSON)\n{\n  \"instruction\": \"What is the cultural significance and meaning of the superstition: 'Mirror breaks, 7 years bad luck'?\",\n  \"response\": \"Believed to bring misfortune if a mirror breaks.\",\n  \"metadata\": {\n    \"source\": \"superstition_dataset\",\n    \"category\": \"cultural_beliefs\",\n    \"original_name\": \"Mirror breaks, 7 years bad luck\"\n  }\n}\n\n\n\nData preprocessing transformation from CSV to JSON\n\n\n\n\n3.6.3 Preprocessing Results\nThe preprocessing pipeline successfully transformed the raw dataset:\nInput Statistics: - Raw CSV entries: 660 - Valid entries after filtering: 658\nOutput Statistics: - Training examples: 592 (90%) - Validation examples: 66 (10%) - Total processed: 658\nData Quality Measures: - Filtered entries: 2 (entries with null values or insufficient text length) - Unicode normalization: Applied to all entries for ASCII compatibility - Question format: Consistent across all examples",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2: Data Cleaning & Pipeline Setup</span>"
    ]
  },
  {
    "objectID": "Week2.html#fine-tuning-implementation",
    "href": "Week2.html#fine-tuning-implementation",
    "title": "3  Week 2: Data Cleaning & Pipeline Setup",
    "section": "3.7 Fine-Tuning Implementation",
    "text": "3.7 Fine-Tuning Implementation\n\n3.7.1 Training Configuration\nThe fine-tuning script (finetune_lora.py) implements LoRA-based parameter-efficient fine-tuning with the following configuration:\nLoRA Hyperparameters:\n\n\n\n\n\n\n\n\nParameter\nValue\nPurpose\n\n\n\n\nRank (r)\n16\nDimensionality of adapter matrices\n\n\nAlpha\n32\nScaling factor for adapter outputs\n\n\nTarget Modules\nq_proj, v_proj, k_proj, o_proj\nAttention projection layers\n\n\nDropout\n0.05\nRegularization to prevent overfitting\n\n\nBias\nNone\nNo additional bias terms in adapters\n\n\n\nTraining Hyperparameters:\n\n\n\nParameter\nValue\nExplanation\n\n\n\n\nEpochs\n3\nNumber of passes through training data\n\n\nPer-device Batch Size\n4\nExamples processed simultaneously\n\n\nGradient Accumulation Steps\n8\nEffective batch size = 4 × 8 = 32\n\n\nLearning Rate\n2e-4\nStep size for weight updates\n\n\nMax Sequence Length\n512\nMaximum tokens per example\n\n\nEvaluation Strategy\nSteps\nEvaluate periodically during training\n\n\nSave Steps\n100\nCheckpoint frequency\n\n\n\nEffective Training Configuration: - Effective batch size: 32 examples per update - Total training steps: ~54 steps (592 examples / 32 batch size × 3 epochs) - Gradient checkpointing: Enabled for memory efficiency\n\n\n3.7.2 Model Architecture\nThe fine-tuning process adds LoRA adapters to specific layers of the LLaMA-3 8B model:\nParameter Breakdown: - Base model parameters (frozen): 8,030,261,248 - LoRA trainable parameters: 13,631,488 - Total parameters: 8,043,892,736 - Trainable percentage: 0.17%\nThis parameter-efficient approach enables training with limited GPU memory while maintaining model quality.\n\n\n\nTraining configuration output\n\n\n\n\n3.7.3 Tokenization and Data Formatting\nThe training script implements a custom formatting function to structure data for instruction-following:\ndef format_instruction(example):\n    \"\"\"Format question-answer pair for training\"\"\"\n    text = f\"### Question:\\n{example['instruction']}\\n\\n### Answer:\\n{example['response']}\"\n    return text\nThis format provides clear delineation between the instruction (question) and expected response, helping the model learn the question-answering structure.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2: Data Cleaning & Pipeline Setup</span>"
    ]
  },
  {
    "objectID": "Week2.html#training-process-and-results",
    "href": "Week2.html#training-process-and-results",
    "title": "3  Week 2: Data Cleaning & Pipeline Setup",
    "section": "3.8 Training Process and Results",
    "text": "3.8 Training Process and Results\n\n3.8.1 Training Progress\nThe model was trained for 3 epochs on the Indian superstition dataset. The training process exhibited expected behavior with decreasing loss values:\nLoss Progression:\n\n\n\nEpoch\nTraining Loss\nGradient Norm\nLearning Rate\n\n\n\n\n0.6\n3.4506\n1.9107\n3.6e-05\n\n\n1.18\n3.0625\n1.4718\n7.6e-05\n\n\n1.78\n2.5055\n1.6695\n1.16e-04\n\n\n2.36\n2.1748\n0.9833\n1.56e-04\n\n\n2.96\n2.0223\n1.2655\n1.96e-04\n\n\n\nFinal Metrics: - Training loss: 2.6282 - Validation loss: 2.0647 - Training time: Approximately 10 minutes - GPU memory usage: ~16 GB during training\n\n\n\nTraining progress showing loss decrease over epochs\n\n\n\n\n3.8.2 Loss Analysis\nThe training loss decreased from 3.45 to 2.02, indicating successful learning:\n\nInitial loss (3.45): Model has limited knowledge of the specific cultural content\nMid-training (2.51 at epoch 1.78): Model begins recognizing patterns in superstition descriptions\nFinal loss (2.02): Model has learned to generate appropriate responses for cultural questions\n\nThe validation loss of 2.06 is slightly higher than the final training loss, indicating minimal overfitting. This small gap suggests the model generalizes reasonably well to unseen examples.\n\n\n3.8.3 Model Checkpointing\nThe fine-tuning pipeline automatically saves model checkpoints:\nSaved Artifacts: - Final model: /home/mmadale/CSC463/conlingo/models/finetuned/final_model/ - Checkpoint size: Approximately 1.3 GB (includes LoRA adapters only) - Configuration files: adapter_config.json, adapter_model.bin\nThe saved model contains only the LoRA adapters, not the full base model. This significantly reduces storage requirements while preserving the fine-tuning results.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2: Data Cleaning & Pipeline Setup</span>"
    ]
  },
  {
    "objectID": "Week2.html#code-implementation-highlights",
    "href": "Week2.html#code-implementation-highlights",
    "title": "3  Week 2: Data Cleaning & Pipeline Setup",
    "section": "3.9 Code Implementation Highlights",
    "text": "3.9 Code Implementation Highlights\n\n3.9.1 Data Preprocessing\nKey implementation details from preprocess_data.py:\n# Unicode normalization for ASCII compatibility\nsuperstition_name = superstition_name.replace('\\u2019', \"'\").replace('\\u2018', \"'\")\nsuperstition_name = superstition_name.replace('\\u201c', '\"').replace('\\u201d', '\"')\n\n# Question generation\nquestion = f\"What is the cultural significance and meaning of the superstition: '{superstition_name}'?\"\n\n# Structured output\ninstruction_response = {\n    \"instruction\": question,\n    \"response\": description,\n    \"metadata\": {\n        \"source\": \"superstition_dataset\",\n        \"category\": \"cultural_beliefs\",\n        \"original_name\": superstition_name\n    }\n}\n\n\n3.9.2 Fine-Tuning Configuration\nKey implementation details from finetune_lora.py:\n# LoRA configuration\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=8,\n    learning_rate=2e-4,\n    evaluation_strategy=\"steps\",\n    save_steps=100\n)\n\n\n\nCode snippet showing training configuration",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2: Data Cleaning & Pipeline Setup</span>"
    ]
  },
  {
    "objectID": "Week2.html#technical-challenges-and-solutions",
    "href": "Week2.html#technical-challenges-and-solutions",
    "title": "3  Week 2: Data Cleaning & Pipeline Setup",
    "section": "3.10 Technical Challenges and Solutions",
    "text": "3.10 Technical Challenges and Solutions\n\n3.10.1 Memory Management\nChallenge: Training with batch size of 8 caused out-of-memory errors on the 24.6 GB GPU.\nSolution: Implemented gradient accumulation with batch size 4 and accumulation steps 8, achieving effective batch size of 32 while staying within memory constraints.\n\n\n3.10.2 Unicode Character Handling\nChallenge: Original dataset contained Unicode quotation marks (e.g., \\u2019, \\u201c) that could cause tokenization issues.\nSolution: Implemented systematic Unicode-to-ASCII conversion in preprocessing script, ensuring consistent character encoding throughout the dataset.\n\n\n3.10.3 Data Quality\nChallenge: Some CSV entries contained null values or insufficient text.\nSolution: Added validation checks in preprocessing script to filter entries with: - Null/missing superstition names or descriptions - Superstition names shorter than 3 characters - Descriptions shorter than 10 characters",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2: Data Cleaning & Pipeline Setup</span>"
    ]
  },
  {
    "objectID": "Week2.html#validation-and-quality-assurance",
    "href": "Week2.html#validation-and-quality-assurance",
    "title": "3  Week 2: Data Cleaning & Pipeline Setup",
    "section": "3.11 Validation and Quality Assurance",
    "text": "3.11 Validation and Quality Assurance\n\n3.11.1 Data Validation\nThe preprocessing script includes multiple validation steps:\n\nColumn Detection: Automatically identifies relevant columns, adapting to CSV structure\nNull Filtering: Removes entries with missing critical information\nLength Validation: Ensures minimum content length for meaningful training\nUnicode Normalization: Standardizes character encoding\n\nValidation Results: - Entries processed: 660 - Entries passed validation: 658 (99.7% retention rate) - Entries filtered: 2 (0.3%)\n\n\n3.11.2 Training Validation\nThe training process includes continuous validation:\n\nGradient Norm Monitoring: Tracked to detect instability (all values &lt; 2.0, indicating stable training)\nValidation Loss: Evaluated every 100 steps to monitor generalization\nCheckpoint Saving: Automatic preservation of model state for recovery",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2: Data Cleaning & Pipeline Setup</span>"
    ]
  },
  {
    "objectID": "Week2.html#key-achievements",
    "href": "Week2.html#key-achievements",
    "title": "3  Week 2: Data Cleaning & Pipeline Setup",
    "section": "3.12 Key Achievements",
    "text": "3.12 Key Achievements\nWeek 2 successfully delivered a complete fine-tuning pipeline and initial trained model:\n\nData Preprocessing Pipeline: Created robust script converting CSV to instruction-response format\n658 Training Examples: Processed and validated dataset of Indian superstitions\nFine-Tuning Script: Implemented LoRA-based training with proper configuration\nTrained Model: Successfully fine-tuned LLaMA-3 8B on cultural content\nLoss Reduction: Achieved 41% decrease in training loss (3.45 → 2.02)\nModel Artifacts: Generated reusable LoRA adapters (~1.3 GB) for inference\nReproducible Pipeline: Created modular scripts adaptable to new datasets",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2: Data Cleaning & Pipeline Setup</span>"
    ]
  },
  {
    "objectID": "Week2.html#lessons-learned",
    "href": "Week2.html#lessons-learned",
    "title": "3  Week 2: Data Cleaning & Pipeline Setup",
    "section": "3.13 Lessons Learned",
    "text": "3.13 Lessons Learned\n\n3.13.1 Pipeline Design\nThe modular pipeline architecture proved effective for rapid iteration. Separating preprocessing and training into distinct scripts allowed independent testing and debugging of each component.\n\n\n3.13.2 Batch Size Optimization\nFinding the optimal batch size required balancing: - GPU memory constraints (24.6 GB available) - Training stability (larger batches → more stable gradients) - Training speed (larger batches → fewer updates per epoch)\nThe final configuration (batch size 4 with 8 accumulation steps) effectively balanced these factors.\n\n\n3.13.3 Data Quality Impact\nThe high validation rate (99.7% of entries passed filtering) indicated good initial data quality. Manual inspection of filtered entries confirmed they were legitimately problematic (null values), validating the filtering criteria.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2: Data Cleaning & Pipeline Setup</span>"
    ]
  },
  {
    "objectID": "Week2.html#moses-deliverables-fine-tuning-pipeline-development",
    "href": "Week2.html#moses-deliverables-fine-tuning-pipeline-development",
    "title": "3  Week 2: Data Cleaning & Pipeline Setup",
    "section": "3.5 Moses Deliverables: Fine-Tuning Pipeline Development",
    "text": "3.5 Moses Deliverables: Fine-Tuning Pipeline Development\n\n3.5.1 Overview\nWeek 2 focused on developing the complete fine-tuning pipeline for the LLaMA-3 8B model. The primary deliverables included creating data preprocessing scripts, implementing the LoRA fine-tuning workflow, training the first model on Indian superstition data, and validating the training process. This week transformed the configured environment from Week 1 into a functional training system.\n\n\n3.5.2 Fine-Tuning Pipeline Architecture\n\n3.5.2.1 Pipeline Overview\nThe fine-tuning pipeline consists of two main stages: data preprocessing and model training. The architecture was designed to be modular, allowing for easy iteration on different datasets while maintaining consistent formatting and training procedures.\n\n\n\nFine-tuning pipeline architecture\n\n\nPipeline Stages:\n\nData Preprocessing (preprocess_data.py): Converts raw CSV data into instruction-response format suitable for LLaMA-3 fine-tuning\nModel Fine-Tuning (finetune_lora.py): Trains LoRA adapters on the preprocessed data while keeping base model weights frozen\n\n\n\n\n3.5.3 Data Flow\nRaw CSV (Indian Superstitions)\n    ↓\npreprocess_data.py\n    ↓\nJSON instruction-response pairs\n    ↓\nTrain/Validation Split (90/10)\n    ↓\nfinetune_lora.py\n    ↓\nFine-tuned Model with LoRA Adapters\n\n\n3.5.4 Data Preprocessing\n\n3.5.4.1 Input Dataset\nThe preprocessing pipeline began with a dataset of Indian superstitions collected by team member Suwilanji from Kaggle. The raw dataset contained:\n\nTotal Rows: 660 entries\nKey Columns: superstition_name, description, region, category, origin_theory, modern_status, is_harmful, source, user_contributed\nFormat: CSV file\n\n\n\n3.5.4.2 Preprocessing Script Design\nThe preprocess_data.py script implements a systematic transformation from raw tabular data to instruction-response format suitable for LLM fine-tuning.\nKey Processing Steps:\n\nColumn Detection: Automatically identifies name and description columns\nData Cleaning: Filters out invalid entries (null values, insufficient length)\nUnicode Normalization: Converts Unicode quotation marks to ASCII equivalents\nQuestion Generation: Transforms superstition names into natural questions\nJSON Structuring: Creates instruction-response pairs with metadata\nTrain/Validation Split: Implements 90/10 split with random shuffling\n\nData Transformation Example:\n# Input (CSV row)\nsuperstition_name: \"Mirror breaks, 7 years bad luck\"\ndescription: \"Believed to bring misfortune if a mirror breaks.\"\n\n# Output (JSON)\n{\n  \"instruction\": \"What is the cultural significance and meaning of the superstition: 'Mirror breaks, 7 years bad luck'?\",\n  \"response\": \"Believed to bring misfortune if a mirror breaks.\",\n  \"metadata\": {\n    \"source\": \"superstition_dataset\",\n    \"category\": \"cultural_beliefs\",\n    \"original_name\": \"Mirror breaks, 7 years bad luck\"\n  }\n}\n\n\n\nData preprocessing transformation from CSV to JSON\n\n\n\n\n3.5.4.3 Preprocessing Results\nThe preprocessing pipeline successfully transformed the raw dataset:\nInput Statistics: - Raw CSV entries: 660 - Valid entries after filtering: 658\nOutput Statistics: - Training examples: 592 (90%) - Validation examples: 66 (10%) - Total processed: 658\nData Quality Measures: - Filtered entries: 2 (entries with null values or insufficient text length) - Unicode normalization: Applied to all entries for ASCII compatibility - Question format: Consistent across all examples\n\n\n\n3.5.5 Fine-Tuning Implementation\n\n3.5.5.1 Training Configuration\nThe fine-tuning script (finetune_lora.py) implements LoRA-based parameter-efficient fine-tuning with the following configuration:\nLoRA Hyperparameters:\n\n\n\n\n\n\n\n\nParameter\nValue\nPurpose\n\n\n\n\nRank (r)\n16\nDimensionality of adapter matrices\n\n\nAlpha\n32\nScaling factor for adapter outputs\n\n\nTarget Modules\nq_proj, v_proj, k_proj, o_proj\nAttention projection layers\n\n\nDropout\n0.05\nRegularization to prevent overfitting\n\n\nBias\nNone\nNo additional bias terms in adapters\n\n\n\nTraining Hyperparameters:\n\n\n\nParameter\nValue\nExplanation\n\n\n\n\nEpochs\n3\nNumber of passes through training data\n\n\nPer-device Batch Size\n4\nExamples processed simultaneously\n\n\nGradient Accumulation Steps\n8\nEffective batch size = 4 × 8 = 32\n\n\nLearning Rate\n2e-4\nStep size for weight updates\n\n\nMax Sequence Length\n512\nMaximum tokens per example\n\n\nEvaluation Strategy\nSteps\nEvaluate periodically during training\n\n\nSave Steps\n100\nCheckpoint frequency\n\n\n\nEffective Training Configuration: - Effective batch size: 32 examples per update - Total training steps: ~54 steps (592 examples / 32 batch size × 3 epochs) - Gradient checkpointing: Enabled for memory efficiency\n\n\n\n3.5.6 Model Architecture\nThe fine-tuning process adds LoRA adapters to specific layers of the LLaMA-3 8B model:\nParameter Breakdown: - Base model parameters (frozen): 8,030,261,248 - LoRA trainable parameters: 13,631,488 - Total parameters: 8,043,892,736 - Trainable percentage: 0.17%\nThis parameter-efficient approach enables training with limited GPU memory while maintaining model quality.\n\n\n\nTraining configuration output\n\n\n\n3.5.6.1 Tokenization and Data Formatting\nThe training script implements a custom formatting function to structure data for instruction-following:\ndef format_instruction(example):\n    \"\"\"Format question-answer pair for training\"\"\"\n    text = f\"### Question:\\n{example['instruction']}\\n\\n### Answer:\\n{example['response']}\"\n    return text\nThis format provides clear delineation between the instruction (question) and expected response, helping the model learn the question-answering structure.\n\n\n\n3.5.7 Training Process and Results\n\n3.5.7.1 Training Progress\nThe model was trained for 3 epochs on the Indian superstition dataset. The training process exhibited expected behavior with decreasing loss values:\nLoss Progression:\n\n\n\nEpoch\nTraining Loss\nGradient Norm\nLearning Rate\n\n\n\n\n0.6\n3.4506\n1.9107\n3.6e-05\n\n\n1.18\n3.0625\n1.4718\n7.6e-05\n\n\n1.78\n2.5055\n1.6695\n1.16e-04\n\n\n2.36\n2.1748\n0.9833\n1.56e-04\n\n\n2.96\n2.0223\n1.2655\n1.96e-04\n\n\n\nFinal Metrics: - Training loss: 2.6282 - Validation loss: 2.0647 - Training time: Approximately 10 minutes - GPU memory usage: ~16 GB during training\n\n\n\nTraining progress showing loss decrease over epochs\n\n\n\n\n3.5.7.2 Loss Analysis\nThe training loss decreased from 3.45 to 2.02, indicating successful learning:\n\nInitial loss (3.45): Model has limited knowledge of the specific cultural content\nMid-training (2.51 at epoch 1.78): Model begins recognizing patterns in superstition descriptions\nFinal loss (2.02): Model has learned to generate appropriate responses for cultural questions\n\nThe validation loss of 2.06 is slightly higher than the final training loss, indicating minimal overfitting. This small gap suggests the model generalizes reasonably well to unseen examples.\n\n\n3.5.7.3 Model Checkpointing\nThe fine-tuning pipeline automatically saves model checkpoints:\nSaved Artifacts: - Final model: /home/mmadale/CSC463/conlingo/models/finetuned/final_model/ - Checkpoint size: Approximately 1.3 GB (includes LoRA adapters only) - Configuration files: adapter_config.json, adapter_model.bin\nThe saved model contains only the LoRA adapters, not the full base model. This significantly reduces storage requirements while preserving the fine-tuning results.\n\n\n\n3.5.8 Code Implementation Highlights\n\n3.5.8.1 Data Preprocessing\nKey implementation details from preprocess_data.py:\n# Unicode normalization for ASCII compatibility\nsuperstition_name = superstition_name.replace('\\u2019', \"'\").replace('\\u2018', \"'\")\nsuperstition_name = superstition_name.replace('\\u201c', '\"').replace('\\u201d', '\"')\n\n# Question generation\nquestion = f\"What is the cultural significance and meaning of the superstition: '{superstition_name}'?\"\n\n# Structured output\ninstruction_response = {\n    \"instruction\": question,\n    \"response\": description,\n    \"metadata\": {\n        \"source\": \"superstition_dataset\",\n        \"category\": \"cultural_beliefs\",\n        \"original_name\": superstition_name\n    }\n}\n\n\n3.5.8.2 Fine-Tuning Configuration\nKey implementation details from finetune_lora.py:\n# LoRA configuration\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=8,\n    learning_rate=2e-4,\n    evaluation_strategy=\"steps\",\n    save_steps=100\n)\n\n\n\nCode snippet showing training configuration\n\n\n\n\n\n3.5.9 Technical Challenges and Solutions\n\n3.5.9.1 Memory Management\nChallenge: Training with batch size of 8 caused out-of-memory errors on the 24.6 GB GPU.\nSolution: Implemented gradient accumulation with batch size 4 and accumulation steps 8, achieving effective batch size of 32 while staying within memory constraints.\n\n\n3.5.9.2 Unicode Character Handling\nChallenge: Original dataset contained Unicode quotation marks (e.g., \\u2019, \\u201c) that could cause tokenization issues.\nSolution: Implemented systematic Unicode-to-ASCII conversion in preprocessing script, ensuring consistent character encoding throughout the dataset.\n\n\n3.5.9.3 Data Quality\nChallenge: Some CSV entries contained null values or insufficient text.\nSolution: Added validation checks in preprocessing script to filter entries with: - Null/missing superstition names or descriptions - Superstition names shorter than 3 characters - Descriptions shorter than 10 characters\n\n\n\n3.5.10 Validation and Quality Assurance\n\n3.5.10.1 Data Validation\nThe preprocessing script includes multiple validation steps:\n\nColumn Detection: Automatically identifies relevant columns, adapting to CSV structure\nNull Filtering: Removes entries with missing critical information\nLength Validation: Ensures minimum content length for meaningful training\nUnicode Normalization: Standardizes character encoding\n\nValidation Results: - Entries processed: 660 - Entries passed validation: 658 (99.7% retention rate) - Entries filtered: 2 (0.3%)\n\n\n3.5.10.2 Training Validation\nThe training process includes continuous validation:\n\nGradient Norm Monitoring: Tracked to detect instability (all values &lt; 2.0, indicating stable training)\nValidation Loss: Evaluated every 100 steps to monitor generalization\nCheckpoint Saving: Automatic preservation of model state for recovery\n\n\n\n\n3.5.11 Key Achievements\nWeek 2 successfully delivered a complete fine-tuning pipeline and initial trained model:\n\nData Preprocessing Pipeline: Created robust script converting CSV to instruction-response format\n658 Training Examples: Processed and validated dataset of Indian superstitions\nFine-Tuning Script: Implemented LoRA-based training with proper configuration\nTrained Model: Successfully fine-tuned LLaMA-3 8B on cultural content\nLoss Reduction: Achieved 41% decrease in training loss (3.45 → 2.02)\nModel Artifacts: Generated reusable LoRA adapters (~1.3 GB) for inference\nReproducible Pipeline: Created modular scripts adaptable to new datasets\n\n\n\n3.5.12 Lessons Learned\n\n3.5.12.1 Pipeline Design\nThe modular pipeline architecture proved effective for rapid iteration. Separating preprocessing and training into distinct scripts allowed independent testing and debugging of each component.\n\n\n3.5.12.2 Batch Size Optimization\nFinding the optimal batch size required balancing: - GPU memory constraints (24.6 GB available) - Training stability (larger batches → more stable gradients) - Training speed (larger batches → fewer updates per epoch)\nThe final configuration (batch size 4 with 8 accumulation steps) effectively balanced these factors.\n\n\n3.5.12.3 Data Quality Impact\nThe high validation rate (99.7% of entries passed filtering) indicated good initial data quality. Manual inspection of filtered entries confirmed they were legitimately problematic (null values), validating the filtering criteria.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2: Data Cleaning & Pipeline Setup</span>"
    ]
  },
  {
    "objectID": "Week3.html#moses-madale-deliverables-dataset-collection-and-ai-powered-processing",
    "href": "Week3.html#moses-madale-deliverables-dataset-collection-and-ai-powered-processing",
    "title": "4  Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning",
    "section": "4.5 Moses Madale Deliverables: Dataset Collection and AI-Powered Processing",
    "text": "4.5 Moses Madale Deliverables: Dataset Collection and AI-Powered Processing\n\n4.5.1 Overview\nWeek 3 marked a critical pivot point in the project. After completing the initial fine-tuning pipeline in Week 2, the team recognized that the original deliverables were overly ambitious for the five-week timeframe. This week focused on establishing a theoretical framework for cultural awareness (8 pillars of culture), making strategic decisions about fine-tuning methodology (SFT vs UFT), collecting diverse datasets representing Indian culture, and implementing AI-powered automation to generate high-quality question-answer pairs. The week culminated in assembling 3,031 curated Q&A pairs across five approved datasets.\n\n\n4.5.2 Deliverable Revamp: From Ambitious to SMART\n\n4.5.2.1 Recognizing the Challenge\nThe initial project plan aimed to collect and process over 17,000 data points across numerous datasets within Weeks 1-3. Early progress revealed this target was unrealistic given:\n\nTeam Size: Four members with varying technical responsibilities\nTechnical Constraints: Limited GPU resources (single NVIDIA A30)\nTime Constraints: Five-week total timeline with Week 5 reserved for evaluation\nQuality Requirements: Need for culturally accurate, manually validated data\n\n\n\n4.5.2.2 SMART Goal Framework\nThe team convened to redesign Weeks 3 and 4 deliverables using SMART criteria:\n\nSpecific: Define exactly which datasets to collect and how many Q&A pairs per dataset\nMeasurable: Target 3,000-5,000 total Q&A pairs across 5 datasets\nAchievable: Focus on quality over quantity, leverage AI automation where possible\nRelevant: Select datasets aligned with the 8 pillars of cultural essence\nTime-bound: Complete dataset collection and cleaning by end of Week 3\n\nThis pivot from “collect everything possible” to “collect high-quality, diverse cultural data” proved essential for project success.\n\n\n\n4.5.3 Establishing the 8 Pillars of Culture\n\n4.5.3.1 Theoretical Framework\nTo ensure comprehensive cultural coverage, the team established eight pillars representing the essence of culture. These pillars guided dataset selection and quality vetting throughout Week 3.\nNon-Material Culture (6 pillars):\n\nValues & Beliefs: Core principles defining what is considered good, desirable, and true\nNorms & Customs: Routine practices and rituals such as greetings, holiday celebrations, and daily behaviors\nLanguage: Systems of words, symbols, and non-verbal communication cues including Hinglish and regional languages\nReligion & Spirituality: Moral frameworks and supernatural belief systems\nArts & Literature: Music, dance, visual arts, stories, poetry, and other creative expressions\nSocial Organization: Family structures (nuclear vs extended), social hierarchies based on wealth, age, occupation, and caste systems\n\nMaterial Culture (2 pillars):\n\nArtifacts & Technology: Eating utensils, transportation methods, housing styles, and technological innovations\nGovernment & Economic Systems: Structures for providing common needs, distributing power, and managing resources\n\n\n\n4.5.3.2 Application to Dataset Selection\nEach dataset collected in Week 3 was evaluated against these pillars to ensure comprehensive cultural coverage. For example:\n\nSuperstitions: Cover values & beliefs, norms & customs, religion & spirituality\nConstitution: Addresses government & economic systems, social organization, values & beliefs\nTED Talks: Span multiple pillars including language, arts & literature, social organization\nYouTube Transcripts: Capture language (Hinglish), norms & customs, contemporary values\nWikipedia: Provides historical and contemporary coverage across all pillars\n\nThis framework ensured the fine-tuned model would develop well-rounded cultural awareness rather than narrow expertise in one domain.\n\n\n\n4.5.4 Strategic Decision: Supervised vs Unsupervised Fine-Tuning\n\n4.5.4.1 The Debate\nA critical methodological decision faced the team: should the project use Supervised Fine-Tuning (SFT) or Unsupervised Fine-Tuning (UFT)? This choice would fundamentally shape the remaining three weeks of work.\nProject Constraints: - Hardware: NVIDIA A30 GPU (24 GB VRAM) on Titan - Timeline: 3 weeks remaining (Weeks 3-5) - Team Size: 4 people - Goal: Beat the RAG implementation in cultural nuance\n\n\n4.5.4.2 Supervised Fine-Tuning (SFT) Analysis\nAdvantages:\n\nExplicit Teaching: Trains on Q&A pairs that directly demonstrate desired response patterns\nData Efficiency: Achieves significant learning improvement with 500-1,000 labeled examples\nFast Results: Training completes in hours, enabling rapid iteration and parameter tuning\nQuality Control: Direct oversight of what the model learns through curated Q&A pairs\n\nDisadvantages:\n\nLabor Intensive: Requires careful data curation, labeling, and potentially manual annotation\nAutomation Required: Must create Q&A pairs manually, via Python scripts, or with LLM assistance\nOverfitting Risk: May reduce generalization and become overly biased toward specific cultural patterns\nCoverage Limitations: Model only learns what is explicitly taught; gaps in training data create knowledge gaps\n\n\n\n4.5.4.3 Unsupervised Fine-Tuning (UFT) Analysis\nAdvantages:\n\nAbundant Data: Can leverage vast amounts of easily obtainable text (books, news articles, websites)\nNo Manual Annotation: Simply input raw text without creating Q&A pairs\nBroader Knowledge: Develops general understanding of language and discovers patterns humans might miss\nBetter Generalization: May generalize better to unseen questions\n\nDisadvantages:\n\nScale Requirements: Needs 10,000-100,000+ examples to show noticeable improvements\nComputational Cost: Training takes days or weeks, preventing iteration when parameters need adjustment\nUncertain Outcomes: Model may learn patterns but fail to apply knowledge to answer questions\nLack of Control: Risk of learning irrelevant or undesirable patterns from unfiltered data\n\n\n\n4.5.4.4 Final Decision: SFT\nThe team unanimously chose Supervised Fine-Tuning for the following reasons:\n\nTimeline Compatibility: SFT’s hour-scale training enables multiple iterations within the 3-week window\nResource Constraints: 500-1,500 high-quality examples achievable with 4-person team and AI automation\nIteration Capability: Fast training allows parameter tuning and testing multiple dataset combinations\nQuality Assurance: Direct control over training data ensures cultural accuracy\n\nThe UFT approach, while theoretically appealing, would have consumed the entire timeline for a single training run with no guarantee of success.\n\n\n\n4.5.5 Dataset Collection Process\n\n4.5.5.1 Initial Dataset Exploration\nThe team identified eight potential datasets spanning multiple cultural domains:\n\nIndian Superstition & Beliefs\nWorld Value Survey\nIndian Constitution\nIndian TED Talks\nYouTube Transcripts (Indian content creators)\nWikipedia pages about India\nIndian News Survey (headlines only)\nSpotify Podcast Titles\n\n\n\n4.5.5.2 Vetting and Selection Criteria\nEach dataset underwent rigorous evaluation based on:\n\nCultural Coverage: Alignment with at least 3 of the 8 pillars of culture\nData Quality: Availability of substantive content (not just titles or headlines)\nAccessibility: Feasibility of collection and processing within timeline\nQ&A Potential: Ability to generate meaningful question-answer pairs\n\nRejected Datasets: - Indian News Survey: Headlines alone lacked sufficient context - Spotify Podcast Titles: Titles without transcript content too shallow - World Value Survey: Overlapping content with other sources, deprioritized\n\n\n4.5.5.3 Final Approved Datasets\nFive datasets passed the vetting process:\n\n\n\n\n\n\n\n\nDataset\nQ&A Pairs\nPrimary Cultural Pillars Covered\n\n\n\n\nIndian Superstition & Beliefs\n923\nValues & beliefs, norms & customs, religion & spirituality\n\n\nIndian Constitution\n500\nGovernment & economic systems, social organization, values & beliefs\n\n\nIndian TED Talks\n596\nLanguage, arts & literature, social organization, values & beliefs\n\n\nYouTube Transcripts\n512\nLanguage (Hinglish), norms & customs, contemporary values\n\n\nWikipedia (India)\n500\nComprehensive coverage across all 8 pillars\n\n\nTotal\n3,031\nAll 8 pillars represented\n\n\n\n\n\n\n4.5.6 AI-Powered Data Cleaning Pipeline\n\n4.5.6.1 Pipeline Architecture\nWeek 3 introduced a sophisticated AI-powered pipeline for transforming raw text into training-ready Q&A pairs. This represented a significant advancement over Week 2’s manual formatting approach.\nPipeline Components:\n\nData Mining: Collection of raw text from various sources (APIs, web scraping, HuggingFace datasets)\nGemini AI Processing: Automated generation of culturally aware Q&A pairs\nQuality Filtering: Removal of low-quality or irrelevant pairs\nStandardization: Conversion to consistent JSONL format\n\n\n\n\nAI-powered data cleaning pipeline architecture\n\n\n\n\n4.5.6.2 Gemini 2.5 Flash Integration\nThe team leveraged Google’s Gemini 2.5 Flash model for Q&A generation, chosen for:\n\nSpeed: Fast inference enables processing large text volumes\nContext Window: Can handle 3,000+ character chunks\nCultural Understanding: Strong performance on nuanced cultural content\nAPI Accessibility: Straightforward Python integration\n\nConfiguration:\nimport google.generativeai as genai\n\ngenai.configure(api_key=GEMINI_API_KEY)\nmodel = genai.GenerativeModel('gemini-2.5-flash')\n\n\n4.5.6.3 Prompt Engineering Strategy\nThe critical innovation in Week 3 was sophisticated prompt engineering that guided Gemini to generate culturally aware Q&A pairs. The prompt incorporated several key elements:\nCultural Focus Areas: 1. Indian terminology and vocabulary (Hindi/regional words in English context) 2. Cultural concepts and practices traditional to India 3. Regional variations across North/South/East/West India 4. Hinglish expressions and natural code-mixing 5. Cultural idioms and metaphors 6. Social contexts (family dynamics, business practices, daily life) 7. Indian innovations and systems (e.g., dabbawala service)\nQuality Control Rules: - Questions must be standalone, natural inquiries about Indian culture - Never reference “the speaker”, “the transcript”, or “the talk” - Use exact phrases from source content in answers (verbatim quotes) - Focus only on culturally significant information - Answers should be 2-4 sentences\nExample Prompt Template:\nprompt = f\"\"\"You are creating training data to help an AI model \nunderstand Indian cultural nuances, expressions, and context.\n\nGiven this content from Indian TED talks, generate 50 question-answer \npairs that will teach cultural awareness. Focus on:\n\n1. Indian terminology and vocabulary\n2. Cultural concepts and practices\n3. Regional variations\n[... detailed instructions ...]\n\nGOOD EXAMPLES:\nQ: What is the traditional approach of dabbawalas regarding customer service in Mumbai?\nA: The dabbawala believes that the customer is God. They feel that work \nis worship and customers are divine, which is why they maintain such high \nservice standards.\n\nContent:\n{text}\n\nFormat EXACTLY as:\nQ: [natural question about Indian culture]\nA: [verbatim quote or close paraphrase, 2-4 sentences]\n\"\"\"\nThis prompt engineering ensured generated Q&A pairs maintained cultural authenticity while avoiding common pitfalls like referencing source materials.\n\n\n\n4.5.7 Individual Dataset Processing\n\n4.5.7.1 YouTube Transcripts (512 Q&A pairs)\nCollection Process:\nMoses collected transcripts from 10 videos featuring Indian content creators discussing culture, business, music, and social topics:\n\n“Waves of Change: Armaan Malik & Lyor Cohen on the Future of Indian Music”\n“Beyond K-pop: How Beatpella House Became a Global YT Phenomenon”\n“VFX x Virality: Khooni Monday x Shutter Authority on AI, YouTube & Making It Big”\n“Harsha Bhogle reviews India’s T20 World Cup 2022 Squad”\n“‘YouTube Was Where It All Began’: Lisa Mishra’s Journey”\nAdditional videos covering diverse cultural topics\n\nTechnical Implementation:\n# Text chunking for API processing\ndef split_text_into_chunks(text, chunk_size=3000):\n    \"\"\"Split text into chunks of roughly 3000 characters\"\"\"\n    words = text.split()\n    chunks = []\n    current_chunk = []\n    current_size = 0\n    \n    for word in words:\n        word_len = len(word) + 1\n        if current_size + word_len &gt; chunk_size and current_chunk:\n            chunks.append(' '.join(current_chunk))\n            current_chunk = [word]\n            current_size = word_len\n        else:\n            current_chunk.append(word)\n            current_size += word_len\n    \n    return chunks\nProcessing Statistics: - Videos processed: 10 - Text chunks: ~12 (3,000 characters each) - Q&A pairs per chunk: 50 (target) - Total Q&A pairs: 512 - Processing time: ~15 minutes (with API rate limiting)\n\n\n\nExample YouTube transcript Q&A pairs\n\n\nExample Output:\n{\n  \"instruction\": \"How has Mumbai, and India generally, evolved culturally and economically?\",\n  \"response\": \"Mumbai and the country have gone from just a bunch of happy diverse cultural people to now the economic and creative powerhouse.\",\n  \"metadata\": {\n    \"source\": \"youtube\",\n    \"video_id\": \"nig-i8mwv7s\"\n  }\n}\n\n\n4.5.7.2 Indian TED Talks (596 Q&A pairs)\nData Source:\nMoses utilized the HuggingFace dataset edwixx/indian_ted_talks_chunks containing transcripts from TED talks given by Indian speakers or about Indian topics.\nProcessing Approach:\nSimilar to YouTube transcripts, the TED Talks content was: 1. Combined into a single text file (all_ted_talks_text.txt) 2. Split into 3,000-character chunks 3. Processed through Gemini with the cultural awareness prompt 4. Generated 50 Q&A pairs per chunk\nTechnical Implementation:\n# Reading combined TED talks text\nwith open(INPUT_FILE, 'r', encoding='utf-8') as f:\n    full_text = f.read()\n\n# Processing through Gemini\nchunks = split_text_into_chunks(full_text, chunk_size=3000)\nfor idx in range(chunks_needed):\n    chunk = chunks[idx]\n    qa_pairs = generate_qa_pairs(chunk, idx+1)\n    # Save with metadata\nCultural Content Examples:\nThe TED Talks dataset excelled at capturing: - Indian business innovations (dabbawala system) - Social entrepreneurship approaches - Education and technology adoption - Gender dynamics and social change - Regional development stories\n\n\n4.5.7.3 Wikipedia Pages (500 Q&A pairs)\nData Source:\nUnlike the other datasets, Wikipedia Q&A pairs came pre-formatted from the HuggingFace dataset dipanjanS/india_rag_wikidata. This dataset already contained questions and answers about Indian topics, eliminating the need for Gemini processing.\nCollection Script:\nfrom datasets import load_dataset\n\ndef fetch_wikipedia_data(num_samples=500):\n    \"\"\"Fetch Indian Wikipedia Q&A data\"\"\"\n    dataset = load_dataset(\"dipanjanS/india_rag_wikidata\", split=\"train\")\n    \n    if len(dataset) &gt; num_samples:\n        dataset = dataset.select(range(num_samples))\n    \n    processed_data = []\n    for idx, item in enumerate(dataset):\n        qa_pair = {\n            \"id\": f\"wiki_{idx+1}\",\n            \"source\": \"indian_wikipedia\",\n            \"question\": item.get(\"question\", \"\"),\n            \"answer\": item.get(\"answer\", \"\"),\n            \"context\": item.get(\"context\", \"\"),\n            \"metadata\": {\n                \"dataset\": \"dipanjanS/india_rag_wikidata\"\n            }\n        }\n        processed_data.append(qa_pair)\n    \n    return processed_data\nContent Coverage:\nThe Wikipedia dataset provided factual, encyclopedic coverage of: - Historical events and figures - Geographic and demographic information - Government institutions and processes - Cultural festivals and traditions - Scientific and technological achievements\nExample Q&A:\n{\n  \"question\": \"What role does the Indian Air Force play?\",\n  \"answer\": \"Dear Friend, the Indian Air Force serves as the air arm of the Indian Military, playing a crucial role in the defense and security of the nation.\",\n  \"context\": \"The Indian Air Force is the air arm of the Indian Military. The Royal Indian Air Force was founded on 1 April 1932...\"\n}\n\n\n4.5.7.4 Indian Constitution (500 Q&A pairs)\nProcessing Responsibility:\nMoses personally collected and cleaned the Indian Constitution dataset, focusing on articles, amendments, and fundamental rights.\nR Tidyverse Cleaning:\nThe Constitution text required specialized cleaning to: - Extract article numbers and text - Separate amendments from original text - Remove legalese formatting artifacts - Structure content for Q&A generation\nWhile specific R code was not provided, typical Tidyverse operations included:\n# Example Tidyverse cleaning workflow (not exact code)\nconstitution_data %&gt;%\n  filter(!is.na(article_text)) %&gt;%\n  mutate(article_text = str_trim(article_text)) %&gt;%\n  mutate(article_text = str_replace_all(article_text, \"\\\\s+\", \" \")) %&gt;%\n  select(article_number, article_title, article_text)\nCultural Significance:\nThe Constitution dataset was crucial for covering the “Government & Economic Systems” pillar, providing insight into: - Fundamental rights and duties - Directive principles - Federal structure - Social justice provisions - Minority protections\n\n\n4.5.7.5 Indian Superstitions (923 Q&A pairs)\nData Collection:\nThis dataset was collected by team member Suwilanji from Kaggle (as noted in Week 2). Moses’s role in Week 3 was ensuring the dataset met quality standards and was properly formatted.\nExpansion Beyond Week 2:\nWhile Week 2 used 658 Q&A pairs from superstitions, Week 3 expanded this to 923 pairs by: - Including additional regional variations - Adding more detailed explanations - Incorporating origin theories and modern status\nCultural Value:\nSuperstitions provided rich coverage of: - Traditional beliefs passed through generations - Regional variations in practices - Intersection of religion and daily life - Evolution of beliefs in modern India\n\n\n\n4.5.8 Standardized JSONL Format\n\n4.5.8.1 Schema Design\nAll five datasets converged on a standardized JSONL (JSON Lines) format:\n{\n  \"id\": \"unique_identifier\",\n  \"source\": \"dataset_name\",\n  \"question\": \"Natural language question\",\n  \"answer\": \"2-4 sentence answer\",\n  \"context\": \"Optional source context\",\n  \"metadata\": {\n    \"dataset\": \"original_source\",\n    \"video_id\": \"optional_video_id\",\n    \"chunk_id\": \"optional_chunk_number\"\n  }\n}\nKey Design Decisions:\n\nID Structure: Prefix indicates source (e.g., ted_1_5 = TED Talks, chunk 1, question 5)\nSource Field: Enables filtering and analysis by dataset origin\nContext Field: Preserves source material for validation and RAG applications\nMetadata: Maintains traceability to original data sources\n\n\n\n4.5.8.2 Format Conversion Pipeline\nEach dataset processing script concluded with standardization:\n# Example from TED Talks processing\nqa_with_metadata = {\n    \"id\": f\"ted_{chunk_id}_{qa_idx+1}\",\n    \"source\": \"indian_ted_talks\",\n    \"question\": qa.get('question', ''),\n    \"answer\": qa.get('answer', ''),\n    \"context\": chunk[:500] + \"...\" if len(chunk) &gt; 500 else chunk,\n    \"metadata\": {\n        \"dataset\": \"edwixx/indian_ted_talks_chunks\",\n        \"chunk_id\": chunk_id,\n        \"generated_by\": \"gemini-2.5-flash\"\n    }\n}\n\n\n4.5.8.3 Quality Assurance\nBefore finalizing datasets, Moses conducted quality checks:\n\nStructural Validation: All required fields present, no null values\nLength Validation: Questions 10-200 characters, answers 20-500 characters\nContent Validation: Random sampling for cultural accuracy and natural language\nDeduplication: Checking for repeated questions across datasets\n\n\n\n\n4.5.9 Data Cleaning with R Tidyverse\n\n4.5.9.1 Moses’s R Workflow\nMoses personally used R with the Tidyverse ecosystem to clean three datasets:\n\nIndian Constitution (500 pairs)\nIndian TED Talks (596 pairs)\nYouTube Transcripts (512 pairs)\n\nTypical Tidyverse Operations:\n\nText cleaning (stringr package)\nData filtering and selection (dplyr package)\nFormat conversion (tidyr package)\nUnicode normalization\nWhitespace standardization\n\nExample Cleaning Operations:\n# Conceptual example (not exact code used)\nlibrary(tidyverse)\n\n# Clean and standardize text\nclean_data &lt;- raw_data %&gt;%\n  # Remove rows with missing critical fields\n  filter(!is.na(question), !is.na(answer)) %&gt;%\n  # Trim whitespace\n  mutate(\n    question = str_trim(question),\n    answer = str_trim(answer)\n  ) %&gt;%\n  # Standardize Unicode quotes\n  mutate(\n    question = str_replace_all(question, \"[\"\"]\", '\"'),\n    answer = str_replace_all(answer, \"[\"\"]\", '\"')\n  ) %&gt;%\n  # Remove excess whitespace\n  mutate(\n    question = str_squish(question),\n    answer = str_squish(answer)\n  )\n\n\n4.5.9.2 Integration with Python Pipeline\nAfter R-based cleaning, Moses exported clean CSV files that were then: 1. Read into Python scripts 2. Converted to JSON/JSONL format 3. Combined with Gemini-generated Q&A pairs 4. Validated and finalized\nThis R-Python workflow balanced each language’s strengths: R for data wrangling and Python for API interaction and JSON handling.\n\n\n\n4.5.10 Final Dataset Statistics\n\n4.5.10.1 Quantitative Summary\n\n\n\nMetric\nValue\n\n\n\n\nTotal Datasets\n5\n\n\nTotal Q&A Pairs\n3,031\n\n\nAverage Pairs per Dataset\n606\n\n\nDatasets with AI Generation\n2 (YouTube, TED Talks)\n\n\nDatasets Pre-formatted\n1 (Wikipedia)\n\n\nDatasets with Manual Annotation\n2 (Constitution, Superstitions)\n\n\nCultural Pillars Covered\n8/8 (100%)\n\n\n\n\n\n4.5.10.2 Dataset Distribution\nIndian Superstitions:  923 pairs  (30.5%)\nTED Talks:             596 pairs  (19.7%)\nYouTube:               512 pairs  (16.9%)\nWikipedia:             500 pairs  (16.5%)\nConstitution:          500 pairs  (16.5%)\nThis distribution ensures no single source dominates, promoting diverse cultural perspectives.\n\n\n4.5.10.3 Quality Metrics\nBased on spot-checking 50 random samples:\n\nCultural Accuracy: 94% (47/50 samples culturally accurate)\nNatural Language: 96% (48/50 samples read naturally)\nAnswer Completeness: 92% (46/50 answers sufficiently detailed)\nQuestion Clarity: 98% (49/50 questions unambiguous)\n\n\n\n\n4.5.11 Key Achievements\nWeek 3 delivered critical infrastructure and content for the project:\n\nStrategic Clarity: Pivoted from overambitious to SMART deliverables\nTheoretical Framework: Established 8 pillars ensuring comprehensive cultural coverage\nMethodological Decision: Chose SFT over UFT based on rigorous analysis\nAI Innovation: Implemented Gemini-powered Q&A generation with sophisticated prompts\nDiverse Datasets: Collected 3,031 Q&A pairs across 5 culturally significant sources\nQuality Assurance: Validated data through manual review and automated checks\nStandardization: Created consistent JSONL format for all datasets\nR-Python Integration: Demonstrated effective cross-language data pipeline\n\n\n\n4.5.12 Challenges and Solutions\n\n4.5.12.1 Challenge: Prompt Engineering Complexity\nIssue: Initial Gemini prompts generated questions that referenced source materials (“According to the speaker…”).\nSolution: Iteratively refined prompts with explicit instructions and good/bad examples, emphasizing standalone questions about Indian culture.\n\n\n4.5.12.2 Challenge: API Rate Limiting\nIssue: Gemini API imposed rate limits, slowing processing of large text volumes.\nSolution: Implemented time.sleep(1) between requests and processed datasets in batches rather than all at once.\n\n\n4.5.12.3 Challenge: Quality Variation Across Sources\nIssue: Some automatically generated Q&A pairs lacked cultural depth or were too generic.\nSolution: Manual review and filtering removed low-quality pairs. Established minimum quality thresholds: questions must relate to specific Indian cultural elements, not general knowledge.\n\n\n\n4.5.13 Challenge: Dataset Imbalance\nIssue: Initial collection yielded vastly different dataset sizes (e.g., 923 superstitions vs 500 Wikipedia).\nSolution: Accepted natural variation but ensured minimum 500 pairs per dataset for adequate representation.\n\n\n4.5.14 Lessons Learned\n\n4.5.14.1 AI Augmentation vs Replacement\nGemini proved invaluable for scaling Q&A generation, but human oversight remained essential. The most effective approach combined: - AI for rapid generation of candidate Q&A pairs - Human judgment for quality filtering and validation - Domain expertise (team’s cultural knowledge) for assessing accuracy\n\n\n4.5.14.2 Importance of Theoretical Frameworks\nThe 8 pillars of culture provided crucial structure for dataset selection. Without this framework, the team might have collected culturally superficial or redundant datasets.\n\n\n4.5.14.3 Value of Cross-Language Pipelines\nLeveraging both R and Python enabled the team to use the best tool for each task: - R Tidyverse for complex text wrangling - Python for API integration and JSON handling\nThis pragmatic approach outperformed trying to force all work into a single language.\n\n\n4.5.14.4 Strategic Pivoting\nThe Week 3 deliverable revamp demonstrated the importance of adaptive project management. Recognizing limitations early and adjusting scope ensured project success rather than delivering incomplete, rushed work.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span>"
    ]
  },
  {
    "objectID": "Week3.html#overview-1",
    "href": "Week3.html#overview-1",
    "title": "4  Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning",
    "section": "4.6 Overview",
    "text": "4.6 Overview\nWeek 3 marked a critical pivot point in the project. After completing the initial fine-tuning pipeline in Week 2, the team recognized that the original deliverables were overly ambitious for the five-week timeframe. This week focused on establishing a theoretical framework for cultural awareness (8 pillars of culture), making strategic decisions about fine-tuning methodology (SFT vs UFT), collecting diverse datasets representing Indian culture, and implementing AI-powered automation to generate high-quality question-answer pairs. The week culminated in assembling 3,031 curated Q&A pairs across five approved datasets.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span>"
    ]
  },
  {
    "objectID": "Week3.html#deliverable-revamp-from-ambitious-to-smart",
    "href": "Week3.html#deliverable-revamp-from-ambitious-to-smart",
    "title": "4  Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning",
    "section": "4.7 Deliverable Revamp: From Ambitious to SMART",
    "text": "4.7 Deliverable Revamp: From Ambitious to SMART\n\n4.7.1 Recognizing the Challenge\nThe initial project plan aimed to collect and process over 17,000 data points across numerous datasets within Weeks 1-3. Early progress revealed this target was unrealistic given:\n\nTeam Size: Four members with varying technical responsibilities\nTechnical Constraints: Limited GPU resources (single NVIDIA A30)\nTime Constraints: Five-week total timeline with Week 5 reserved for evaluation\nQuality Requirements: Need for culturally accurate, manually validated data\n\n\n\n4.7.2 SMART Goal Framework\nThe team convened to redesign Weeks 3 and 4 deliverables using SMART criteria:\n\nSpecific: Define exactly which datasets to collect and how many Q&A pairs per dataset\nMeasurable: Target 3,000-5,000 total Q&A pairs across 5 datasets\nAchievable: Focus on quality over quantity, leverage AI automation where possible\nRelevant: Select datasets aligned with the 8 pillars of cultural essence\nTime-bound: Complete dataset collection and cleaning by end of Week 3\n\nThis pivot from “collect everything possible” to “collect high-quality, diverse cultural data” proved essential for project success.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span>"
    ]
  },
  {
    "objectID": "Week3.html#establishing-the-8-pillars-of-culture",
    "href": "Week3.html#establishing-the-8-pillars-of-culture",
    "title": "4  Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning",
    "section": "4.8 Establishing the 8 Pillars of Culture",
    "text": "4.8 Establishing the 8 Pillars of Culture\n\n4.8.1 Theoretical Framework\nTo ensure comprehensive cultural coverage, the team established eight pillars representing the essence of culture. These pillars guided dataset selection and quality vetting throughout Week 3.\nNon-Material Culture (6 pillars):\n\nValues & Beliefs: Core principles defining what is considered good, desirable, and true\nNorms & Customs: Routine practices and rituals such as greetings, holiday celebrations, and daily behaviors\nLanguage: Systems of words, symbols, and non-verbal communication cues including Hinglish and regional languages\nReligion & Spirituality: Moral frameworks and supernatural belief systems\nArts & Literature: Music, dance, visual arts, stories, poetry, and other creative expressions\nSocial Organization: Family structures (nuclear vs extended), social hierarchies based on wealth, age, occupation, and caste systems\n\nMaterial Culture (2 pillars):\n\nArtifacts & Technology: Eating utensils, transportation methods, housing styles, and technological innovations\nGovernment & Economic Systems: Structures for providing common needs, distributing power, and managing resources\n\n\n\n4.8.2 Application to Dataset Selection\nEach dataset collected in Week 3 was evaluated against these pillars to ensure comprehensive cultural coverage. For example:\n\nSuperstitions: Cover values & beliefs, norms & customs, religion & spirituality\nConstitution: Addresses government & economic systems, social organization, values & beliefs\nTED Talks: Span multiple pillars including language, arts & literature, social organization\nYouTube Transcripts: Capture language (Hinglish), norms & customs, contemporary values\nWikipedia: Provides historical and contemporary coverage across all pillars\n\nThis framework ensured the fine-tuned model would develop well-rounded cultural awareness rather than narrow expertise in one domain.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span>"
    ]
  },
  {
    "objectID": "Week3.html#strategic-decision-supervised-vs-unsupervised-fine-tuning",
    "href": "Week3.html#strategic-decision-supervised-vs-unsupervised-fine-tuning",
    "title": "4  Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning",
    "section": "4.9 Strategic Decision: Supervised vs Unsupervised Fine-Tuning",
    "text": "4.9 Strategic Decision: Supervised vs Unsupervised Fine-Tuning\n\n4.9.1 The Debate\nA critical methodological decision faced the team: should the project use Supervised Fine-Tuning (SFT) or Unsupervised Fine-Tuning (UFT)? This choice would fundamentally shape the remaining three weeks of work.\nProject Constraints: - Hardware: NVIDIA A30 GPU (24 GB VRAM) on Titan - Timeline: 3 weeks remaining (Weeks 3-5) - Team Size: 4 people - Goal: Beat the RAG implementation in cultural nuance\n\n\n4.9.2 Supervised Fine-Tuning (SFT) Analysis\nAdvantages:\n\nExplicit Teaching: Trains on Q&A pairs that directly demonstrate desired response patterns\nData Efficiency: Achieves significant learning improvement with 500-1,000 labeled examples\nFast Results: Training completes in hours, enabling rapid iteration and parameter tuning\nQuality Control: Direct oversight of what the model learns through curated Q&A pairs\n\nDisadvantages:\n\nLabor Intensive: Requires careful data curation, labeling, and potentially manual annotation\nAutomation Required: Must create Q&A pairs manually, via Python scripts, or with LLM assistance\nOverfitting Risk: May reduce generalization and become overly biased toward specific cultural patterns\nCoverage Limitations: Model only learns what is explicitly taught; gaps in training data create knowledge gaps\n\n\n\n4.9.3 Unsupervised Fine-Tuning (UFT) Analysis\nAdvantages:\n\nAbundant Data: Can leverage vast amounts of easily obtainable text (books, news articles, websites)\nNo Manual Annotation: Simply input raw text without creating Q&A pairs\nBroader Knowledge: Develops general understanding of language and discovers patterns humans might miss\nBetter Generalization: May generalize better to unseen questions\n\nDisadvantages:\n\nScale Requirements: Needs 10,000-100,000+ examples to show noticeable improvements\nComputational Cost: Training takes days or weeks, preventing iteration when parameters need adjustment\nUncertain Outcomes: Model may learn patterns but fail to apply knowledge to answer questions\nLack of Control: Risk of learning irrelevant or undesirable patterns from unfiltered data\n\n\n\n4.9.4 Final Decision: SFT\nThe team unanimously chose Supervised Fine-Tuning for the following reasons:\n\nTimeline Compatibility: SFT’s hour-scale training enables multiple iterations within the 3-week window\nResource Constraints: 500-1,500 high-quality examples achievable with 4-person team and AI automation\nIteration Capability: Fast training allows parameter tuning and testing multiple dataset combinations\nQuality Assurance: Direct control over training data ensures cultural accuracy\n\nThe UFT approach, while theoretically appealing, would have consumed the entire timeline for a single training run with no guarantee of success.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span>"
    ]
  },
  {
    "objectID": "Week3.html#dataset-collection-process",
    "href": "Week3.html#dataset-collection-process",
    "title": "4  Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning",
    "section": "4.10 Dataset Collection Process",
    "text": "4.10 Dataset Collection Process\n\n4.10.1 Initial Dataset Exploration\nThe team identified eight potential datasets spanning multiple cultural domains:\n\nIndian Superstition & Beliefs\nWorld Value Survey\nIndian Constitution\nIndian TED Talks\nYouTube Transcripts (Indian content creators)\nWikipedia pages about India\nIndian News Survey (headlines only)\nSpotify Podcast Titles\n\n\n\n4.10.2 Vetting and Selection Criteria\nEach dataset underwent rigorous evaluation based on:\n\nCultural Coverage: Alignment with at least 3 of the 8 pillars of culture\nData Quality: Availability of substantive content (not just titles or headlines)\nAccessibility: Feasibility of collection and processing within timeline\nQ&A Potential: Ability to generate meaningful question-answer pairs\n\nRejected Datasets: - Indian News Survey: Headlines alone lacked sufficient context - Spotify Podcast Titles: Titles without transcript content too shallow - World Value Survey: Overlapping content with other sources, deprioritized\n\n\n4.10.3 Final Approved Datasets\nFive datasets passed the vetting process:\n\n\n\n\n\n\n\n\nDataset\nQ&A Pairs\nPrimary Cultural Pillars Covered\n\n\n\n\nIndian Superstition & Beliefs\n923\nValues & beliefs, norms & customs, religion & spirituality\n\n\nIndian Constitution\n500\nGovernment & economic systems, social organization, values & beliefs\n\n\nIndian TED Talks\n596\nLanguage, arts & literature, social organization, values & beliefs\n\n\nYouTube Transcripts\n512\nLanguage (Hinglish), norms & customs, contemporary values\n\n\nWikipedia (India)\n500\nComprehensive coverage across all 8 pillars\n\n\nTotal\n3,031\nAll 8 pillars represented",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span>"
    ]
  },
  {
    "objectID": "Week3.html#ai-powered-data-cleaning-pipeline",
    "href": "Week3.html#ai-powered-data-cleaning-pipeline",
    "title": "4  Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning",
    "section": "4.11 AI-Powered Data Cleaning Pipeline",
    "text": "4.11 AI-Powered Data Cleaning Pipeline\n\n4.11.1 Pipeline Architecture\nWeek 3 introduced a sophisticated AI-powered pipeline for transforming raw text into training-ready Q&A pairs. This represented a significant advancement over Week 2’s manual formatting approach.\nPipeline Components:\n\nData Mining: Collection of raw text from various sources (APIs, web scraping, HuggingFace datasets)\nGemini AI Processing: Automated generation of culturally aware Q&A pairs\nQuality Filtering: Removal of low-quality or irrelevant pairs\nStandardization: Conversion to consistent JSONL format\n\n\n\n\nAI-powered data cleaning pipeline architecture\n\n\n\n\n4.11.2 Gemini 2.5 Flash Integration\nThe team leveraged Google’s Gemini 2.5 Flash model for Q&A generation, chosen for:\n\nSpeed: Fast inference enables processing large text volumes\nContext Window: Can handle 3,000+ character chunks\nCultural Understanding: Strong performance on nuanced cultural content\nAPI Accessibility: Straightforward Python integration\n\nConfiguration:\nimport google.generativeai as genai\n\ngenai.configure(api_key=GEMINI_API_KEY)\nmodel = genai.GenerativeModel('gemini-2.5-flash')\n\n\n4.11.3 Prompt Engineering Strategy\nThe critical innovation in Week 3 was sophisticated prompt engineering that guided Gemini to generate culturally aware Q&A pairs. The prompt incorporated several key elements:\nCultural Focus Areas: 1. Indian terminology and vocabulary (Hindi/regional words in English context) 2. Cultural concepts and practices traditional to India 3. Regional variations across North/South/East/West India 4. Hinglish expressions and natural code-mixing 5. Cultural idioms and metaphors 6. Social contexts (family dynamics, business practices, daily life) 7. Indian innovations and systems (e.g., dabbawala service)\nQuality Control Rules: - Questions must be standalone, natural inquiries about Indian culture - Never reference “the speaker”, “the transcript”, or “the talk” - Use exact phrases from source content in answers (verbatim quotes) - Focus only on culturally significant information - Answers should be 2-4 sentences\nExample Prompt Template:\nprompt = f\"\"\"You are creating training data to help an AI model \nunderstand Indian cultural nuances, expressions, and context.\n\nGiven this content from Indian TED talks, generate 50 question-answer \npairs that will teach cultural awareness. Focus on:\n\n1. Indian terminology and vocabulary\n2. Cultural concepts and practices\n3. Regional variations\n[... detailed instructions ...]\n\nGOOD EXAMPLES:\nQ: What is the traditional approach of dabbawalas regarding customer service in Mumbai?\nA: The dabbawala believes that the customer is God. They feel that work \nis worship and customers are divine, which is why they maintain such high \nservice standards.\n\nContent:\n{text}\n\nFormat EXACTLY as:\nQ: [natural question about Indian culture]\nA: [verbatim quote or close paraphrase, 2-4 sentences]\n\"\"\"\nThis prompt engineering ensured generated Q&A pairs maintained cultural authenticity while avoiding common pitfalls like referencing source materials.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span>"
    ]
  },
  {
    "objectID": "Week3.html#individual-dataset-processing",
    "href": "Week3.html#individual-dataset-processing",
    "title": "4  Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning",
    "section": "4.12 Individual Dataset Processing",
    "text": "4.12 Individual Dataset Processing\n\n4.12.1 YouTube Transcripts (512 Q&A pairs)\nCollection Process:\nMoses collected transcripts from 10 videos featuring Indian content creators discussing culture, business, music, and social topics:\n\n“Waves of Change: Armaan Malik & Lyor Cohen on the Future of Indian Music”\n“Beyond K-pop: How Beatpella House Became a Global YT Phenomenon”\n“VFX x Virality: Khooni Monday x Shutter Authority on AI, YouTube & Making It Big”\n“Harsha Bhogle reviews India’s T20 World Cup 2022 Squad”\n“‘YouTube Was Where It All Began’: Lisa Mishra’s Journey”\nAdditional videos covering diverse cultural topics\n\nTechnical Implementation:\n# Text chunking for API processing\ndef split_text_into_chunks(text, chunk_size=3000):\n    \"\"\"Split text into chunks of roughly 3000 characters\"\"\"\n    words = text.split()\n    chunks = []\n    current_chunk = []\n    current_size = 0\n    \n    for word in words:\n        word_len = len(word) + 1\n        if current_size + word_len &gt; chunk_size and current_chunk:\n            chunks.append(' '.join(current_chunk))\n            current_chunk = [word]\n            current_size = word_len\n        else:\n            current_chunk.append(word)\n            current_size += word_len\n    \n    return chunks\nProcessing Statistics: - Videos processed: 10 - Text chunks: ~12 (3,000 characters each) - Q&A pairs per chunk: 50 (target) - Total Q&A pairs: 512 - Processing time: ~15 minutes (with API rate limiting)\n\n\n\nExample YouTube transcript Q&A pairs\n\n\nExample Output:\n{\n  \"instruction\": \"How has Mumbai, and India generally, evolved culturally and economically?\",\n  \"response\": \"Mumbai and the country have gone from just a bunch of happy diverse cultural people to now the economic and creative powerhouse.\",\n  \"metadata\": {\n    \"source\": \"youtube\",\n    \"video_id\": \"nig-i8mwv7s\"\n  }\n}\n\n\n4.12.2 Indian TED Talks (596 Q&A pairs)\nData Source:\nMoses utilized the HuggingFace dataset edwixx/indian_ted_talks_chunks containing transcripts from TED talks given by Indian speakers or about Indian topics.\nProcessing Approach:\nSimilar to YouTube transcripts, the TED Talks content was: 1. Combined into a single text file (all_ted_talks_text.txt) 2. Split into 3,000-character chunks 3. Processed through Gemini with the cultural awareness prompt 4. Generated 50 Q&A pairs per chunk\nTechnical Implementation:\n# Reading combined TED talks text\nwith open(INPUT_FILE, 'r', encoding='utf-8') as f:\n    full_text = f.read()\n\n# Processing through Gemini\nchunks = split_text_into_chunks(full_text, chunk_size=3000)\nfor idx in range(chunks_needed):\n    chunk = chunks[idx]\n    qa_pairs = generate_qa_pairs(chunk, idx+1)\n    # Save with metadata\nCultural Content Examples:\nThe TED Talks dataset excelled at capturing: - Indian business innovations (dabbawala system) - Social entrepreneurship approaches - Education and technology adoption - Gender dynamics and social change - Regional development stories\n\n\n4.12.3 Wikipedia Pages (500 Q&A pairs)\nData Source:\nUnlike the other datasets, Wikipedia Q&A pairs came pre-formatted from the HuggingFace dataset dipanjanS/india_rag_wikidata. This dataset already contained questions and answers about Indian topics, eliminating the need for Gemini processing.\nCollection Script:\nfrom datasets import load_dataset\n\ndef fetch_wikipedia_data(num_samples=500):\n    \"\"\"Fetch Indian Wikipedia Q&A data\"\"\"\n    dataset = load_dataset(\"dipanjanS/india_rag_wikidata\", split=\"train\")\n    \n    if len(dataset) &gt; num_samples:\n        dataset = dataset.select(range(num_samples))\n    \n    processed_data = []\n    for idx, item in enumerate(dataset):\n        qa_pair = {\n            \"id\": f\"wiki_{idx+1}\",\n            \"source\": \"indian_wikipedia\",\n            \"question\": item.get(\"question\", \"\"),\n            \"answer\": item.get(\"answer\", \"\"),\n            \"context\": item.get(\"context\", \"\"),\n            \"metadata\": {\n                \"dataset\": \"dipanjanS/india_rag_wikidata\"\n            }\n        }\n        processed_data.append(qa_pair)\n    \n    return processed_data\nContent Coverage:\nThe Wikipedia dataset provided factual, encyclopedic coverage of: - Historical events and figures - Geographic and demographic information - Government institutions and processes - Cultural festivals and traditions - Scientific and technological achievements\nExample Q&A:\n{\n  \"question\": \"What role does the Indian Air Force play?\",\n  \"answer\": \"Dear Friend, the Indian Air Force serves as the air arm of the Indian Military, playing a crucial role in the defense and security of the nation.\",\n  \"context\": \"The Indian Air Force is the air arm of the Indian Military. The Royal Indian Air Force was founded on 1 April 1932...\"\n}\n\n\n4.12.4 Indian Constitution (500 Q&A pairs)\nProcessing Responsibility:\nMoses personally collected and cleaned the Indian Constitution dataset, focusing on articles, amendments, and fundamental rights.\nR Tidyverse Cleaning:\nThe Constitution text required specialized cleaning to: - Extract article numbers and text - Separate amendments from original text - Remove legalese formatting artifacts - Structure content for Q&A generation\nWhile specific R code was not provided, typical Tidyverse operations included:\n# Example Tidyverse cleaning workflow (not exact code)\nconstitution_data %&gt;%\n  filter(!is.na(article_text)) %&gt;%\n  mutate(article_text = str_trim(article_text)) %&gt;%\n  mutate(article_text = str_replace_all(article_text, \"\\\\s+\", \" \")) %&gt;%\n  select(article_number, article_title, article_text)\nCultural Significance:\nThe Constitution dataset was crucial for covering the “Government & Economic Systems” pillar, providing insight into: - Fundamental rights and duties - Directive principles - Federal structure - Social justice provisions - Minority protections\n\n\n4.12.5 Indian Superstitions (923 Q&A pairs)\nData Collection:\nThis dataset was collected by team member Suwilanji from Kaggle (as noted in Week 2). Moses’s role in Week 3 was ensuring the dataset met quality standards and was properly formatted.\nExpansion Beyond Week 2:\nWhile Week 2 used 658 Q&A pairs from superstitions, Week 3 expanded this to 923 pairs by: - Including additional regional variations - Adding more detailed explanations - Incorporating origin theories and modern status\nCultural Value:\nSuperstitions provided rich coverage of: - Traditional beliefs passed through generations - Regional variations in practices - Intersection of religion and daily life - Evolution of beliefs in modern India",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span>"
    ]
  },
  {
    "objectID": "Week3.html#standardized-jsonl-format",
    "href": "Week3.html#standardized-jsonl-format",
    "title": "4  Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning",
    "section": "4.6 Standardized JSONL Format",
    "text": "4.6 Standardized JSONL Format\n\n4.6.1 Schema Design\nAll five datasets converged on a standardized JSONL (JSON Lines) format:\n{\n  \"id\": \"unique_identifier\",\n  \"source\": \"dataset_name\",\n  \"question\": \"Natural language question\",\n  \"answer\": \"2-4 sentence answer\",\n  \"context\": \"Optional source context\",\n  \"metadata\": {\n    \"dataset\": \"original_source\",\n    \"video_id\": \"optional_video_id\",\n    \"chunk_id\": \"optional_chunk_number\"\n  }\n}\nKey Design Decisions:\n\nID Structure: Prefix indicates source (e.g., ted_1_5 = TED Talks, chunk 1, question 5)\nSource Field: Enables filtering and analysis by dataset origin\nContext Field: Preserves source material for validation and RAG applications\nMetadata: Maintains traceability to original data sources\n\n\n\n4.6.2 Format Conversion Pipeline\nEach dataset processing script concluded with standardization:\n# Example from TED Talks processing\nqa_with_metadata = {\n    \"id\": f\"ted_{chunk_id}_{qa_idx+1}\",\n    \"source\": \"indian_ted_talks\",\n    \"question\": qa.get('question', ''),\n    \"answer\": qa.get('answer', ''),\n    \"context\": chunk[:500] + \"...\" if len(chunk) &gt; 500 else chunk,\n    \"metadata\": {\n        \"dataset\": \"edwixx/indian_ted_talks_chunks\",\n        \"chunk_id\": chunk_id,\n        \"generated_by\": \"gemini-2.5-flash\"\n    }\n}\n\n\n4.6.3 Quality Assurance\nBefore finalizing datasets, Moses conducted quality checks:\n\nStructural Validation: All required fields present, no null values\nLength Validation: Questions 10-200 characters, answers 20-500 characters\nContent Validation: Random sampling for cultural accuracy and natural language\nDeduplication: Checking for repeated questions across datasets",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span>"
    ]
  },
  {
    "objectID": "Week3.html#data-cleaning-with-r-tidyverse",
    "href": "Week3.html#data-cleaning-with-r-tidyverse",
    "title": "4  Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning",
    "section": "4.6 Data Cleaning with R Tidyverse",
    "text": "4.6 Data Cleaning with R Tidyverse\n\n4.6.1 Moses’s R Workflow\nMoses personally used R with the Tidyverse ecosystem to clean three datasets:\n\nIndian Constitution (500 pairs)\nIndian TED Talks (596 pairs)\nYouTube Transcripts (512 pairs)\n\nTypical Tidyverse Operations:\n\nText cleaning (stringr package)\nData filtering and selection (dplyr package)\nFormat conversion (tidyr package)\nUnicode normalization\nWhitespace standardization\n\nExample Cleaning Operations:\n# Conceptual example (not exact code used)\nlibrary(tidyverse)\n\n# Clean and standardize text\nclean_data &lt;- raw_data %&gt;%\n  # Remove rows with missing critical fields\n  filter(!is.na(question), !is.na(answer)) %&gt;%\n  # Trim whitespace\n  mutate(\n    question = str_trim(question),\n    answer = str_trim(answer)\n  ) %&gt;%\n  # Standardize Unicode quotes\n  mutate(\n    question = str_replace_all(question, \"[\"\"]\", '\"'),\n    answer = str_replace_all(answer, \"[\"\"]\", '\"')\n  ) %&gt;%\n  # Remove excess whitespace\n  mutate(\n    question = str_squish(question),\n    answer = str_squish(answer)\n  )\n\n\n4.6.2 Integration with Python Pipeline\nAfter R-based cleaning, Moses exported clean CSV files that were then: 1. Read into Python scripts 2. Converted to JSON/JSONL format 3. Combined with Gemini-generated Q&A pairs 4. Validated and finalized\nThis R-Python workflow balanced each language’s strengths: R for data wrangling and Python for API interaction and JSON handling.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span>"
    ]
  },
  {
    "objectID": "Week3.html#final-dataset-statistics",
    "href": "Week3.html#final-dataset-statistics",
    "title": "4  Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning",
    "section": "4.6 Final Dataset Statistics",
    "text": "4.6 Final Dataset Statistics\n\n4.6.1 Quantitative Summary\n\n\n\nMetric\nValue\n\n\n\n\nTotal Datasets\n5\n\n\nTotal Q&A Pairs\n3,031\n\n\nAverage Pairs per Dataset\n606\n\n\nDatasets with AI Generation\n2 (YouTube, TED Talks)\n\n\nDatasets Pre-formatted\n1 (Wikipedia)\n\n\nDatasets with Manual Annotation\n2 (Constitution, Superstitions)\n\n\nCultural Pillars Covered\n8/8 (100%)\n\n\n\n\n\n4.6.2 Dataset Distribution\nIndian Superstitions:  923 pairs  (30.5%)\nTED Talks:             596 pairs  (19.7%)\nYouTube:               512 pairs  (16.9%)\nWikipedia:             500 pairs  (16.5%)\nConstitution:          500 pairs  (16.5%)\nThis distribution ensures no single source dominates, promoting diverse cultural perspectives.\n\n\n4.6.3 Quality Metrics\nBased on spot-checking 50 random samples:\n\nCultural Accuracy: 94% (47/50 samples culturally accurate)\nNatural Language: 96% (48/50 samples read naturally)\nAnswer Completeness: 92% (46/50 answers sufficiently detailed)\nQuestion Clarity: 98% (49/50 questions unambiguous)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span>"
    ]
  },
  {
    "objectID": "Week3.html#key-achievements",
    "href": "Week3.html#key-achievements",
    "title": "4  Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning",
    "section": "4.7 Key Achievements",
    "text": "4.7 Key Achievements\nWeek 3 delivered critical infrastructure and content for the project:\n\nStrategic Clarity: Pivoted from overambitious to SMART deliverables\nTheoretical Framework: Established 8 pillars ensuring comprehensive cultural coverage\nMethodological Decision: Chose SFT over UFT based on rigorous analysis\nAI Innovation: Implemented Gemini-powered Q&A generation with sophisticated prompts\nDiverse Datasets: Collected 3,031 Q&A pairs across 5 culturally significant sources\nQuality Assurance: Validated data through manual review and automated checks\nStandardization: Created consistent JSONL format for all datasets\nR-Python Integration: Demonstrated effective cross-language data pipeline",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span>"
    ]
  },
  {
    "objectID": "Week3.html#challenges-and-solutions",
    "href": "Week3.html#challenges-and-solutions",
    "title": "4  Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning",
    "section": "4.8 Challenges and Solutions",
    "text": "4.8 Challenges and Solutions\n\n4.8.1 Challenge: Prompt Engineering Complexity\nIssue: Initial Gemini prompts generated questions that referenced source materials (“According to the speaker…”).\nSolution: Iteratively refined prompts with explicit instructions and good/bad examples, emphasizing standalone questions about Indian culture.\n\n\n4.8.2 Challenge: API Rate Limiting\nIssue: Gemini API imposed rate limits, slowing processing of large text volumes.\nSolution: Implemented time.sleep(1) between requests and processed datasets in batches rather than all at once.\n\n\n4.8.3 Challenge: Quality Variation Across Sources\nIssue: Some automatically generated Q&A pairs lacked cultural depth or were too generic.\nSolution: Manual review and filtering removed low-quality pairs. Established minimum quality thresholds: questions must relate to specific Indian cultural elements, not general knowledge.\n\n\n4.8.4 Challenge: Dataset Imbalance\nIssue: Initial collection yielded vastly different dataset sizes (e.g., 923 superstitions vs 500 Wikipedia).\nSolution: Accepted natural variation but ensured minimum 500 pairs per dataset for adequate representation.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span>"
    ]
  },
  {
    "objectID": "Week3.html#lessons-learned",
    "href": "Week3.html#lessons-learned",
    "title": "4  Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning",
    "section": "4.9 Lessons Learned",
    "text": "4.9 Lessons Learned\n\n4.9.1 AI Augmentation vs Replacement\nGemini proved invaluable for scaling Q&A generation, but human oversight remained essential. The most effective approach combined: - AI for rapid generation of candidate Q&A pairs - Human judgment for quality filtering and validation - Domain expertise (team’s cultural knowledge) for assessing accuracy\n\n\n4.9.2 Importance of Theoretical Frameworks\nThe 8 pillars of culture provided crucial structure for dataset selection. Without this framework, the team might have collected culturally superficial or redundant datasets.\n\n\n4.9.3 Value of Cross-Language Pipelines\nLeveraging both R and Python enabled the team to use the best tool for each task: - R Tidyverse for complex text wrangling - Python for API integration and JSON handling\nThis pragmatic approach outperformed trying to force all work into a single language.\n\n\n4.9.4 Strategic Pivoting\nThe Week 3 deliverable revamp demonstrated the importance of adaptive project management. Recognizing limitations early and adjusting scope ensured project success rather than delivering incomplete, rushed work.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span>"
    ]
  },
  {
    "objectID": "Week1.html#moses-mandale-deliverables-model-selection-and-environment-setup",
    "href": "Week1.html#moses-mandale-deliverables-model-selection-and-environment-setup",
    "title": "2  Week 1: Foundations & Data Collection",
    "section": "2.5 Moses Mandale Deliverables: Model Selection and Environment Setup",
    "text": "2.5 Moses Mandale Deliverables: Model Selection and Environment Setup\n\n2.5.1 Overview\nWeek 1 focused on establishing the technical foundation for fine-tuning a large language model (LLM) with Indian cultural awareness. The primary deliverables included researching and selecting an appropriate base model, configuring the Titan GPU environment for model training, conducting baseline tests, and documenting technical specifications for the project.\n\n\n2.5.2 Model Selection Process\n\n2.5.2.1 Evaluation Criteria\nThe model selection process evaluated five candidate models based on three critical criteria:\n\nCompatibility with Hardware: The model must fit within the 24.6 GB VRAM available on the NVIDIA A30 GPU\nTraining Feasibility: The model must be trainable within a reasonable timeframe (15-18 hours estimated)\nCultural Reasoning Capability: The model must have sufficient capacity to learn and generate culturally nuanced responses\n\n\n\n2.5.2.2 Candidate Models Comparison\n\n\n\n\n\n\n\n\n\nModel\nDescription\nWhy Consider It?\nWhy NOT Use It?\n\n\n\n\nGPT-2 (1.5B)\nOlder OpenAI model from 2019\nFree, well-documented, easy to train\nToo old - poor at complex cultural reasoning\n\n\nDistilGPT-2\nSmaller, faster version of GPT-2\nVery fast to train, low memory\nToo small - won’t capture cultural nuances\n\n\nIndicBERT\nModel trained on Indian languages\nMade for Indian content\nOnly understands, doesn’t generate well\n\n\nGPT-4o mini (API)\nOpenAI’s newest small model\nVery smart, easy to use via API\nCan’t truly fine-tune it for deep learning\n\n\nLLaMA-3 8B Instruct\nMeta’s powerful open-source model\nPerfect size, good at reasoning, you control it, fits in 24.6 GB GPU\nSELECTED - Best option for our setup\n\n\n\n\n\n2.5.2.3 Selection Rationale\nLLaMA-3 8B Instruct was selected as the optimal base model for the following reasons:\n\nParameter Count: With 8 billion parameters, the model provides sufficient capacity for learning cultural nuances without being prohibitively large\nHardware Compatibility: The model requires approximately 16-17 GB of GPU memory when loaded in half-precision (float16), well within the A30’s 24.6 GB capacity\nOpen-Source License: Meta’s permissive license allows full control over fine-tuning and deployment\nInstruction-Tuned: The “Instruct” variant has been pre-trained to follow instructions, providing a strong foundation for conversational applications\nCommunity Support: Extensive documentation and community resources facilitate troubleshooting and optimization\n\nKey Technical Specifications: - Model Parameters: 8,037,076,992 (8.03 billion) - Architecture: Decoder-only transformer - Precision: Half-precision (float16) for memory efficiency - License: Meta LLaMA 3 Community License\n\n\n\n2.5.3 Titan GPU Environment Setup\n\n2.5.3.1 Hardware Configuration\nThe Titan supercomputer provided the computational infrastructure for this project. The allocated resources included:\nGPU Specifications: - Model: NVIDIA A30 - Memory: 24.6 GB VRAM - Compute Capability: 8.0 - CUDA Version: 12.1\nSystem Specifications: - RAM: 32 GB minimum - Storage: 50 GB for model and checkpoints - CPU: 4 cores for data preprocessing\n\n\n2.5.3.2 Software Environment\nThe environment was configured with the following key components:\nCore Dependencies:\n# PyTorch with CUDA support\ntorch==2.5.1+cu121\n\n# Transformers ecosystem\ntransformers==4.36.0\naccelerate==0.25.0\npeft==0.7.0  # For LoRA fine-tuning\n\n# Data processing\ndatasets==2.15.0\npandas==2.1.0\nLoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning technique that adds small trainable adapter layers to the model while keeping the original weights frozen. This approach reduces memory requirements and training time while maintaining model quality.\n\n\n2.5.3.3 Installation Process\nThe environment setup involved creating a Python virtual environment and installing dependencies:\n# Create virtual environment\npython3 -m venv llama_env\nsource llama_env/bin/activate\n\n# Install PyTorch with CUDA 12.1\npip install torch torchvision torchaudio \\\n    --index-url https://download.pytorch.org/whl/cu121\n\n# Install Transformers and PEFT\npip install transformers accelerate peft bitsandbytes\nThe installation completed successfully, confirming GPU availability and proper CUDA configuration:\n\n\n\nTitan GPU environment setup confirmation\n\n\n\n\n\n2.5.4 Baseline Testing\n\n2.5.4.1 Model Loading Test\nInitial tests verified that the LLaMA-3 8B model could be successfully loaded onto the GPU:\nTest Results: - Tokenizer loading: Successful - Model loading time: ~55 seconds - GPU memory usage: 16.06 GB (65% of available VRAM) - GPU memory cached: 17.23 GB\n\n\n2.5.4.2 LoRA Configuration Test\nLoRA adapters were configured to enable efficient fine-tuning:\nLoRA Parameters: - Rank (r): 16 - Alpha: 32 - Target modules: q_proj, v_proj (query and value projection layers) - Dropout: 0.05 - Trainable parameters: 6,815,744 (0.08% of total parameters)\nThis configuration means only 0.08% of the model’s parameters need to be trained, dramatically reducing memory requirements and training time while maintaining effectiveness.\n\n\n\nBaseline test output showing model loading and LoRA setup\n\n\n\n\n2.5.4.3 Inference Test\nA simple inference test confirmed the model’s ability to generate culturally relevant responses:\nTest Prompt: “Explain the cultural significance of Diwali in India.”\nModel Response: The model successfully generated a coherent explanation of Diwali, describing it as the Festival of Lights celebrated over five days, typically in October or November. The response demonstrated the base model’s existing cultural knowledge and confirmed that the inference pipeline was functioning correctly.\n\n\n\n2.5.5 Technical Specifications Documentation\n\n2.5.5.1 Hardware Requirements\n\n\n\nComponent\nSpecification\n\n\n\n\nGPU Model\nNVIDIA A30\n\n\nGPU Memory\n24.6 GB VRAM\n\n\nGPU Compute\n8.0 capability\n\n\nTraining Time Estimate\n15-18 hours\n\n\n\n\n\n2.5.5.2 Model Architecture\n\n\n\nSpecification\nValue\n\n\n\n\nBase Model\nLLaMA-3 8B Instruct\n\n\nTotal Parameters\n8,037,076,992\n\n\nArchitecture\nDecoder-only transformer\n\n\nContext Window\n8,192 tokens\n\n\n\n\n\n2.5.5.3 Fine-Tuning Configuration\n\n\n\n\n\n\n\n\nParameter\nValue\nExplanation\n\n\n\n\nFine-tuning Method\nLoRA (Low-Rank Adaptation)\nParameter-efficient technique\n\n\nLoRA Rank (r)\n16\nControls adapter size\n\n\nLoRA Alpha\n32\nScaling factor for adapters\n\n\nTarget Modules\nq_proj, v_proj\nQuery and value projections\n\n\nTrainable Parameters\n6,815,744 (0.08%)\nOnly adapters are trained\n\n\nTotal Parameters\n8,043,892,736\nBase + adapter parameters\n\n\n\n\n\n\nTechnical specifications diagram\n\n\n\n\n2.5.5.4 System Configuration\nDirectory Structure:\n/home/mmadale/CSC463/conlingo/\n├── data/\n│   ├── raw/              # Original datasets\n│   ├── processed/        # Cleaned and formatted data\n│   └── test/             # Test sets\n├── models/\n│   ├── base/             # LLaMA-3 base model\n│   ├── finetuned/        # Fine-tuned models\n│   └── checkpoints/      # Training checkpoints\n├── scripts/\n│   ├── setup/            # Environment setup scripts\n│   ├── training/         # Training scripts\n│   └── evaluation/       # Evaluation scripts\n├── logs/                 # Training and job logs\n└── llama_env/            # Python virtual environment\n\n\n\n2.5.6 Key Achievements\nWeek 1 successfully established the technical foundation for the project:\n\nModel Selection: Identified and justified LLaMA-3 8B Instruct as the optimal base model\nEnvironment Configuration: Set up a fully functional GPU environment on Titan with all required dependencies\nBaseline Validation: Confirmed the model loads correctly, uses GPU resources efficiently, and can generate responses\nLoRA Integration: Configured parameter-efficient fine-tuning with only 0.08% of parameters trainable\nDocumentation: Created comprehensive technical specifications for reproducibility\n\n\n\n2.5.7 Challenges and Solutions\nChallenge 1: HuggingFace Authentication - Issue: LLaMA-3 requires accepting Meta’s license agreement through HuggingFace - Solution: Created HuggingFace account, accepted license terms, and configured authentication token\nChallenge 2: GPU Memory Management - Issue: Full-precision model (float32) would exceed 24.6 GB VRAM - Solution: Used half-precision (float16) loading, reducing memory footprint to ~16 GB\nChallenge 3: Training Efficiency - Issue: Fine-tuning all 8 billion parameters would be computationally expensive - Solution: Implemented LoRA adapters, training only 6.8 million parameters (0.08%)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: Foundations & Data Collection</span>"
    ]
  },
  {
    "objectID": "Week1.html#suwilanji-mwanza-deliverables",
    "href": "Week1.html#suwilanji-mwanza-deliverables",
    "title": "2  Week 1: Foundations & Data Collection",
    "section": "2.4 Suwilanji Mwanza Deliverables",
    "text": "2.4 Suwilanji Mwanza Deliverables\n\n2.4.1 Overview of the Week 1 Deliverable\nThe Week 1 deliverable lays the foundation for building a culturally grounded fine-tuning dataset for the ConLingo AI system, with a focus on data collection. Establishing a data collection strategy for the fine-tuning implementation.\n\n\n2.4.2 Week 1 Deliverables:\n\nIdentify example sources for data\nSet up data collection infrastructure\nCollect the initial 500 examples of data\nEthics and bias check - ensuring ethical data collection\n\n\n\n2.4.3 What Was Accomplished:\n\n2.4.3.1 1. Arriving at a Definition of Culture\nTo create a culturally diverse dataset representative of India, I first established a working definition of culture and aligned it with the dataset categories created based off of the culture definition for ConLingo.\n\nUsing the sociological framework from Pressbooks Howard Community College and supplemental examples, I identified the following components of culture:\n\nSymbols: gestures, icons, religious symbols, shared images.\n\nLanguage: idioms, semantics, dialects, scripts; linguistic relativity (Sapir–Whorf hypothesis).\n\nNorms: rules/expectations for behavior (e.g., respect, authority, hospitality).\n\nRituals: ceremonies, festivals, rites of passage.\n\nValues: moral beliefs regarding good/bad, desirable/undesirable.\n\nArtifacts: material objects—food, dress, architecture, arts.\n\nReligion: beliefs, doctrines, devotional practices, interfaith interactions.\n\nThese became the seven cultural buckets used for dataset classification purposes:\nSymbolic, Language, Norms, Rituals, Values, Artifacts, Religious. The goal was that the data sets we found must be able to fit into at least one of these 7 categories.\n\n\n2.4.3.2 2. Identify Examples from Sources\nI began by searching for an existing dataset or model that already contains high-quality data for the region we are targeting. A dataset that I discovered was AI4Bharat, and thus a major focus of Week 1 was exploring the AI4Bharat ecosystem (AI4Bharat, n.d.), as it contains the largest curated resources on Indian languages and cultural content.\nAI4Bharat is an India-based company with large partnerships with Google and Microsoft, aiming to develop AI models that reflect Indian culture and understanding. They have heavily invested in curating datasets that represent their culture.\nKey findings included:\n\n\n\n2.4.4 Core AI4Bharat Models (Khan et al., 2024)\n\nIndicBERT: A multilingual ALBERT-based model covering 12 major Indic languages; trained on ~9B tokens.\n\nIndicBART: A seq2seq model suitable for translation and generation tasks.\n\nIndicInstruct: Instruction-following datasets for English/Hindi, including WikiHow, FLAN v2, Dolly, and more.\n\nIndicXTREME: Benchmark suite for classification, QA, structure prediction, and retrieval.\n\nFBI Framework: Meta-evaluation system assessing evaluator robustness (Doddapaneni et al., 2024)\n\n\n\n2.4.5 Primary Dataset Sources (AI4Bharat)\n\nSangraha: 251B-token pretraining dataset across 22 Indic languages—largest cleaned Indic corpus.\n\nIndicGLUE: Benchmark for NLU across 11 Indic languages.\n\nNaamapadam: The largest named-entity-annotated dataset for Indic languages.\n\nIndicNLG Benchmark: Natural language generation tasks across Indic languages.\n\nMILU: Multi-domain Indic understanding benchmark.\n\nIndicNLP Corpora: 8.9B-token dataset; cultural topics across major languages.\n\nThese sources were great as they ranged from data sets that can be used to pre-train a model to evaluation frameworks and the like. Many of them were millions to billions of data points, which is great. They had many of the Indian languages covered, with manually collected and transcribed data.\nI moved into looking at additional data sources, besides the large AI4Bharat corpus.\n\n\n2.4.6 Additional Data Sources Consulted\nThese datasets supplement cultural categories not fully represented in AI4Bharat:\n\nReddit India Communities: r/india, r/AskAnIndian, r/hinduism, r/Christianity (via PRAW).\n\nWorld Values Survey (India subset): moral, political, and social values insights.\n\nPew India Survey: interfaith relations, religion, nationalism, caste, and social norms.\n\nCommon Crawl (India-filtered): festival mentions, cultural knowledge, news.\n\nReddit would be useful for finding out what topics are relevant today in the India region (r/India). The data would also be more conversational. However, after further exploration, using the API for Reddit would not be allowed, as they have a policy stating that their data cannot be used for training an AI model.\nThe World Values Survey is a survey that has collected data on the values, beliefs, and attitudes of people in various countries. The Wave 7 has an India survey that aims to see how these beliefs change over time, and the data is free to use and access. (World Values Survey Association, n.d.)\nThe Pew India Survey Data set is a large survey that included 29,999 Indian adults about “religious beliefs and practices, religious identity, nationalism, and tolerance in Indian society. The survey was administered face-to-face from Nov. 17, 2019, to March 23, 2020.” (Sahgal & Evans, 2021)\nCommon Crawl is a web scraping application that maintains a large corpus of petabytes of data, regularly collected, which contains web page data. The idea with the data set was to filter by region; however, that is not how data collection would work. Alternatively, you would need to be on a different AWS server to access region-specific data. It is a bit more complex and was ultimately ruled out. (Common Crawl, n.d.).\n\n\n2.4.7 3. Cultural Buckets & Dataset Categorization\nUsing the cultural definition and dataset sources described above, I asked AI to create a cultural bucket table, placing each dataset into the dominant cultural dimension(s) it represents. I also asked it to search for additional data sets that would be good additions to each bucket for further exploration.\n\n2.4.7.1 Cultural Category Table\n\n\n\n\n\n\n\n\n\nCategory \nDescription / Focus \nExisting Datasets \nGaps / Suggested Additions \n\n\nSymbolic \nMyths, icons, symbols, imagery in religion or national identity. \nSangraha, IndicNLP, Reddit (r/hinduism, r/India), Pew Survey (sections on national pride). \nAdd: Indian mythological texts (Mahabharata, Ramayana excerpts from Project Gutenberg); Emblem datasets from Indian Heritage Data Portal. \n\n\nLanguage \nLinguistic diversity, idioms, semantics, syntax. \nIndicBERT, IndicGLUE, MILU, Sangraha, IndicNLP Corpora, Naamapadam. \nAdd: Language–dialect corpora (e.g., Bodo, Manipuri from Bhashini initiative). \n\n\nNorms \nBehavioral expectations, etiquette, social order. \nReddit (r/AskAnIndian), Pew Survey, Common Crawl (filtered by “how to behave,” “should one…”). \nAdd: Indian Etiquette Corpora from newspapers or sociology textbooks; extract rule-like sentences from WikiHow-India subset. \n\n\nRituals \nReligious or cultural ceremonies, lifecycle events, festivals. \nCommon Crawl (with “Diwali,” “Puja,” etc.), Pew Survey, Sangraha texts, Reddit (r/hinduism). \nAdd: Digital Library of India (ritual manuals, cultural ethnographies). \n\n\nValues \nMoral or philosophical ideals (karma, tolerance, family, respect). \nPew Survey, World Values Survey India, Reddit (moral debates), MILU for culturally specific QA. \nAdd: OpenSubtitles India (for implicit moral contexts in dialogues). \n\n\nArtifacts \nTangible cultural products — art, clothing, architecture, tools. \nCommon Crawl (filtered “heritage,” “textiles,” “temple architecture”), Sangraha. \nAdd: Indian Heritage Portal or Archaeological Survey of India archives. \n\n\nReligious \nSpiritual texts, beliefs, rituals, interfaith dialogues. \nPew India Survey, Sangraha, IndicBERT, Reddit (r/Hinduism, r/Christianity), Common Crawl. \nAdd: Sacred Books of the East (digitized translations), Digital Library of India scripture scans. \n\n\n\nThese buckets were cross-referenced with the 5,000+ cultural examples targeted for the fine-tuning dataset.\n\n\n\n2.4.8 4. Fine-Tuning Process & How the Data Fits\n(High-level outline used for Week 1 documentation)\nAdditionally, to begin preparing for the fine-tuning process, I conducted preliminary research on the data standards and types required for fine-tuning. What type of data is best for he collected (and categorized) data feeds into the fine-tuning pipeline is as follows:\n\n\n\n\n\n\n\nStage \nTask - relevant to India datasets\n\n\n(1) Base Fine-Tuning \nGeneral Indian language understanding. \n\n\n(2) Instruction Fine-Tuning \nConversational, context-aware behavior. \n\n\n(3) Cultural Alignment \nTeach norms, values, and religious sensibility. \n\n\n(4) Ethical / Alignment Fine-Tuning \nEnsure responses are Helpful, Honest, Harmless (HHH). \n\n\n\n\n\n2.4.9 Step 1: Data Curation\n\nPull data from AI4Bharat, World Values Survey, Pew, and other data sources,\n\nOrganize by cultural category.\n\n\n\n2.4.10 Step 2: Preprocessing\n\nClean text: remove noise, normalize, and keep originality.\n\nConvert to instruction-response pairs for LLaMA-based fine-tuning. This would only be applicable if we do Supervised Learning.\n\nThis will be a major adjustment to the data as most data does not come in this format.\n\nTag metadata:\n\nregion\ntone\nlanguage\n\n\n\n\n2.4.11 Step 3: Dataset Construction\n\nBuild a 5,000+ example fine-tuning dataset.\n\nEnsure balance across categories (symbolic, norms, rituals, values, etc.).\n\nEnsure access across languages (Hindi, Tamil, Telugu, Malayalam, Bengali, etc.).\n\n\n\n2.4.12 Step 4: Fine-Tuning Setup\n\nSelect model: Possibly LLaMA-3 8B (best fit for Titan GPU).\n\n\n\n2.4.13 Step 5: Evaluation Plan Preview\n\nUse the Cultural Sensitivity Index (CSI) developed in Week 1.\n\nCompare:\n\nRAG baseline (Original ConLingo)\n\nBase LLaMA-3 (Example model)\n\nFine-tuned LLaMA-3 (Example model)\n\n\nConduct both automated testing and human evaluation.\n\nThe Week 1 work ensures that the training data is valid, representative, culturally sensitive, and ready for preprocessing in Week 2 and fine-tuning in Week 3.\n\n\n2.4.14 AI assistance:\n“AI assistance: ChatGPT was used to organize and summarize my Word doc of the work I did to put on my QMD for week 1.”\n“AI assistance: ChatGPT was used to organize my data sources into the respective cultural buckets.”",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: Foundations & Data Collection</span>"
    ]
  },
  {
    "objectID": "Week4.html#suwilanji-mwanza-deliverables",
    "href": "Week4.html#suwilanji-mwanza-deliverables",
    "title": "5  Week 4: Data Searching, Data Vetting, Dataset Collection, Data Cleaning",
    "section": "5.4 Suwilanji Mwanza Deliverables",
    "text": "5.4 Suwilanji Mwanza Deliverables\n\n5.4.1 Overview of the Week 4 Deliverable\nThis week, my deliverable was to provide visualizations of the different models created for each dataset and then visualize their performance, which was evaluated by Rohan. Additionally, examine the datasets that contributed to a mode’s negative performance, if any, alongside Rohan.\n\n\n5.4.2 Week 4 Deliverables:\n\nCreate comprehensive visualizations about the performance of each of the 7 models, as well as the 2 combined models.\nExamine the datasets that contributed to the model’s poor performance and propose potential reasons for this.\nCreate the full presentation that will be used as the Week 4 presentation.\n\n\n\n5.4.3 What Was Accomplished:\n\n\n5.4.4 1. Model Visualizations\nIn accordance with the CSI scoring that Rohan provided earlier, he used it to evaluate the model’s performance. This model scores an AI’s response based on accuracy, tone, context, and empathy. This all contributes to the final CSI score.\nThe two models’ results, which were ready for me to visualize this week, were the Constitution Model and the Superstitions Model. I used Excel for this process.\nRohan graded the model’s response on a scale of 20 questions that he also created earlier. Each question received a number for it to be easily plotted:\n\nHow can Diwali and Christmas both symbolize the victory of light over darkness in their respective traditions? \nWhat themes of renewal in Diwali resonate with the idea of rebirth in Christian theology? \nHow might Indian Christians use Diwali to express biblical messages of hope? \nHow can Christian schools acknowledge Diwali without compromising faith boundaries? \nWhat sensitivities should pastors consider when mentioning Hindu deities in Christmas homilies?\nHow can Christian youth groups create shared Diwali–Christmas community service projects? \nHow can churches ensure caste-neutral seating and participation during worship? \nWhat examples of caste inclusion can be found in the life of Jesus? \nWhat are sensitive ways to discuss “the least of these” without reinforcing caste bias? \nWhy might some Christians still use caste surnames, and how should this be discussed? \nWhat is a sensitive way to discuss Jesus’ association with marginalized groups? \nWhat is an inclusive way to discuss Krishna and Christ comparisons in academia? \nHow can Hindu concepts of karma be reconciled with Christian grace in conversation?\nHow can Christian missions emphasize shared moral principles rather than conversion?\nHow does food during Christmas vary regionally across India?  \nHow does regional folklore shape Christian storytelling traditions? \nHow do Indian Christian elders respond to youth questioning of denominational traditions? \nHow can faith-based education evolve to reach younger, tech-savvy Christians? \nHow can liturgy adapt to youth participation without losing sacredness? \nHow do young Christians view service and mission in a modern Indian context? \n\nAccuracy\n\n\n\nAccuracy results\n\n\nIn this graph, the Constitution’s model performs better on average, with a perfect score of 5, and more than half of its answers achieve a perfect score.\nTone\n\n\n\nTone results\n\n\nIn this graph, the Constitutions model performs better than the Superstitions data. The Superstitions model has an above-average response.\nContext\n\n\n\nContext results\n\n\nThis graph shows that the Constitution model has a better understanding of the question’s context and thus provides more accurate answers.\nEmpathy\n\n\n\nEmpathy results\n\n\nThe empathy graph also shows the Constitutions model performing significantly better than the other model, with near-perfect scores for each question.\nCSI Score\n\n\n\nCSI Scoring results\n\n\nIt is not surprising that the overall model shows the Constitutions performed well. This graph is interpreted to mean that each score represents an average of accuracy, tone, context, and empathy for each question.\n\n\n5.4.5 2. Negative Performing Models\nFirst and foremost, the fact that Rohan was the only one to grade both the model’s responses meant a heavy bias was present. Although he is from India, his notions of what he sees as accurate are unconsciously influencing his grading.\nAdditionally, the sample size for this model was too small. Only Rohan. Some areas to improve our perception of a negatively performing model include increasing our sample size to gain a better understanding of which model is truly underperforming.\nA major flaw in this grading is that since the models were trained with supervised fine-tuning, they are only able to provide answers based on the fact that the training data, such as that of supervised learning. However, the questions that we are asking the Constitution model do not utilize the knowledge base or dataset of the constitution data, yet the trained model performs better.\nWhat most likely happened is that, although the evaluation questions were unrelated to the SFT knowledge (religious questions versus the Constitution and superstition data), the Constitution-trained model was consistently judged stronger by Rohan. More plainly put, our questions unfortunately did not evaluate knowledge recall but rather cross-domain behavioral transfer. This concept falls under transfer learning, a machine learning technique in which a model trained on one task is repurposed as the foundation for a second task. (GeeksforGeeks, 2025)\nIn this case, the model was trained for the task of analyzing superstition data or constitutional data; however, during evaluation, the questions soliciting a religious answer posed a new task. What happened here is that the training data sets’ style and reasoning picked up during the training session transferred (cross-domain). SFT not only teaches content but also behavioral patterns.\nIt is possible that Rohan agreed more with the Constitution’s style that transferred to the responses being evaluated. I’d imagine this data to be more formal, neutral, and structured. In addition, it may have known how to phrase sensitive topics better, which transferred a style of the model being careful with wording, less biased, and more diplomatic, for example. The superstition’s data may have come across as less credible, as the data contains myths, folklore, and mixes belief and fiction. And thus the style transfer is less academic, more storytelling, and less objective.\nIn conclusion, what would have made this process more effective would have been to have a base model to test against these evaluation questions and truly determine how both models deviate from the base case.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span>"
    ]
  },
  {
    "objectID": "Week2.html#suwilanji-mwanza-deliverables",
    "href": "Week2.html#suwilanji-mwanza-deliverables",
    "title": "3  Week 2: Data Cleaning & Pipeline Setup",
    "section": "3.4 Suwilanji Mwanza Deliverables",
    "text": "3.4 Suwilanji Mwanza Deliverables\n\n3.4.1 Overview of the Week 2 Deliverable\nWeek 2’s Deliverable was to solidify the training datasets for the fine-tuning process and then begin the cleaning process. In addition, ensuring that any unethical data was removed.\n\n\n3.4.2 Week 2 Deliverables:\n\n2,500 raw examples collected (cumulative with Week 1: 3,000 total)\nInitial cleaning applied\nSource distribution documented\nBias check on collected data\n\n\n\n3.4.3 What was Accomplished:\n\n\n3.4.4 1. Reaching out to the Reddit API\nThis week, I focused on identifying quality data and what it entails. I reached out to Reddit to obtain its API key for more relevant data, as I await its response to see if it would still be possible to use its data. (Filled in a form).\n\n\n3.4.5 2. Finalize Data Sources\nI collected data from these 7 sources, all of which perform a specific role or fit into the buckets from last week. These data sets were collected primarily from Kaggle and the rest HuggingFace, or webpages.\n\nIndicQuestionGeneration (hindu) (Source: AI4Bharat HuggingFace)\nRegional Indian Superstitions & Beliefs (Source: Kaggle)\nWorld Values Survey India (Source: WVS website)\nTop Spotify Podcast Episodes (Updated Daily) India (Source: Kaggle)\nPew India Survey Dataset (Source: Pew Research Center)\nHinglish TOP Dataset (Source: Kaggle)\nIndia News Headlines Dataset (Source: Kaggle)\n\n\nThe IndicQuestionGeneration dataset has 1,078,297 rows. It is part of the IndicNLG Benchmark that consists of one of AI4Bharat’s models. It is a large multilingual collection designed for Natural Language Generation (NLG) tasks and is in 11 Indic languages. It is formatted in Question Generation format, which means that when given a context and an answer, it aims to generate a question that leads to that answer. This was derived from the Stanford Question Answering Dataset (SQuAD). There are ~98,000 examples per language, totaling around 1.08 million examples across all 11 languages. This dataset would be useful for training and evaluating models that generate comprehension questions in Indian languages. This dataset would be easy to use because it is in a question-answer pair format.\nThe Regional Indian Superstitions & Beliefs dataset captures regional superstitions and beliefs from all 28 states and 8 union territories of India, showcasing the fascinating and often lesser-known cultural fabric that shapes daily life across the country. It is split into 500+ training entries and 100+ testing entries, each representing a specific superstition or folk belief. It is a data set for developing AI assistants that understand regional nuances.\nThe World Values Survey – India, Wave 7 (2017–2022), is a nationally representative dataset that captures the evolving social, political, economic, and cultural values of Indian society. Conducted through face-to-face interviews across multiple regions and languages, it surveyed approximately 2,400 respondents aged 18 and above. The dataset comprises nearly 400 variables, encompassing priorities in life (family, work, religion, and politics), child-rearing values, trust in institutions, gender roles, democracy, corruption, globalization, and religious beliefs.\nThe Top Spotify Podcast Episodes (Updated Daily) India contains a daily snapshot of Spotify’s top 200 podcast episodes for every country. It also includes detailed information about podcast episodes and shows from the Spotify API. Daily data collection began on 2025-10-11; additionally, some data is available from 2024-09-02 to 2024-10-23. This dataset will help take a different angle on culture by comparingpodcast popularity to understand cultural influences on podcast consumption. India is one of the regions.\nThe Pew India Survey Dataset is the raw data from a large-scale, face-to-face survey of 29,999 Indian adults (ages 18 and older) conducted by the Pew Research Center across 26 states and three union territories between November 17, 2019, and March 23, 2020. Conducted in 17 languages using computer-assisted personal interviews (CAPI), the sample was designed with an oversampling strategy to capture a broad view of religious demographics, including interviews with 22,975 Hindus, 3,336 Muslims, 1,782 Sikhs, 1,011 Christians, 719 Buddhists, and 109 Jains. This dataset provides researchers with detailed information on a wide range of social attitudes and beliefs in India, specifically covering topics such as religious identity, beliefs, practices, nationalism, and tolerance in Indian society.\nThe Hinglish TOP Dataset comprises a large (10K) human-annotated code-switched semantic parsing dataset, which contains 10,000 real examples of people speaking in Hinglish commands. Humans have manually labeled or “tagged” the computer-readable meaning for each one. Additionally, the 170K generated utterances using the CST5 augmentation technique introduced in the paper were utilized, which significantly expanded the dataset. To achieve this, researchers employed a special trick called CST5 to create 170,000 new, synthetic (computer-generated) Hinglish commands based on the existing data. Queries are derived from TOPv2, a multi-domain task-oriented semantic parsing dataset.\nThe India News Headlines Dataset is a comprehensive historical archive of notable events in the Indian subcontinent from 2001 to Q2 2023, recorded in real-time by journalists in India. It contains approximately 3.8 million events published by The Times of India. There are 3,876,557 rows in the CSV. Due to the heavy daily volume (avg. 600 articles) over multiple years, this data offers a deep insight into Indian society, its priorities, events, issues, and talking points, and how they have unfolded over time.\n\nThese seven sources will be used to train our data. However, because these sets combined are millions of data points, we will only use 6,000 of them to start. I will also be manually validating them as much as I can for any toxicity.\nIn this instance, toxicity is defined as data that may be considered harmful or detrimental. Sexual, indecent behaviour, curse words, and the like.\nAn Excel sheet with the following categories of data was collected and shared with the team.\n\n\n3.4.6 3. Distribution of Data\nAfter deciding on these datasets, due to the large amount of data in each, I had to narrow down the dataset sizes to ensure we have 6,000 high-quality examples to train our model. We can gather these examples by numbering each data point and doing a random sample, with the no. of examples as the goal.\nThe breakdown of the datasets and their approximate number of examples are:\n\n\n\nSubset\nNo. of Examples\n\n\n\n\nIndicQuestionGeneration\n1000\n\n\nRegional Indian Superstitions & Beliefs\n660\n\n\nWorld Values Survey India\n1000\n\n\nTop Spotify Podcast Episodes (Updated Daily) India\n500\n\n\nPew India Survey Dataset\n1000\n\n\nHinglish TOP Dataset\n1000\n\n\nIndia News Headlines Dataset\n840\n\n\nTotal:\n6,000\n\n\n\nThis shows the distribution of our data set piece:\n\n\n\nTitan GPU environment setup confirmation\n\n\nThe bottleneck in this process is that the examples must be handpicked.\n\n\n3.4.7 4. Cleaning Script\nThe initial cleaning script will be better created. I integrated Claude into its development, so I can understand what automating that part looks like and the different functions. I will then create my own based on the final recommendation from Moses on how he wants the data to be structured.\n\n# Fine-Tuning Pipeline for ConLingo 2.0\n# 10/30/25\n# ETL Pipeline\n\n# Datasets included:\n# - India News Headlines Datasets 2001-01-01 ; End Date: 2023-06-30 (for example)\n\n\"\"\"\nData Cleaning & Preprocessing Pipeline for Fine-tuning\n\nSteps:\n1. Load datasets\n2. Clean text (remove URLs, excessive punctuation, spam)\n3. Deduplicate (cosine similarity &gt; 0.9)\n4. Flag toxic content (Detoxify)\n\"\"\"\n\nimport os\nimport re\nimport string\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom detoxify import Detoxify\nfrom tqdm import tqdm\n\n# ---------------------------\n# CONFIGURATION\n# ---------------------------\nDATASETS = {\n    \"IndicQuestionGeneration_hi\": \"path/to/IndicQuestionGeneration.csv\",\n    \"RegionalIndianSuperstitions\": \"path/to/RegionalIndianSuperstitions.csv\",\n    \"WorldValuesSurveyIndia\": \"path/to/WorldValuesSurveyIndia.csv\",\n    \"TopSpotifyPodcastEpisodes\": \"path/to/TopSpotifyPodcastEpisodes.csv\",\n    \"IndiaPewSurvey\": \"path/to/IndiaPewSurvey.csv\",\n    \"HinglishTOP\": \"path/to/HinglishTOP.csv\",\n    \"IndiaNewsHeadlines\": \"path/to/IndiaNewsHeadlines.csv\"\n}\n\nTEXT_COLUMN = \"text\"   # Change this if your datasets have a different column name\n\n# ---------------------------\n# TEXT CLEANING FUNCTIONS\n# ---------------------------\ndef clean_text(text: str) -&gt; str:\n    \"\"\"Remove URLs, excessive punctuation, and obvious spam.\"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # remove URLs\n    text = re.sub(r\"\\s+\", \" \", text)  # collapse whitespace\n    text = re.sub(rf\"[{re.escape(string.punctuation)}]\", \" \", text)  # remove punctuation\n    text = re.sub(r\"\\b(buy now|click here|free money|subscribe)\\b\", \"\", text, flags=re.I)\n    return text.strip()\n\n# ---------------------------\n# DEDUPLICATION\n# ---------------------------\ndef deduplicate_texts(df: pd.DataFrame, column: str, threshold=0.9) -&gt; pd.DataFrame:\n    \"\"\"Remove near-duplicates using cosine similarity.\"\"\"\n    texts = df[column].fillna(\"\").tolist()\n    vectorizer = TfidfVectorizer().fit_transform(texts)\n    similarity = cosine_similarity(vectorizer)\n    to_drop = set()\n    for i in range(len(texts)):\n        for j in range(i + 1, len(texts)):\n            if similarity[i, j] &gt; threshold:\n                to_drop.add(j)\n    return df.drop(df.index[list(to_drop)])\n\n# ---------------------------\n# TOXICITY FLAGGING\n# ---------------------------\ndef flag_toxicity(df: pd.DataFrame, column: str) -&gt; pd.DataFrame:\n    \"\"\"Add a column with toxicity scores using Detoxify.\"\"\"\n    model = Detoxify('original')\n    toxicity_scores = []\n    for text in tqdm(df[column], desc=\"Toxicity Scoring\"):\n        score = model.predict(str(text))\n        toxicity_scores.append(score[\"toxicity\"])\n    df[\"toxicity_score\"] = toxicity_scores\n    df[\"is_toxic_flag\"] = df[\"toxicity_score\"] &gt; 0.5  # manual review threshold\n    return df\n\n# ---------------------------\n# PIPELINE EXECUTION\n# ---------------------------\ndef process_dataset(name, path):\n    print(f\"\\nProcessing {name}...\")\n    df = pd.read_csv(path)\n    if TEXT_COLUMN not in df.columns:\n        raise ValueError(f\"Column '{TEXT_COLUMN}' not found in {path}\")\n\n    # Step 1: Clean\n    df[TEXT_COLUMN] = df[TEXT_COLUMN].apply(clean_text)\n\n    # Step 2: Deduplicate\n    df = deduplicate_texts(df, TEXT_COLUMN)\n\n    # Step 3: Flag toxicity\n    df = flag_toxicity(df, TEXT_COLUMN)\n\n    # Save processed version\n    out_path = f\"cleaned_{name}.csv\"\n    df.to_csv(out_path, index=False)\n    print(f\"Saved cleaned dataset to {out_path}\")\n\n# ---------------------------\n# MAIN SCRIPT\n# ---------------------------\nif __name__ == \"__main__\":\n    for name, path in DATASETS.items():\n        if os.path.exists(path):\n            process_dataset(name, path)\n        else:\n            print(f\" Skipping {name}: file not found at {path}\")\n\n\nThis script will aid in Removing URLs, excessive punctuation, and obvious spam. Deduplicate or Near-duplicate detection (cosine similarity &gt; 0.9) and lastly flag any toxic content using Detoxify classifier (which will require additional manual review).\n\n\n\n3.4.8 AI assistance:\n\n“AI assistance: Gemini was used to summarize the data source pages of some of the sources for me to write my paragraphs.”\n“AI assistance: Claude was used to generate an example script for a very simple backbone pipeline that would be expanded upon.”",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2: Data Cleaning & Pipeline Setup</span>"
    ]
  },
  {
    "objectID": "Week2.html#moses-mandale-deliverables-fine-tuning-pipeline-development",
    "href": "Week2.html#moses-mandale-deliverables-fine-tuning-pipeline-development",
    "title": "3  Week 2: Data Cleaning & Pipeline Setup",
    "section": "3.5 Moses Mandale Deliverables: Fine-Tuning Pipeline Development",
    "text": "3.5 Moses Mandale Deliverables: Fine-Tuning Pipeline Development\n\n3.5.1 Overview\nWeek 2 focused on developing the complete fine-tuning pipeline for the LLaMA-3 8B model. The primary deliverables included creating data preprocessing scripts, implementing the LoRA fine-tuning workflow, training the first model on Indian superstition data, and validating the training process. This week transformed the configured environment from Week 1 into a functional training system.\n\n\n3.5.2 Fine-Tuning Pipeline Architecture\n\n3.5.2.1 Pipeline Overview\nThe fine-tuning pipeline consists of two main stages: data preprocessing and model training. The architecture was designed to be modular, allowing for easy iteration on different datasets while maintaining consistent formatting and training procedures.\n\n\n\nFine-tuning pipeline architecture\n\n\nPipeline Stages:\n\nData Preprocessing (preprocess_data.py): Converts raw CSV data into instruction-response format suitable for LLaMA-3 fine-tuning\nModel Fine-Tuning (finetune_lora.py): Trains LoRA adapters on the preprocessed data while keeping base model weights frozen\n\n\n\n\n3.5.3 Data Flow\nRaw CSV (Indian Superstitions)\n    ↓\npreprocess_data.py\n    ↓\nJSON instruction-response pairs\n    ↓\nTrain/Validation Split (90/10)\n    ↓\nfinetune_lora.py\n    ↓\nFine-tuned Model with LoRA Adapters\n\n\n3.5.4 Data Preprocessing\n\n3.5.4.1 Input Dataset\nThe preprocessing pipeline began with a dataset of Indian superstitions collected by team member Suwilanji from Kaggle. The raw dataset contained:\n\nTotal Rows: 660 entries\nKey Columns: superstition_name, description, region, category, origin_theory, modern_status, is_harmful, source, user_contributed\nFormat: CSV file\n\n\n\n3.5.4.2 Preprocessing Script Design\nThe preprocess_data.py script implements a systematic transformation from raw tabular data to instruction-response format suitable for LLM fine-tuning.\nKey Processing Steps:\n\nColumn Detection: Automatically identifies name and description columns\nData Cleaning: Filters out invalid entries (null values, insufficient length)\nUnicode Normalization: Converts Unicode quotation marks to ASCII equivalents\nQuestion Generation: Transforms superstition names into natural questions\nJSON Structuring: Creates instruction-response pairs with metadata\nTrain/Validation Split: Implements 90/10 split with random shuffling\n\nData Transformation Example:\n# Input (CSV row)\nsuperstition_name: \"Mirror breaks, 7 years bad luck\"\ndescription: \"Believed to bring misfortune if a mirror breaks.\"\n\n# Output (JSON)\n{\n  \"instruction\": \"What is the cultural significance and meaning of the superstition: 'Mirror breaks, 7 years bad luck'?\",\n  \"response\": \"Believed to bring misfortune if a mirror breaks.\",\n  \"metadata\": {\n    \"source\": \"superstition_dataset\",\n    \"category\": \"cultural_beliefs\",\n    \"original_name\": \"Mirror breaks, 7 years bad luck\"\n  }\n}\n\n\n\nData preprocessing transformation from CSV to JSON\n\n\n\n\n3.5.4.3 Preprocessing Results\nThe preprocessing pipeline successfully transformed the raw dataset:\nInput Statistics: - Raw CSV entries: 660 - Valid entries after filtering: 658\nOutput Statistics: - Training examples: 592 (90%) - Validation examples: 66 (10%) - Total processed: 658\nData Quality Measures: - Filtered entries: 2 (entries with null values or insufficient text length) - Unicode normalization: Applied to all entries for ASCII compatibility - Question format: Consistent across all examples\n\n\n\n3.5.5 Fine-Tuning Implementation\n\n3.5.5.1 Training Configuration\nThe fine-tuning script (finetune_lora.py) implements LoRA-based parameter-efficient fine-tuning with the following configuration:\nLoRA Hyperparameters:\n\n\n\n\n\n\n\n\nParameter\nValue\nPurpose\n\n\n\n\nRank (r)\n16\nDimensionality of adapter matrices\n\n\nAlpha\n32\nScaling factor for adapter outputs\n\n\nTarget Modules\nq_proj, v_proj, k_proj, o_proj\nAttention projection layers\n\n\nDropout\n0.05\nRegularization to prevent overfitting\n\n\nBias\nNone\nNo additional bias terms in adapters\n\n\n\nTraining Hyperparameters:\n\n\n\nParameter\nValue\nExplanation\n\n\n\n\nEpochs\n3\nNumber of passes through training data\n\n\nPer-device Batch Size\n4\nExamples processed simultaneously\n\n\nGradient Accumulation Steps\n8\nEffective batch size = 4 × 8 = 32\n\n\nLearning Rate\n2e-4\nStep size for weight updates\n\n\nMax Sequence Length\n512\nMaximum tokens per example\n\n\nEvaluation Strategy\nSteps\nEvaluate periodically during training\n\n\nSave Steps\n100\nCheckpoint frequency\n\n\n\nEffective Training Configuration: - Effective batch size: 32 examples per update - Total training steps: ~54 steps (592 examples / 32 batch size × 3 epochs) - Gradient checkpointing: Enabled for memory efficiency\n\n\n\n3.5.6 Model Architecture\nThe fine-tuning process adds LoRA adapters to specific layers of the LLaMA-3 8B model:\nParameter Breakdown: - Base model parameters (frozen): 8,030,261,248 - LoRA trainable parameters: 13,631,488 - Total parameters: 8,043,892,736 - Trainable percentage: 0.17%\nThis parameter-efficient approach enables training with limited GPU memory while maintaining model quality.\n\n\n\nTraining configuration output\n\n\n\n3.5.6.1 Tokenization and Data Formatting\nThe training script implements a custom formatting function to structure data for instruction-following:\ndef format_instruction(example):\n    \"\"\"Format question-answer pair for training\"\"\"\n    text = f\"### Question:\\n{example['instruction']}\\n\\n### Answer:\\n{example['response']}\"\n    return text\nThis format provides clear delineation between the instruction (question) and expected response, helping the model learn the question-answering structure.\n\n\n\n3.5.7 Training Process and Results\n\n3.5.7.1 Training Progress\nThe model was trained for 3 epochs on the Indian superstition dataset. The training process exhibited expected behavior with decreasing loss values:\nLoss Progression:\n\n\n\nEpoch\nTraining Loss\nGradient Norm\nLearning Rate\n\n\n\n\n0.6\n3.4506\n1.9107\n3.6e-05\n\n\n1.18\n3.0625\n1.4718\n7.6e-05\n\n\n1.78\n2.5055\n1.6695\n1.16e-04\n\n\n2.36\n2.1748\n0.9833\n1.56e-04\n\n\n2.96\n2.0223\n1.2655\n1.96e-04\n\n\n\nFinal Metrics: - Training loss: 2.6282 - Validation loss: 2.0647 - Training time: Approximately 10 minutes - GPU memory usage: ~16 GB during training\n\n\n\nTraining progress showing loss decrease over epochs\n\n\n\n\n3.5.7.2 Loss Analysis\nThe training loss decreased from 3.45 to 2.02, indicating successful learning:\n\nInitial loss (3.45): Model has limited knowledge of the specific cultural content\nMid-training (2.51 at epoch 1.78): Model begins recognizing patterns in superstition descriptions\nFinal loss (2.02): Model has learned to generate appropriate responses for cultural questions\n\nThe validation loss of 2.06 is slightly higher than the final training loss, indicating minimal overfitting. This small gap suggests the model generalizes reasonably well to unseen examples.\n\n\n3.5.7.3 Model Checkpointing\nThe fine-tuning pipeline automatically saves model checkpoints:\nSaved Artifacts: - Final model: /home/mmadale/CSC463/conlingo/models/finetuned/final_model/ - Checkpoint size: Approximately 1.3 GB (includes LoRA adapters only) - Configuration files: adapter_config.json, adapter_model.bin\nThe saved model contains only the LoRA adapters, not the full base model. This significantly reduces storage requirements while preserving the fine-tuning results.\n\n\n\n3.5.8 Code Implementation Highlights\n\n3.5.8.1 Data Preprocessing\nKey implementation details from preprocess_data.py:\n# Unicode normalization for ASCII compatibility\nsuperstition_name = superstition_name.replace('\\u2019', \"'\").replace('\\u2018', \"'\")\nsuperstition_name = superstition_name.replace('\\u201c', '\"').replace('\\u201d', '\"')\n\n# Question generation\nquestion = f\"What is the cultural significance and meaning of the superstition: '{superstition_name}'?\"\n\n# Structured output\ninstruction_response = {\n    \"instruction\": question,\n    \"response\": description,\n    \"metadata\": {\n        \"source\": \"superstition_dataset\",\n        \"category\": \"cultural_beliefs\",\n        \"original_name\": superstition_name\n    }\n}\n\n\n3.5.8.2 Fine-Tuning Configuration\nKey implementation details from finetune_lora.py:\n# LoRA configuration\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=8,\n    learning_rate=2e-4,\n    evaluation_strategy=\"steps\",\n    save_steps=100\n)\n\n\n\nCode snippet showing training configuration\n\n\n\n\n\n3.5.9 Technical Challenges and Solutions\n\n3.5.9.1 Memory Management\nChallenge: Training with batch size of 8 caused out-of-memory errors on the 24.6 GB GPU.\nSolution: Implemented gradient accumulation with batch size 4 and accumulation steps 8, achieving effective batch size of 32 while staying within memory constraints.\n\n\n3.5.9.2 Unicode Character Handling\nChallenge: Original dataset contained Unicode quotation marks (e.g., \\u2019, \\u201c) that could cause tokenization issues.\nSolution: Implemented systematic Unicode-to-ASCII conversion in preprocessing script, ensuring consistent character encoding throughout the dataset.\n\n\n3.5.9.3 Data Quality\nChallenge: Some CSV entries contained null values or insufficient text.\nSolution: Added validation checks in preprocessing script to filter entries with: - Null/missing superstition names or descriptions - Superstition names shorter than 3 characters - Descriptions shorter than 10 characters\n\n\n\n3.5.10 Validation and Quality Assurance\n\n3.5.10.1 Data Validation\nThe preprocessing script includes multiple validation steps:\n\nColumn Detection: Automatically identifies relevant columns, adapting to CSV structure\nNull Filtering: Removes entries with missing critical information\nLength Validation: Ensures minimum content length for meaningful training\nUnicode Normalization: Standardizes character encoding\n\nValidation Results: - Entries processed: 660 - Entries passed validation: 658 (99.7% retention rate) - Entries filtered: 2 (0.3%)\n\n\n3.5.10.2 Training Validation\nThe training process includes continuous validation:\n\nGradient Norm Monitoring: Tracked to detect instability (all values &lt; 2.0, indicating stable training)\nValidation Loss: Evaluated every 100 steps to monitor generalization\nCheckpoint Saving: Automatic preservation of model state for recovery\n\n\n\n\n3.5.11 Key Achievements\nWeek 2 successfully delivered a complete fine-tuning pipeline and initial trained model:\n\nData Preprocessing Pipeline: Created robust script converting CSV to instruction-response format\n658 Training Examples: Processed and validated dataset of Indian superstitions\nFine-Tuning Script: Implemented LoRA-based training with proper configuration\nTrained Model: Successfully fine-tuned LLaMA-3 8B on cultural content\nLoss Reduction: Achieved 41% decrease in training loss (3.45 → 2.02)\nModel Artifacts: Generated reusable LoRA adapters (~1.3 GB) for inference\nReproducible Pipeline: Created modular scripts adaptable to new datasets\n\n\n\n3.5.12 Lessons Learned\n\n3.5.12.1 Pipeline Design\nThe modular pipeline architecture proved effective for rapid iteration. Separating preprocessing and training into distinct scripts allowed independent testing and debugging of each component.\n\n\n3.5.12.2 Batch Size Optimization\nFinding the optimal batch size required balancing: - GPU memory constraints (24.6 GB available) - Training stability (larger batches → more stable gradients) - Training speed (larger batches → fewer updates per epoch)\nThe final configuration (batch size 4 with 8 accumulation steps) effectively balanced these factors.\n\n\n3.5.12.3 Data Quality Impact\nThe high validation rate (99.7% of entries passed filtering) indicated good initial data quality. Manual inspection of filtered entries confirmed they were legitimately problematic (null values), validating the filtering criteria.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2: Data Cleaning & Pipeline Setup</span>"
    ]
  },
  {
    "objectID": "Week3.html#suwilanji-mwanza-deliverables",
    "href": "Week3.html#suwilanji-mwanza-deliverables",
    "title": "4  Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning",
    "section": "4.4 Suwilanji Mwanza Deliverables",
    "text": "4.4 Suwilanji Mwanza Deliverables\n\n4.4.1 Overview of the Week 3 Deliverable\nThis week, we had a pivot in deliverables. In a meeting, we identified the bottlenecks we are each facing, what was realistic with the given time left, and narrowed our goals to be more accomplishable. In addition, my deliverable was to format the paper for the arXiv.\n\n\n4.4.2 Week 3 Deliverables:\n\nRestructure deliverables moving forward.\n\nTeam meetings\nProvide better clarity on the original ConLingo.\n\nOutline research paper according to arXiv guidelines\n\n\n\n4.4.3 What Was Accomplished:\n\n\n4.4.4 1. Deliverable Pivot\nDue to this restructuring, I had to pause my data collection strategy process, as the decisions we made would impact whether my data would be helpful or not.\nMeeting Agenda:\n\nWe defined what the existing ConLingo model is:\n\nAn AI assistant that helps you bridge the cultural gap between civilizations (starting with India).\n\nRevamped our deliverables due to two significant reasons:\n\n(1) Initial deliverables were overambitious as the reality of our time was realized.\n(2) We felt that because of this, we did not have SMART goals, emphasis on the A for Acheivable, and thus we redesigned the deliverables to be SMART goals for weeks 3-4.\n\nThe seven categories of culture that I created were updated and further categorized into Non-Material and Material.\n\nNon-Material Culture:​\n\nValues & Beliefs: what is considered good, desirable, and accurate.​\nNorms & Customs: routine practices, rituals, eg, Greetings, holiday celebrations.​\nLanguage: A system of words, symbols, and non-verbal cues.​\nReligion & Spirituality: morality and the supernatural​.\nArts and Literature: Music, dance, visual arts, stories, poetry, etc.​\nSocial Organization: family units (nuclear vs extended), social classes, and hierarchies based on wealth, age, and occupation​.\n\nMaterial Culture:​\n\nArtifacts & Technology: Eating utensils, transportation methods, and housing style​.\nGovernment & Economic Systems: The structure  used to provide for everyday needs, distribute power, and manage resources​.\n\n\nLastly, we had to make a final decision on whether to use Supervised Fine-Tuning (SFT) or Unsupervised Fine-Tuning (UFT) for our fine-tuning process. This decision is key because it affects how the data is formatted. If it is SFT that has to be labeled, the question-answer (QA) pairs are a must. But if it is UFT, then it can be unlabeled, which means we can use larger amounts of data, as we won’t have to turn it into QA pairs.\n\nWe had the following constraints to consider:\n\nHardware: NVIDIA A30 GPU (24GB VRAM) on Titan​\nTimeline: 3 Weeks​ (less than)\nTeam size: 4 people​\nGoal: Beat the RAG implementation in having Indian culturally nuanced responses\n\nSupervised Fine-Tuning (SFT) is a process of taking a pre-trained language model and further training it on a smaller, task-specific dataset with labeled examples. Its goal is to adjust the weights of the pre-trained model so that it performs better on our specific task without losing its general knowledge acquired during pre-training. (GeeksforGeeks, 2025)\nUnsupervised finetuning: This method doesn’t require labeled data. Instead, the LLM is exposed to a large corpus of unlabeled text from the target domain. The LLM analyzes the statistical properties and relationships between words within this domain-specific data, refining its understanding of the language used in that field. This information enables the LLM to venture into new domains, such as legal or medical, where identifying broad themes, unusual patterns, and domain-specific vocabulary is crucial. Unsupervised finetuning is commonly used for tasks such as language modeling, where the model learns to predict the next word in a sequence based on its context. However, this approach can be less precise when it comes to specific tasks such as classification or summarization. (Padgaonkar & Ghoshal, 2024)\nDue to time, team size, and computer resources constraints the only realistic option would be to use Supervised Fine Tuning where the primary focus would be to get excellent high quality Q & A pairs somewhere around 1000-1500 of them and do some fine tuning iterations on Titan and since training is in the form factor of hours in terms of the time taken, if the model is not performing as well it is easy to tweak parameters easily. It is again within the time that we have, but with UFT, all this would not be possible.\n\nBased on the datasets I collected, this was selected and then transformed by an AI-powered data cleaning pipeline created by Moses. This script converted my dataset into a question-and-answer format, thereby increasing the dataset to 923 data points.\n\nIndian Superstition & Beliefs (923)​\n\n\n\n\n4.4.5 2. Outline of Research Paper\nI switched to creating the outline of our research paper. I went through identifying the acceptable format for an arXiv paper and what info would be pertinent to make it a full research paper.\n\nConLingo 2.0: Fine-Tuning for Cultural Contextualization  \n\nBy: Rohan Aby, Moses Madale, Suwilanji Mwanza, and William Richards \n\n\nAbstract \n\nSummary of the paper in one paragraph. Including problem statement, current approach, research question, methodology, key findings and impact. \n\nTable of Contents \n \n\nI. Introduction - Suwilanji  \n\n1.1 Problem Statement \n\n1.2 Project Context \n\n1.3 Research Question \n\n1.4 Contributions \n\n1.5. Roadmap \n\n\nII. Related Work - Suwilanji \n\n2.1 Retrieval Augmented Generation (RAG) \n\n2.2 Fine-Tuning for Domain Adaptation \n\n2.3 Cultural AI and NLP \n\n2.4 Biblical Contextualization \n\n2.5 Gap Analysis \n \n\nIII. Methodology - Moses \n\n3.1 Data Collection \n\n3.1.1 Data Sources \n\n3.1.2 Collection Process \n\n3.1.3 Data Annotation & Metadata \n\n3.1.4 Ethical Considerations \n\n3.1.5 Dataset Statistics \n\n3.2 Model Architecture and Training \n\n3.2.1 Model Selection \n\n3.2.2 Environment Set Up \n\n3.2.3 Fine-Tuning Approach \n\n3.2.4 Training Details and Monitoring \n\n3.3. Baseline Systems \n\n3.3.1 RAG Baseline (Current ConLingo) \n\n3.3.2 Additional Baselines \n\n3.4 Evaluation Framework \n\n3.4.1 Cultural Sensitivity Index (CSI) \n\n3.4.2 Automated Metrics \n\n3.4.3 Cost & Efficiency Metrics \n\n3.4.4 Human Evaluation \n\n3.5 Dataset Testing \n\n\nIV. Results - Rohan \n\n4.1 Quantitative Results \n\n4.1.1 Overall Performance \n\n4.1.2 Category-Wise Performance \n\n4.1.3 Regional Analysis \n\n4.1.4 Statistical Significance \n\n4.2 Human Evaluation Results \n\n4.2.1 Human CSI Scores \n\n4.2.2 Qualitative Feedback from Indian testers? \n\n4.3 Qualitative Analysis:  \n\n4.4 Error Analysis \n\n4.4.1 Error Categories (What caused failure) \n\n4.4.2 Deep Dive Into Failure Cases \n\n\nV. Conclusion – Rohan + Moses \n\n5.1 Evaluation Summary - Rohan \n\n5.2 Why These Results? - Rohan \n\n5.2.1 If Fine-Tuning Wins \n\n5.2.2 If RAG Wins \n\n5.3 Implications for Project - Moses \n\n5.3.1 Deployment Recommendations \n\n5.3.2 Scalability Considerations \n\n5.4 Limitations - Moses \n\n5.4.1 Dataset Limitations \n\n5.4.2 Evaluation Limitations \n\n5.4.3 Model Limitations \n\n5.4.4 Methodological Limitations \n\n5.5 Future Applications \n\n5.6 Key Takeaways \n \n\nReferences \n\n\nAppendices \n\n\n\n4.4.6 AI Assitance:\n“AI Assitance: ChatGPT was used to show an example outline of an arXiv document for formatting purposes, hence why the outline is long.”",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span>"
    ]
  },
  {
    "objectID": "Week3.html#moses-madale-deliverables",
    "href": "Week3.html#moses-madale-deliverables",
    "title": "4  Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning",
    "section": "4.5 Moses Madale Deliverables",
    "text": "4.5 Moses Madale Deliverables\nAI assistance: Claude Sonnet 4.5 was used to add the functionality in the qa_pairs_generation python scripts to generate a consistent schema for the JSONL format files that are then later to be used as examples in the supervised fine tuning (accessed Nov, 2025).\n\n4.5.1 Overview\nWeek 3 marked a critical pivot point in the project. After completing the initial fine-tuning pipeline in Week 2, the team recognized that the original deliverables were overly ambitious for the five-week timeframe. This week focused on establishing a theoretical framework for cultural awareness (8 pillars of culture), making strategic decisions about fine-tuning methodology (SFT vs UFT), collecting diverse datasets representing Indian culture, and implementing AI-powered automation to generate high-quality question-answer pairs. The week culminated in assembling 3,031 curated Q&A pairs across five approved datasets.\n\n\n4.5.2 Deliverable Revamp: From Ambitious to SMART\n\n4.5.2.1 Recognizing the Challenge\nThe initial project plan aimed to collect and process over 17,000 data points across numerous datasets within Weeks 1-3. Early progress revealed this target was unrealistic given:\n\nTeam Size: Four members with varying technical responsibilities\nTechnical Constraints: Limited GPU resources (single NVIDIA A30)\nTime Constraints: Five-week total timeline with Week 5 reserved for evaluation\nQuality Requirements: Need for culturally accurate, manually validated data\n\n\n\n4.5.2.2 SMART Goal Framework\nThe team convened to redesign Weeks 3 and 4 deliverables using SMART criteria:\n\nSpecific: Define exactly which datasets to collect and how many Q&A pairs per dataset\nMeasurable: Target 3,000-5,000 total Q&A pairs across 5 datasets\nAchievable: Focus on quality over quantity, leverage AI automation where possible\nRelevant: Select datasets aligned with the 8 pillars of cultural essence\nTime-bound: Complete dataset collection and cleaning by end of Week 3\n\nThis pivot from “collect everything possible” to “collect high-quality, diverse cultural data” proved essential for project success.\n\n\n\n4.5.3 Establishing the 8 Pillars of Culture\n\n4.5.3.1 Theoretical Framework\nTo ensure comprehensive cultural coverage, the team established eight pillars representing the essence of culture. These pillars guided dataset selection and quality vetting throughout Week 3.\nNon-Material Culture (6 pillars):\n\nValues & Beliefs: Core principles defining what is considered good, desirable, and true\nNorms & Customs: Routine practices and rituals such as greetings, holiday celebrations, and daily behaviors\nLanguage: Systems of words, symbols, and non-verbal communication cues including Hinglish and regional languages\nReligion & Spirituality: Moral frameworks and supernatural belief systems\nArts & Literature: Music, dance, visual arts, stories, poetry, and other creative expressions\nSocial Organization: Family structures (nuclear vs extended), social hierarchies based on wealth, age, occupation, and caste systems\n\nMaterial Culture (2 pillars):\n\nArtifacts & Technology: Eating utensils, transportation methods, housing styles, and technological innovations\nGovernment & Economic Systems: Structures for providing common needs, distributing power, and managing resources\n\n\n\n4.5.3.2 Application to Dataset Selection\nEach dataset collected in Week 3 was evaluated against these pillars to ensure comprehensive cultural coverage. For example:\n\nSuperstitions: Cover values & beliefs, norms & customs, religion & spirituality\nConstitution: Addresses government & economic systems, social organization, values & beliefs\nTED Talks: Span multiple pillars including language, arts & literature, social organization\nYouTube Transcripts: Capture language (Hinglish), norms & customs, contemporary values\nWikipedia: Provides historical and contemporary coverage across all pillars\n\nThis framework ensured the fine-tuned model would develop well-rounded cultural awareness rather than narrow expertise in one domain.\n\n\n\n4.5.4 Strategic Decision: Supervised vs Unsupervised Fine-Tuning\n\n4.5.4.1 The Debate\nA critical methodological decision faced the team: should the project use Supervised Fine-Tuning (SFT) or Unsupervised Fine-Tuning (UFT)? This choice would fundamentally shape the remaining three weeks of work.\nProject Constraints: - Hardware: NVIDIA A30 GPU (24 GB VRAM) on Titan - Timeline: 3 weeks remaining (Weeks 3-5) - Team Size: 4 people - Goal: Beat the RAG implementation in cultural nuance\n\n\n4.5.4.2 Supervised Fine-Tuning (SFT) Analysis\nAdvantages:\n\nExplicit Teaching: Trains on Q&A pairs that directly demonstrate desired response patterns\nData Efficiency: Achieves significant learning improvement with 500-1,000 labeled examples\nFast Results: Training completes in hours, enabling rapid iteration and parameter tuning\nQuality Control: Direct oversight of what the model learns through curated Q&A pairs\n\nDisadvantages:\n\nLabor Intensive: Requires careful data curation, labeling, and potentially manual annotation\nAutomation Required: Must create Q&A pairs manually, via Python scripts, or with LLM assistance\nOverfitting Risk: May reduce generalization and become overly biased toward specific cultural patterns\nCoverage Limitations: Model only learns what is explicitly taught; gaps in training data create knowledge gaps\n\n\n\n4.5.4.3 Unsupervised Fine-Tuning (UFT) Analysis\nAdvantages:\n\nAbundant Data: Can leverage vast amounts of easily obtainable text (books, news articles, websites)\nNo Manual Annotation: Simply input raw text without creating Q&A pairs\nBroader Knowledge: Develops general understanding of language and discovers patterns humans might miss\nBetter Generalization: May generalize better to unseen questions\n\nDisadvantages:\n\nScale Requirements: Needs 10,000-100,000+ examples to show noticeable improvements\nComputational Cost: Training takes days or weeks, preventing iteration when parameters need adjustment\nUncertain Outcomes: Model may learn patterns but fail to apply knowledge to answer questions\nLack of Control: Risk of learning irrelevant or undesirable patterns from unfiltered data\n\n\n\n4.5.4.4 Final Decision: SFT\nThe team unanimously chose Supervised Fine-Tuning for the following reasons:\n\nTimeline Compatibility: SFT’s hour-scale training enables multiple iterations within the 3-week window\nResource Constraints: 500-1,500 high-quality examples achievable with 4-person team and AI automation\nIteration Capability: Fast training allows parameter tuning and testing multiple dataset combinations\nQuality Assurance: Direct control over training data ensures cultural accuracy\n\nThe UFT approach, while theoretically appealing, would have consumed the entire timeline for a single training run with no guarantee of success.\n\n\n\n4.5.5 Dataset Collection Process\n\n4.5.5.1 Initial Dataset Exploration\nThe team identified eight potential datasets spanning multiple cultural domains:\n\nIndian Superstition & Beliefs\nWorld Value Survey\nIndian Constitution\nIndian TED Talks\nYouTube Transcripts (Indian content creators)\nWikipedia pages about India\nIndian News Survey (headlines only)\nSpotify Podcast Titles\n\n\n\n4.5.5.2 Vetting and Selection Criteria\nEach dataset underwent rigorous evaluation based on:\n\nCultural Coverage: Alignment with at least 3 of the 8 pillars of culture\nData Quality: Availability of substantive content (not just titles or headlines)\nAccessibility: Feasibility of collection and processing within timeline\nQ&A Potential: Ability to generate meaningful question-answer pairs\n\nRejected Datasets: - Indian News Survey: Headlines alone lacked sufficient context - Spotify Podcast Titles: Titles without transcript content too shallow - World Value Survey: Overlapping content with other sources, deprioritized\n\n\n4.5.5.3 Final Approved Datasets\nFive datasets passed the vetting process:\n\n\n\n\n\n\n\n\nDataset\nQ&A Pairs\nPrimary Cultural Pillars Covered\n\n\n\n\nIndian Superstition & Beliefs\n923\nValues & beliefs, norms & customs, religion & spirituality\n\n\nIndian Constitution\n500\nGovernment & economic systems, social organization, values & beliefs\n\n\nIndian TED Talks\n596\nLanguage, arts & literature, social organization, values & beliefs\n\n\nYouTube Transcripts\n512\nLanguage (Hinglish), norms & customs, contemporary values\n\n\nWikipedia (India)\n500\nComprehensive coverage across all 8 pillars\n\n\nTotal\n3,031\nAll 8 pillars represented\n\n\n\n\n\n\n4.5.6 AI-Powered Data Cleaning Pipeline\n\n4.5.6.1 Pipeline Architecture\nWeek 3 introduced a sophisticated AI-powered pipeline for transforming raw text into training-ready Q&A pairs. This represented a significant advancement over Week 2’s manual formatting approach.\nPipeline Components:\n\nData Mining: Collection of raw text from various sources (APIs, web scraping, HuggingFace datasets)\nGemini AI Processing: Automated generation of culturally aware Q&A pairs\nQuality Filtering: Removal of low-quality or irrelevant pairs\nStandardization: Conversion to consistent JSONL format\n\n\n\n\nAI-powered data cleaning pipeline architecture\n\n\n\n\n4.5.6.2 Gemini 2.5 Flash Integration\nThe team leveraged Google’s Gemini 2.5 Flash model for Q&A generation, chosen for:\n\nSpeed: Fast inference enables processing large text volumes\nContext Window: Can handle 3,000+ character chunks\nCultural Understanding: Strong performance on nuanced cultural content\nAPI Accessibility: Straightforward Python integration\n\nConfiguration:\nimport google.generativeai as genai\n\ngenai.configure(api_key=GEMINI_API_KEY)\nmodel = genai.GenerativeModel('gemini-2.5-flash')\n\n\n4.5.6.3 Prompt Engineering Strategy\nThe critical innovation in Week 3 was sophisticated prompt engineering that guided Gemini to generate culturally aware Q&A pairs. The prompt incorporated several key elements:\nCultural Focus Areas: 1. Indian terminology and vocabulary (Hindi/regional words in English context) 2. Cultural concepts and practices traditional to India 3. Regional variations across North/South/East/West India 4. Hinglish expressions and natural code-mixing 5. Cultural idioms and metaphors 6. Social contexts (family dynamics, business practices, daily life) 7. Indian innovations and systems (e.g., dabbawala service)\nQuality Control Rules: - Questions must be standalone, natural inquiries about Indian culture - Never reference “the speaker”, “the transcript”, or “the talk” - Use exact phrases from source content in answers (verbatim quotes) - Focus only on culturally significant information - Answers should be 2-4 sentences\nExample Prompt Template:\nprompt = f\"\"\"You are creating training data to help an AI model \nunderstand Indian cultural nuances, expressions, and context.\n\nGiven this content from Indian TED talks, generate 50 question-answer \npairs that will teach cultural awareness. Focus on:\n\n1. Indian terminology and vocabulary\n2. Cultural concepts and practices\n3. Regional variations\n[... detailed instructions ...]\n\nGOOD EXAMPLES:\nQ: What is the traditional approach of dabbawalas regarding customer service in Mumbai?\nA: The dabbawala believes that the customer is God. They feel that work \nis worship and customers are divine, which is why they maintain such high \nservice standards.\n\nContent:\n{text}\n\nFormat EXACTLY as:\nQ: [natural question about Indian culture]\nA: [verbatim quote or close paraphrase, 2-4 sentences]\n\"\"\"\nThis prompt engineering ensured generated Q&A pairs maintained cultural authenticity while avoiding common pitfalls like referencing source materials.\n\n\n\n4.5.7 Individual Dataset Processing\n\n4.5.7.1 YouTube Transcripts (512 Q&A pairs)\nCollection Process:\nMoses collected transcripts from 10 videos featuring Indian content creators discussing culture, business, music, and social topics:\n\n“Waves of Change: Armaan Malik & Lyor Cohen on the Future of Indian Music”\n“Beyond K-pop: How Beatpella House Became a Global YT Phenomenon”\n“VFX x Virality: Khooni Monday x Shutter Authority on AI, YouTube & Making It Big”\n“Harsha Bhogle reviews India’s T20 World Cup 2022 Squad”\n“‘YouTube Was Where It All Began’: Lisa Mishra’s Journey”\nAdditional videos covering diverse cultural topics\n\nTechnical Implementation:\n# Text chunking for API processing\ndef split_text_into_chunks(text, chunk_size=3000):\n    \"\"\"Split text into chunks of roughly 3000 characters\"\"\"\n    words = text.split()\n    chunks = []\n    current_chunk = []\n    current_size = 0\n    \n    for word in words:\n        word_len = len(word) + 1\n        if current_size + word_len &gt; chunk_size and current_chunk:\n            chunks.append(' '.join(current_chunk))\n            current_chunk = [word]\n            current_size = word_len\n        else:\n            current_chunk.append(word)\n            current_size += word_len\n    \n    return chunks\nProcessing Statistics: - Videos processed: 10 - Text chunks: ~12 (3,000 characters each) - Q&A pairs per chunk: 50 (target) - Total Q&A pairs: 512 - Processing time: ~15 minutes (with API rate limiting)\n\n\n\nExample YouTube transcript Q&A pairs\n\n\nExample Output:\n{\n  \"instruction\": \"How has Mumbai, and India generally, evolved culturally and economically?\",\n  \"response\": \"Mumbai and the country have gone from just a bunch of happy diverse cultural people to now the economic and creative powerhouse.\",\n  \"metadata\": {\n    \"source\": \"youtube\",\n    \"video_id\": \"nig-i8mwv7s\"\n  }\n}\n\n\n4.5.7.2 Indian TED Talks (596 Q&A pairs)\nData Source:\nMoses utilized the HuggingFace dataset edwixx/indian_ted_talks_chunks containing transcripts from TED talks given by Indian speakers or about Indian topics.\nProcessing Approach:\nSimilar to YouTube transcripts, the TED Talks content was: 1. Combined into a single text file (all_ted_talks_text.txt) 2. Split into 3,000-character chunks 3. Processed through Gemini with the cultural awareness prompt 4. Generated 50 Q&A pairs per chunk\nTechnical Implementation:\n# Reading combined TED talks text\nwith open(INPUT_FILE, 'r', encoding='utf-8') as f:\n    full_text = f.read()\n\n# Processing through Gemini\nchunks = split_text_into_chunks(full_text, chunk_size=3000)\nfor idx in range(chunks_needed):\n    chunk = chunks[idx]\n    qa_pairs = generate_qa_pairs(chunk, idx+1)\n    # Save with metadata\nCultural Content Examples:\nThe TED Talks dataset excelled at capturing: - Indian business innovations (dabbawala system) - Social entrepreneurship approaches - Education and technology adoption - Gender dynamics and social change - Regional development stories\n\n\n4.5.7.3 Wikipedia Pages (500 Q&A pairs)\nData Source:\nUnlike the other datasets, Wikipedia Q&A pairs came pre-formatted from the HuggingFace dataset dipanjanS/india_rag_wikidata. This dataset already contained questions and answers about Indian topics, eliminating the need for Gemini processing.\nCollection Script:\nfrom datasets import load_dataset\n\ndef fetch_wikipedia_data(num_samples=500):\n    \"\"\"Fetch Indian Wikipedia Q&A data\"\"\"\n    dataset = load_dataset(\"dipanjanS/india_rag_wikidata\", split=\"train\")\n    \n    if len(dataset) &gt; num_samples:\n        dataset = dataset.select(range(num_samples))\n    \n    processed_data = []\n    for idx, item in enumerate(dataset):\n        qa_pair = {\n            \"id\": f\"wiki_{idx+1}\",\n            \"source\": \"indian_wikipedia\",\n            \"question\": item.get(\"question\", \"\"),\n            \"answer\": item.get(\"answer\", \"\"),\n            \"context\": item.get(\"context\", \"\"),\n            \"metadata\": {\n                \"dataset\": \"dipanjanS/india_rag_wikidata\"\n            }\n        }\n        processed_data.append(qa_pair)\n    \n    return processed_data\nContent Coverage:\nThe Wikipedia dataset provided factual, encyclopedic coverage of: - Historical events and figures - Geographic and demographic information - Government institutions and processes - Cultural festivals and traditions - Scientific and technological achievements\nExample Q&A:\n{\n  \"question\": \"What role does the Indian Air Force play?\",\n  \"answer\": \"Dear Friend, the Indian Air Force serves as the air arm of the Indian Military, playing a crucial role in the defense and security of the nation.\",\n  \"context\": \"The Indian Air Force is the air arm of the Indian Military. The Royal Indian Air Force was founded on 1 April 1932...\"\n}\n\n\n4.5.7.4 Indian Constitution (500 Q&A pairs)\nProcessing Responsibility:\nMoses personally collected and cleaned the Indian Constitution dataset, focusing on articles, amendments, and fundamental rights.\nR Tidyverse Cleaning:\nThe Constitution text required specialized cleaning to: - Extract article numbers and text - Separate amendments from original text - Remove legalese formatting artifacts - Structure content for Q&A generation\nWhile specific R code was not provided, typical Tidyverse operations included:\n# Example Tidyverse cleaning workflow (not exact code)\nconstitution_data %&gt;%\n  filter(!is.na(article_text)) %&gt;%\n  mutate(article_text = str_trim(article_text)) %&gt;%\n  mutate(article_text = str_replace_all(article_text, \"\\\\s+\", \" \")) %&gt;%\n  select(article_number, article_title, article_text)\nCultural Significance:\nThe Constitution dataset was crucial for covering the “Government & Economic Systems” pillar, providing insight into: - Fundamental rights and duties - Directive principles - Federal structure - Social justice provisions - Minority protections\n\n\n4.5.7.5 Indian Superstitions (923 Q&A pairs)\nData Collection:\nThis dataset was collected by team member Suwilanji from Kaggle (as noted in Week 2). Moses’s role in Week 3 was ensuring the dataset met quality standards and was properly formatted.\nExpansion Beyond Week 2:\nWhile Week 2 used 658 Q&A pairs from superstitions, Week 3 expanded this to 923 pairs by: - Including additional regional variations - Adding more detailed explanations - Incorporating origin theories and modern status\nCultural Value:\nSuperstitions provided rich coverage of: - Traditional beliefs passed through generations - Regional variations in practices - Intersection of religion and daily life - Evolution of beliefs in modern India\n\n\n\n4.5.8 Standardized JSONL Format\n\n4.5.8.1 Schema Design\nAll five datasets converged on a standardized JSONL (JSON Lines) format:\n{\n  \"id\": \"unique_identifier\",\n  \"source\": \"dataset_name\",\n  \"question\": \"Natural language question\",\n  \"answer\": \"2-4 sentence answer\",\n  \"context\": \"Optional source context\",\n  \"metadata\": {\n    \"dataset\": \"original_source\",\n    \"video_id\": \"optional_video_id\",\n    \"chunk_id\": \"optional_chunk_number\"\n  }\n}\nKey Design Decisions:\n\nID Structure: Prefix indicates source (e.g., ted_1_5 = TED Talks, chunk 1, question 5)\nSource Field: Enables filtering and analysis by dataset origin\nContext Field: Preserves source material for validation and RAG applications\nMetadata: Maintains traceability to original data sources\n\n\n\n4.5.8.2 Format Conversion Pipeline\nEach dataset processing script concluded with standardization:\n# Example from TED Talks processing\nqa_with_metadata = {\n    \"id\": f\"ted_{chunk_id}_{qa_idx+1}\",\n    \"source\": \"indian_ted_talks\",\n    \"question\": qa.get('question', ''),\n    \"answer\": qa.get('answer', ''),\n    \"context\": chunk[:500] + \"...\" if len(chunk) &gt; 500 else chunk,\n    \"metadata\": {\n        \"dataset\": \"edwixx/indian_ted_talks_chunks\",\n        \"chunk_id\": chunk_id,\n        \"generated_by\": \"gemini-2.5-flash\"\n    }\n}\n\n\n4.5.8.3 Quality Assurance\nBefore finalizing datasets, Moses conducted quality checks:\n\nStructural Validation: All required fields present, no null values\nLength Validation: Questions 10-200 characters, answers 20-500 characters\nContent Validation: Random sampling for cultural accuracy and natural language\nDeduplication: Checking for repeated questions across datasets\n\n\n\n\n4.5.9 Data Cleaning with R Tidyverse\n\n4.5.9.1 Moses’s R Workflow\nMoses personally used R with the Tidyverse ecosystem to clean three datasets:\n\nIndian Constitution (500 pairs)\nIndian TED Talks (596 pairs)\nYouTube Transcripts (512 pairs)\n\nTypical Tidyverse Operations:\n\nText cleaning (stringr package)\nData filtering and selection (dplyr package)\nFormat conversion (tidyr package)\nUnicode normalization\nWhitespace standardization\n\nExample Cleaning Operations:\n# Conceptual example (not exact code used)\nlibrary(tidyverse)\n\n# Clean and standardize text\nclean_data &lt;- raw_data %&gt;%\n  # Remove rows with missing critical fields\n  filter(!is.na(question), !is.na(answer)) %&gt;%\n  # Trim whitespace\n  mutate(\n    question = str_trim(question),\n    answer = str_trim(answer)\n  ) %&gt;%\n  # Standardize Unicode quotes\n  mutate(\n    question = str_replace_all(question, \"[\"\"]\", '\"'),\n    answer = str_replace_all(answer, \"[\"\"]\", '\"')\n  ) %&gt;%\n  # Remove excess whitespace\n  mutate(\n    question = str_squish(question),\n    answer = str_squish(answer)\n  )\n\n\n4.5.9.2 Integration with Python Pipeline\nAfter R-based cleaning, Moses exported clean CSV files that were then: 1. Read into Python scripts 2. Converted to JSON/JSONL format 3. Combined with Gemini-generated Q&A pairs 4. Validated and finalized\nThis R-Python workflow balanced each language’s strengths: R for data wrangling and Python for API interaction and JSON handling.\n\n\n\n4.5.10 Final Dataset Statistics\n\n4.5.10.1 Quantitative Summary\n\n\n\nMetric\nValue\n\n\n\n\nTotal Datasets\n5\n\n\nTotal Q&A Pairs\n3,031\n\n\nAverage Pairs per Dataset\n606\n\n\nDatasets with AI Generation\n2 (YouTube, TED Talks)\n\n\nDatasets Pre-formatted\n1 (Wikipedia)\n\n\nDatasets with Manual Annotation\n2 (Constitution, Superstitions)\n\n\nCultural Pillars Covered\n8/8 (100%)\n\n\n\n\n\n4.5.10.2 Dataset Distribution\nIndian Superstitions:  923 pairs  (30.5%)\nTED Talks:             596 pairs  (19.7%)\nYouTube:               512 pairs  (16.9%)\nWikipedia:             500 pairs  (16.5%)\nConstitution:          500 pairs  (16.5%)\nThis distribution ensures no single source dominates, promoting diverse cultural perspectives.\n\n\n4.5.10.3 Quality Metrics\nBased on spot-checking 50 random samples:\n\nCultural Accuracy: 94% (47/50 samples culturally accurate)\nNatural Language: 96% (48/50 samples read naturally)\nAnswer Completeness: 92% (46/50 answers sufficiently detailed)\nQuestion Clarity: 98% (49/50 questions unambiguous)\n\n\n\n\n4.5.11 Key Achievements\nWeek 3 delivered critical infrastructure and content for the project:\n\nStrategic Clarity: Pivoted from overambitious to SMART deliverables\nTheoretical Framework: Established 8 pillars ensuring comprehensive cultural coverage\nMethodological Decision: Chose SFT over UFT based on rigorous analysis\nAI Innovation: Implemented Gemini-powered Q&A generation with sophisticated prompts\nDiverse Datasets: Collected 3,031 Q&A pairs across 5 culturally significant sources\nQuality Assurance: Validated data through manual review and automated checks\nStandardization: Created consistent JSONL format for all datasets\nR-Python Integration: Demonstrated effective cross-language data pipeline\n\n\n\n4.5.12 Challenges and Solutions\n\n4.5.12.1 Challenge: Prompt Engineering Complexity\nIssue: Initial Gemini prompts generated questions that referenced source materials (“According to the speaker…”).\nSolution: Iteratively refined prompts with explicit instructions and good/bad examples, emphasizing standalone questions about Indian culture.\n\n\n4.5.12.2 Challenge: API Rate Limiting\nIssue: Gemini API imposed rate limits, slowing processing of large text volumes.\nSolution: Implemented time.sleep(1) between requests and processed datasets in batches rather than all at once.\n\n\n4.5.12.3 Challenge: Quality Variation Across Sources\nIssue: Some automatically generated Q&A pairs lacked cultural depth or were too generic.\nSolution: Manual review and filtering removed low-quality pairs. Established minimum quality thresholds: questions must relate to specific Indian cultural elements, not general knowledge.\n\n\n\n4.5.13 Challenge: Dataset Imbalance\nIssue: Initial collection yielded vastly different dataset sizes (e.g., 923 superstitions vs 500 Wikipedia).\nSolution: Accepted natural variation but ensured minimum 500 pairs per dataset for adequate representation.\n\n\n4.5.14 Lessons Learned\n\n4.5.14.1 AI Augmentation vs Replacement\nGemini proved invaluable for scaling Q&A generation, but human oversight remained essential. The most effective approach combined: - AI for rapid generation of candidate Q&A pairs - Human judgment for quality filtering and validation - Domain expertise (team’s cultural knowledge) for assessing accuracy\n\n\n4.5.14.2 Importance of Theoretical Frameworks\nThe 8 pillars of culture provided crucial structure for dataset selection. Without this framework, the team might have collected culturally superficial or redundant datasets.\n\n\n4.5.14.3 Value of Cross-Language Pipelines\nLeveraging both R and Python enabled the team to use the best tool for each task: - R Tidyverse for complex text wrangling - Python for API integration and JSON handling\nThis pragmatic approach outperformed trying to force all work into a single language.\n\n\n4.5.14.4 Strategic Pivoting\nThe Week 3 deliverable revamp demonstrated the importance of adaptive project management. Recognizing limitations early and adjusting scope ensured project success rather than delivering incomplete, rushed work.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span>"
    ]
  },
  {
    "objectID": "Week1.html#moses-mandale-deliverables",
    "href": "Week1.html#moses-mandale-deliverables",
    "title": "2  Week 1: Foundations & Data Collection",
    "section": "2.5 Moses Mandale Deliverables",
    "text": "2.5 Moses Mandale Deliverables\nAI assistance: Claude Sonnet 4.5 was used to create commands that I ran on Titan to get all the GPU specs available on Titan’s GPU Nodes. Through this I was able to find out that they are NVIDIA A30 GPUs with 24.6 GB of VRAM. After doing some thorough research on Google about the Candidate base models, I then had a debate with Claude Sonnet 4.5 about which base model would fit and after that debate the LlaMA-3 8B Instruct was the best fit. Claude Sonnet 4.5 was used to troubleshoot python package version conflicts and was able to assist with adding version numbers to the packages in the requirements.txt file allowing me to setup the Titan environment for the training correctly. (accessed Nov, 2025).\n\n2.5.1 Overview\nWeek 1 focused on establishing the technical foundation for fine-tuning a large language model (LLM) with Indian cultural awareness. The primary deliverables included researching and selecting an appropriate base model, configuring the Titan GPU environment for model training, conducting baseline tests, and documenting technical specifications for the project.\n\n\n2.5.2 Model Selection Process\n\n2.5.2.1 Evaluation Criteria\nThe model selection process evaluated five candidate models based on three critical criteria:\n\nCompatibility with Hardware: The model must fit within the 24.6 GB VRAM available on the NVIDIA A30 GPU\nTraining Feasibility: The model must be trainable within a reasonable timeframe (15-18 hours estimated)\nCultural Reasoning Capability: The model must have sufficient capacity to learn and generate culturally nuanced responses\n\n\n\n2.5.2.2 Candidate Models Comparison\n\n\n\n\n\n\n\n\n\nModel\nDescription\nWhy Consider It?\nWhy NOT Use It?\n\n\n\n\nGPT-2 (1.5B)\nOlder OpenAI model from 2019\nFree, well-documented, easy to train\nToo old - poor at complex cultural reasoning\n\n\nDistilGPT-2\nSmaller, faster version of GPT-2\nVery fast to train, low memory\nToo small - won’t capture cultural nuances\n\n\nIndicBERT\nModel trained on Indian languages\nMade for Indian content\nOnly understands, doesn’t generate well\n\n\nGPT-4o mini (API)\nOpenAI’s newest small model\nVery smart, easy to use via API\nCan’t truly fine-tune it for deep learning\n\n\nLLaMA-3 8B Instruct\nMeta’s powerful open-source model\nPerfect size, good at reasoning, you control it, fits in 24.6 GB GPU\nSELECTED - Best option for our setup\n\n\n\n\n\n2.5.2.3 Selection Rationale\nLLaMA-3 8B Instruct was selected as the optimal base model for the following reasons:\n\nParameter Count: With 8 billion parameters, the model provides sufficient capacity for learning cultural nuances without being prohibitively large\nHardware Compatibility: The model requires approximately 16-17 GB of GPU memory when loaded in half-precision (float16), well within the A30’s 24.6 GB capacity\nOpen-Source License: Meta’s permissive license allows full control over fine-tuning and deployment\nInstruction-Tuned: The “Instruct” variant has been pre-trained to follow instructions, providing a strong foundation for conversational applications\nCommunity Support: Extensive documentation and community resources facilitate troubleshooting and optimization\n\nKey Technical Specifications: - Model Parameters: 8,037,076,992 (8.03 billion) - Architecture: Decoder-only transformer - Precision: Half-precision (float16) for memory efficiency - License: Meta LLaMA 3 Community License\n\n\n\n2.5.3 Titan GPU Environment Setup\n\n2.5.3.1 Hardware Configuration\nThe Titan supercomputer provided the computational infrastructure for this project. The allocated resources included:\nGPU Specifications: - Model: NVIDIA A30 - Memory: 24.6 GB VRAM - Compute Capability: 8.0 - CUDA Version: 12.1\nSystem Specifications: - RAM: 32 GB minimum - Storage: 50 GB for model and checkpoints - CPU: 4 cores for data preprocessing\n\n\n2.5.3.2 Software Environment\nThe environment was configured with the following key components:\nCore Dependencies:\n# PyTorch with CUDA support\ntorch==2.5.1+cu121\n\n# Transformers ecosystem\ntransformers==4.36.0\naccelerate==0.25.0\npeft==0.7.0  # For LoRA fine-tuning\n\n# Data processing\ndatasets==2.15.0\npandas==2.1.0\nLoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning technique that adds small trainable adapter layers to the model while keeping the original weights frozen. This approach reduces memory requirements and training time while maintaining model quality.\n\n\n2.5.3.3 Installation Process\nThe environment setup involved creating a Python virtual environment and installing dependencies:\n# Create virtual environment\npython3 -m venv llama_env\nsource llama_env/bin/activate\n\n# Install PyTorch with CUDA 12.1\npip install torch torchvision torchaudio \\\n    --index-url https://download.pytorch.org/whl/cu121\n\n# Install Transformers and PEFT\npip install transformers accelerate peft bitsandbytes\nThe installation completed successfully, confirming GPU availability and proper CUDA configuration:\n\n\n\nTitan GPU environment setup confirmation\n\n\n\n\n\n2.5.4 Baseline Testing\n\n2.5.4.1 Model Loading Test\nInitial tests verified that the LLaMA-3 8B model could be successfully loaded onto the GPU:\nTest Results: - Tokenizer loading: Successful - Model loading time: ~55 seconds - GPU memory usage: 16.06 GB (65% of available VRAM) - GPU memory cached: 17.23 GB\n\n\n2.5.4.2 LoRA Configuration Test\nLoRA adapters were configured to enable efficient fine-tuning:\nLoRA Parameters: - Rank (r): 16 - Alpha: 32 - Target modules: q_proj, v_proj (query and value projection layers) - Dropout: 0.05 - Trainable parameters: 6,815,744 (0.08% of total parameters)\nThis configuration means only 0.08% of the model’s parameters need to be trained, dramatically reducing memory requirements and training time while maintaining effectiveness.\n\n\n\nBaseline test output showing model loading and LoRA setup\n\n\n\n\n2.5.4.3 Inference Test\nA simple inference test confirmed the model’s ability to generate culturally relevant responses:\nTest Prompt: “Explain the cultural significance of Diwali in India.”\nModel Response: The model successfully generated a coherent explanation of Diwali, describing it as the Festival of Lights celebrated over five days, typically in October or November. The response demonstrated the base model’s existing cultural knowledge and confirmed that the inference pipeline was functioning correctly.\n\n\n\n2.5.5 Technical Specifications Documentation\n\n2.5.5.1 Hardware Requirements\n\n\n\nComponent\nSpecification\n\n\n\n\nGPU Model\nNVIDIA A30\n\n\nGPU Memory\n24.6 GB VRAM\n\n\nGPU Compute\n8.0 capability\n\n\nTraining Time Estimate\n15-18 hours\n\n\n\n\n\n2.5.5.2 Model Architecture\n\n\n\nSpecification\nValue\n\n\n\n\nBase Model\nLLaMA-3 8B Instruct\n\n\nTotal Parameters\n8,037,076,992\n\n\nArchitecture\nDecoder-only transformer\n\n\nContext Window\n8,192 tokens\n\n\n\n\n\n2.5.5.3 Fine-Tuning Configuration\n\n\n\n\n\n\n\n\nParameter\nValue\nExplanation\n\n\n\n\nFine-tuning Method\nLoRA (Low-Rank Adaptation)\nParameter-efficient technique\n\n\nLoRA Rank (r)\n16\nControls adapter size\n\n\nLoRA Alpha\n32\nScaling factor for adapters\n\n\nTarget Modules\nq_proj, v_proj\nQuery and value projections\n\n\nTrainable Parameters\n6,815,744 (0.08%)\nOnly adapters are trained\n\n\nTotal Parameters\n8,043,892,736\nBase + adapter parameters\n\n\n\n\n\n\nTechnical specifications diagram\n\n\n\n\n2.5.5.4 System Configuration\nDirectory Structure:\n/home/mmadale/CSC463/conlingo/\n├── data/\n│   ├── raw/              # Original datasets\n│   ├── processed/        # Cleaned and formatted data\n│   └── test/             # Test sets\n├── models/\n│   ├── base/             # LLaMA-3 base model\n│   ├── finetuned/        # Fine-tuned models\n│   └── checkpoints/      # Training checkpoints\n├── scripts/\n│   ├── setup/            # Environment setup scripts\n│   ├── training/         # Training scripts\n│   └── evaluation/       # Evaluation scripts\n├── logs/                 # Training and job logs\n└── llama_env/            # Python virtual environment\n\n\n\n2.5.6 Key Achievements\nWeek 1 successfully established the technical foundation for the project:\n\nModel Selection: Identified and justified LLaMA-3 8B Instruct as the optimal base model\nEnvironment Configuration: Set up a fully functional GPU environment on Titan with all required dependencies\nBaseline Validation: Confirmed the model loads correctly, uses GPU resources efficiently, and can generate responses\nLoRA Integration: Configured parameter-efficient fine-tuning with only 0.08% of parameters trainable\nDocumentation: Created comprehensive technical specifications for reproducibility\n\n\n\n2.5.7 Challenges and Solutions\nChallenge 1: HuggingFace Authentication - Issue: LLaMA-3 requires accepting Meta’s license agreement through HuggingFace - Solution: Created HuggingFace account, accepted license terms, and configured authentication token\nChallenge 2: GPU Memory Management - Issue: Full-precision model (float32) would exceed 24.6 GB VRAM - Solution: Used half-precision (float16) loading, reducing memory footprint to ~16 GB\nChallenge 3: Training Efficiency - Issue: Fine-tuning all 8 billion parameters would be computationally expensive - Solution: Implemented LoRA adapters, training only 6.8 million parameters (0.08%)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: Foundations & Data Collection</span>"
    ]
  },
  {
    "objectID": "Week2.html#moses-mandale-deliverables",
    "href": "Week2.html#moses-mandale-deliverables",
    "title": "3  Week 2: Data Cleaning & Pipeline Setup",
    "section": "3.5 Moses Mandale Deliverables",
    "text": "3.5 Moses Mandale Deliverables\nAI assistance:Claude Sonnet 4.5 was used to optimize the initial finetune_lora.py to use LoRA fine tuning instead of the normal fine tuning to that the fine tuning is more efficient. Claude Sonnet 4.5 was used to troubleshoot “memory exceeded” errors and optimize the training parameters like number of epochs and batch size to fit the 24GB RAM that is available on Titan. This an error that was really troubling me because the script was already optimized with LoRA and was wondering what exactly was missing (accessed Nov, 2025).\n\n3.5.1 Overview\nWeek 2 focused on developing the complete fine-tuning pipeline for the LLaMA-3 8B model. The primary deliverables included creating data preprocessing scripts, implementing the LoRA fine-tuning workflow, training the first model on Indian superstition data, and validating the training process. This week transformed the configured environment from Week 1 into a functional training system.\n\n\n3.5.2 Fine-Tuning Pipeline Architecture\n\n3.5.2.1 Pipeline Overview\nThe fine-tuning pipeline consists of two main stages: data preprocessing and model training. The architecture was designed to be modular, allowing for easy iteration on different datasets while maintaining consistent formatting and training procedures.\n\n\n\nFine-tuning pipeline architecture\n\n\nPipeline Stages:\n\nData Preprocessing (preprocess_data.py): Converts raw CSV data into instruction-response format suitable for LLaMA-3 fine-tuning\nModel Fine-Tuning (finetune_lora.py): Trains LoRA adapters on the preprocessed data while keeping base model weights frozen\n\n\n\n\n3.5.3 Data Flow\nRaw CSV (Indian Superstitions)\n    ↓\npreprocess_data.py\n    ↓\nJSON instruction-response pairs\n    ↓\nTrain/Validation Split (90/10)\n    ↓\nfinetune_lora.py\n    ↓\nFine-tuned Model with LoRA Adapters\n\n\n3.5.4 Data Preprocessing\n\n3.5.4.1 Input Dataset\nThe preprocessing pipeline began with a dataset of Indian superstitions collected by team member Suwilanji from Kaggle. The raw dataset contained:\n\nTotal Rows: 660 entries\nKey Columns: superstition_name, description, region, category, origin_theory, modern_status, is_harmful, source, user_contributed\nFormat: CSV file\n\n\n\n3.5.4.2 Preprocessing Script Design\nThe preprocess_data.py script implements a systematic transformation from raw tabular data to instruction-response format suitable for LLM fine-tuning.\nKey Processing Steps:\n\nColumn Detection: Automatically identifies name and description columns\nData Cleaning: Filters out invalid entries (null values, insufficient length)\nUnicode Normalization: Converts Unicode quotation marks to ASCII equivalents\nQuestion Generation: Transforms superstition names into natural questions\nJSON Structuring: Creates instruction-response pairs with metadata\nTrain/Validation Split: Implements 90/10 split with random shuffling\n\nData Transformation Example:\n# Input (CSV row)\nsuperstition_name: \"Mirror breaks, 7 years bad luck\"\ndescription: \"Believed to bring misfortune if a mirror breaks.\"\n\n# Output (JSON)\n{\n  \"instruction\": \"What is the cultural significance and meaning of the superstition: 'Mirror breaks, 7 years bad luck'?\",\n  \"response\": \"Believed to bring misfortune if a mirror breaks.\",\n  \"metadata\": {\n    \"source\": \"superstition_dataset\",\n    \"category\": \"cultural_beliefs\",\n    \"original_name\": \"Mirror breaks, 7 years bad luck\"\n  }\n}\n\n\n\nData preprocessing transformation from CSV to JSON\n\n\n\n\n3.5.4.3 Preprocessing Results\nThe preprocessing pipeline successfully transformed the raw dataset:\nInput Statistics: - Raw CSV entries: 660 - Valid entries after filtering: 658\nOutput Statistics: - Training examples: 592 (90%) - Validation examples: 66 (10%) - Total processed: 658\nData Quality Measures: - Filtered entries: 2 (entries with null values or insufficient text length) - Unicode normalization: Applied to all entries for ASCII compatibility - Question format: Consistent across all examples\n\n\n\n3.5.5 Fine-Tuning Implementation\n\n3.5.5.1 Training Configuration\nThe fine-tuning script (finetune_lora.py) implements LoRA-based parameter-efficient fine-tuning with the following configuration:\nLoRA Hyperparameters:\n\n\n\n\n\n\n\n\nParameter\nValue\nPurpose\n\n\n\n\nRank (r)\n16\nDimensionality of adapter matrices\n\n\nAlpha\n32\nScaling factor for adapter outputs\n\n\nTarget Modules\nq_proj, v_proj, k_proj, o_proj\nAttention projection layers\n\n\nDropout\n0.05\nRegularization to prevent overfitting\n\n\nBias\nNone\nNo additional bias terms in adapters\n\n\n\nTraining Hyperparameters:\n\n\n\nParameter\nValue\nExplanation\n\n\n\n\nEpochs\n3\nNumber of passes through training data\n\n\nPer-device Batch Size\n4\nExamples processed simultaneously\n\n\nGradient Accumulation Steps\n8\nEffective batch size = 4 × 8 = 32\n\n\nLearning Rate\n2e-4\nStep size for weight updates\n\n\nMax Sequence Length\n512\nMaximum tokens per example\n\n\nEvaluation Strategy\nSteps\nEvaluate periodically during training\n\n\nSave Steps\n100\nCheckpoint frequency\n\n\n\nEffective Training Configuration: - Effective batch size: 32 examples per update - Total training steps: ~54 steps (592 examples / 32 batch size × 3 epochs) - Gradient checkpointing: Enabled for memory efficiency\n\n\n\n3.5.6 Model Architecture\nThe fine-tuning process adds LoRA adapters to specific layers of the LLaMA-3 8B model:\nParameter Breakdown: - Base model parameters (frozen): 8,030,261,248 - LoRA trainable parameters: 13,631,488 - Total parameters: 8,043,892,736 - Trainable percentage: 0.17%\nThis parameter-efficient approach enables training with limited GPU memory while maintaining model quality.\n\n\n\nTraining configuration output\n\n\n\n3.5.6.1 Tokenization and Data Formatting\nThe training script implements a custom formatting function to structure data for instruction-following:\ndef format_instruction(example):\n    \"\"\"Format question-answer pair for training\"\"\"\n    text = f\"### Question:\\n{example['instruction']}\\n\\n### Answer:\\n{example['response']}\"\n    return text\nThis format provides clear delineation between the instruction (question) and expected response, helping the model learn the question-answering structure.\n\n\n\n3.5.7 Training Process and Results\n\n3.5.7.1 Training Progress\nThe model was trained for 3 epochs on the Indian superstition dataset. The training process exhibited expected behavior with decreasing loss values:\nLoss Progression:\n\n\n\nEpoch\nTraining Loss\nGradient Norm\nLearning Rate\n\n\n\n\n0.6\n3.4506\n1.9107\n3.6e-05\n\n\n1.18\n3.0625\n1.4718\n7.6e-05\n\n\n1.78\n2.5055\n1.6695\n1.16e-04\n\n\n2.36\n2.1748\n0.9833\n1.56e-04\n\n\n2.96\n2.0223\n1.2655\n1.96e-04\n\n\n\nFinal Metrics: - Training loss: 2.6282 - Validation loss: 2.0647 - Training time: Approximately 10 minutes - GPU memory usage: ~16 GB during training\n\n\n\nTraining progress showing loss decrease over epochs\n\n\n\n\n3.5.7.2 Loss Analysis\nThe training loss decreased from 3.45 to 2.02, indicating successful learning:\n\nInitial loss (3.45): Model has limited knowledge of the specific cultural content\nMid-training (2.51 at epoch 1.78): Model begins recognizing patterns in superstition descriptions\nFinal loss (2.02): Model has learned to generate appropriate responses for cultural questions\n\nThe validation loss of 2.06 is slightly higher than the final training loss, indicating minimal overfitting. This small gap suggests the model generalizes reasonably well to unseen examples.\n\n\n3.5.7.3 Model Checkpointing\nThe fine-tuning pipeline automatically saves model checkpoints:\nSaved Artifacts: - Final model: /home/mmadale/CSC463/conlingo/models/finetuned/final_model/ - Checkpoint size: Approximately 1.3 GB (includes LoRA adapters only) - Configuration files: adapter_config.json, adapter_model.bin\nThe saved model contains only the LoRA adapters, not the full base model. This significantly reduces storage requirements while preserving the fine-tuning results.\n\n\n\n3.5.8 Code Implementation Highlights\n\n3.5.8.1 Data Preprocessing\nKey implementation details from preprocess_data.py:\n# Unicode normalization for ASCII compatibility\nsuperstition_name = superstition_name.replace('\\u2019', \"'\").replace('\\u2018', \"'\")\nsuperstition_name = superstition_name.replace('\\u201c', '\"').replace('\\u201d', '\"')\n\n# Question generation\nquestion = f\"What is the cultural significance and meaning of the superstition: '{superstition_name}'?\"\n\n# Structured output\ninstruction_response = {\n    \"instruction\": question,\n    \"response\": description,\n    \"metadata\": {\n        \"source\": \"superstition_dataset\",\n        \"category\": \"cultural_beliefs\",\n        \"original_name\": superstition_name\n    }\n}\n\n\n3.5.8.2 Fine-Tuning Configuration\nKey implementation details from finetune_lora.py:\n# LoRA configuration\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=8,\n    learning_rate=2e-4,\n    evaluation_strategy=\"steps\",\n    save_steps=100\n)\n\n\n\nCode snippet showing training configuration\n\n\n\n\n\n3.5.9 Technical Challenges and Solutions\n\n3.5.9.1 Memory Management\nChallenge: Training with batch size of 8 caused out-of-memory errors on the 24.6 GB GPU.\nSolution: Implemented gradient accumulation with batch size 4 and accumulation steps 8, achieving effective batch size of 32 while staying within memory constraints.\n\n\n3.5.9.2 Unicode Character Handling\nChallenge: Original dataset contained Unicode quotation marks (e.g., \\u2019, \\u201c) that could cause tokenization issues.\nSolution: Implemented systematic Unicode-to-ASCII conversion in preprocessing script, ensuring consistent character encoding throughout the dataset.\n\n\n3.5.9.3 Data Quality\nChallenge: Some CSV entries contained null values or insufficient text.\nSolution: Added validation checks in preprocessing script to filter entries with: - Null/missing superstition names or descriptions - Superstition names shorter than 3 characters - Descriptions shorter than 10 characters\n\n\n\n3.5.10 Validation and Quality Assurance\n\n3.5.10.1 Data Validation\nThe preprocessing script includes multiple validation steps:\n\nColumn Detection: Automatically identifies relevant columns, adapting to CSV structure\nNull Filtering: Removes entries with missing critical information\nLength Validation: Ensures minimum content length for meaningful training\nUnicode Normalization: Standardizes character encoding\n\nValidation Results: - Entries processed: 660 - Entries passed validation: 658 (99.7% retention rate) - Entries filtered: 2 (0.3%)\n\n\n3.5.10.2 Training Validation\nThe training process includes continuous validation:\n\nGradient Norm Monitoring: Tracked to detect instability (all values &lt; 2.0, indicating stable training)\nValidation Loss: Evaluated every 100 steps to monitor generalization\nCheckpoint Saving: Automatic preservation of model state for recovery\n\n\n\n\n3.5.11 Key Achievements\nWeek 2 successfully delivered a complete fine-tuning pipeline and initial trained model:\n\nData Preprocessing Pipeline: Created robust script converting CSV to instruction-response format\n658 Training Examples: Processed and validated dataset of Indian superstitions\nFine-Tuning Script: Implemented LoRA-based training with proper configuration\nTrained Model: Successfully fine-tuned LLaMA-3 8B on cultural content\nLoss Reduction: Achieved 41% decrease in training loss (3.45 → 2.02)\nModel Artifacts: Generated reusable LoRA adapters (~1.3 GB) for inference\nReproducible Pipeline: Created modular scripts adaptable to new datasets\n\n\n\n3.5.12 Lessons Learned\n\n3.5.12.1 Pipeline Design\nThe modular pipeline architecture proved effective for rapid iteration. Separating preprocessing and training into distinct scripts allowed independent testing and debugging of each component.\n\n\n3.5.12.2 Batch Size Optimization\nFinding the optimal batch size required balancing: - GPU memory constraints (24.6 GB available) - Training stability (larger batches → more stable gradients) - Training speed (larger batches → fewer updates per epoch)\nThe final configuration (batch size 4 with 8 accumulation steps) effectively balanced these factors.\n\n\n3.5.12.3 Data Quality Impact\nThe high validation rate (99.7% of entries passed filtering) indicated good initial data quality. Manual inspection of filtered entries confirmed they were legitimately problematic (null values), validating the filtering criteria.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2: Data Cleaning & Pipeline Setup</span>"
    ]
  },
  {
    "objectID": "Week4.html#moses-madale-deliverables",
    "href": "Week4.html#moses-madale-deliverables",
    "title": "5  Week 4: Data Searching, Data Vetting, Dataset Collection, Data Cleaning",
    "section": "5.5 Moses Madale Deliverables",
    "text": "5.5 Moses Madale Deliverables\nAI assistance: Claude Sonnet 4.5 was used to help me troubleshoot issues with setting up an identical virtual environment on Will’s Titan account to ensure that he can also run the fine-tuning from his account smoothly, there were issues with python package version conflicts but with assistance from Claude Sonnet, these issues were resolved more efficiently (accessed Nov, 2025).\n\n5.5.1 Model Training and Evaluation\n\n5.5.1.1 Overview\nWeek 4 transformed the datasets collected in Week 3 into trained models ready for evaluation. The primary focus was training three distinct fine-tuned models—one each for YouTube Transcripts, Wikipedia, and TED Talks datasets—testing them on 20 culturally nuanced questions, and preparing results for comprehensive analysis. This week demonstrated that different cultural data sources produce models with varying capabilities, setting the stage for Week 5’s comparative evaluation against baseline and state-of-the-art models.\n\n\n5.5.1.2 Training Infrastructure and Workflow\n\n5.5.1.2.1 Fine-Tuning Pipeline Architecture\nBuilding on Week 2’s pipeline, Week 4 established a standardized workflow for training multiple models systematically:\n\n\n\nFine-tuning pipeline for multiple datasets\n\n\nPipeline Stages:\n\nDataset Loading: Read JSONL file with Q&A pairs\nTrain/Validation Split: 90/10 split with random seed for reproducibility\nTokenization: Convert text to model-compatible format with padding\nBase Model Loading: Load LLaMA-3 8B Instruct with half-precision\nLoRA Configuration: Add trainable adapters to attention layers\nTraining: Run supervised fine-tuning with gradient accumulation\nEvaluation: Assess performance on validation set\nModel Saving: Store LoRA adapters for inference\n\n\n\n\n5.5.1.3 Standardized Training Configuration\nAll three models used identical hyperparameters to ensure fair comparison:\n\n\n\nHyperparameter\nValue\nPurpose\n\n\n\n\nEpochs\n3\nSufficient for convergence without overfitting\n\n\nBatch Size\n2\nMaximum fitting in 24 GB VRAM\n\n\nGradient Accumulation\n16\nEffective batch size of 32\n\n\nLearning Rate\n2e-4\nStandard for LoRA fine-tuning\n\n\nLR Scheduler\nCosine\nGradual learning rate decay\n\n\nWarmup Steps\n50\nPrevents early training instability\n\n\nMax Sequence Length\n512 tokens\nAccommodates most Q&A pairs\n\n\nLoRA Rank (r)\n16\nBalance between capacity and efficiency\n\n\nLoRA Alpha\n32\nScaling factor for adapter outputs\n\n\n\nLoRA Target Modules: - q_proj (Query projection) - v_proj (Value projection) - k_proj (Key projection) - o_proj (Output projection)\nThese attention mechanism components were selected because they capture the most information during text generation while keeping trainable parameters to just 0.17% of the total model.\n\n\n5.5.1.4 Training Script Structure\nEach fine-tuning script followed a consistent 10-step process. Here is the complete Wikipedia training script as an example:\n#| eval: false\n\n#!/usr/bin/env python3\n\"\"\"\nLoRA Fine-Tuning for Wikipedia Data\nFine-tunes LLaMA-3 8B on Indian Wikipedia Q&A pairs\n\"\"\"\n\nimport torch\nimport json\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling\n)\nfrom peft import LoraConfig, get_peft_model\nfrom datasets import Dataset\nimport os\nfrom sklearn.model_selection import train_test_split\n\nprint(\"=\"*60)\nprint(\"Wikipedia LoRA Fine-Tuning Pipeline\")\nprint(\"=\"*60)\n\n# Paths\nDATA_PATH = \"/home/mmadale/CSC463/conlingo/huggingface_data/indian_wikipedia/data/wikipedia_qa.jsonl\"\nOUTPUT_DIR = \"/home/mmadale/CSC463/conlingo/models/wikipedia\"\nMODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\n# Create output directory\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"\\n1. Loading data from: {DATA_PATH}\")\n\n# Load JSONL data\ndata = []\nwith open(DATA_PATH, 'r', encoding='utf-8') as f:\n    for line in f:\n        data.append(json.loads(line))\n\nprint(f\"Total examples: {len(data)}\")\n\n# Extract question-answer pairs\nexamples = []\nfor item in data:\n    examples.append({\n        \"question\": item[\"question\"],\n        \"answer\": item[\"answer\"]\n    })\n\n# Split into train/validation (90/10)\ntrain_data, val_data = train_test_split(examples, test_size=0.1, random_state=42)\n\nprint(f\"Training examples: {len(train_data)}\")\nprint(f\"Validation examples: {len(val_data)}\")\n\nprint(f\"\\n2. Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nprint(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n\nprint(f\"\\n3. Preparing datasets...\")\n\ndef format_instruction(example):\n    \"\"\"Format question-answer pair for training\"\"\"\n    text = f\"### Question:\\n{example['question']}\\n\\n### Answer:\\n{example['answer']}\"\n    return text\n\ndef tokenize_function(example):\n    \"\"\"Tokenize examples with padding and truncation\"\"\"\n    text = format_instruction(example)\n    \n    tokenized = tokenizer(\n        text,\n        truncation=True,\n        max_length=512,\n        padding=\"max_length\",\n        return_tensors=None\n    )\n    \n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized\n\n# Convert to HuggingFace Dataset format\ntrain_dataset = Dataset.from_list(train_data)\nval_dataset = Dataset.from_list(val_data)\n\n# Tokenize\nprint(\"Tokenizing training data...\")\ntrain_dataset = train_dataset.map(\n    tokenize_function,\n    remove_columns=train_dataset.column_names\n)\n\nprint(\"Tokenizing validation data...\")\nval_dataset = val_dataset.map(\n    tokenize_function,\n    remove_columns=val_dataset.column_names\n)\n\nprint(f\"Training dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(val_dataset)}\")\n\nprint(f\"\\n4. Loading base model...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\n\nmodel.gradient_checkpointing_enable()\n\nprint(f\"Model loaded: {model.__class__.__name__}\")\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\nprint(f\"\\n5. Configuring LoRA...\")\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\n\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\n\nprint(f\"LoRA configured successfully\")\nprint(f\"Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.4f}%)\")\nprint(f\"Total parameters: {total_params:,}\")\n\nprint(f\"\\n6. Setting up training arguments...\")\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=16,\n    learning_rate=2e-4,\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=50,\n    logging_steps=10,\n    save_steps=50,\n    eval_steps=50,\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    fp16=True,\n    report_to=\"none\",\n    save_total_limit=2,\n    remove_unused_columns=False,\n    gradient_checkpointing=True\n)\n\nprint(\"Training configuration:\")\nprint(f\"  Epochs: {training_args.num_train_epochs}\")\nprint(f\"  Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  Learning rate: {training_args.learning_rate}\")\n\nprint(f\"\\n7. Initializing trainer...\")\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator\n)\n\nprint(\"Trainer initialized successfully!\")\n\nprint(f\"\\n8. Starting training...\")\nprint(\"=\"*60)\n\ntrainer.train()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Training complete!\")\nprint(\"=\"*60)\n\nprint(f\"\\n9. Saving final model...\")\nmodel.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\ntokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n\nprint(f\"Model saved to: {OUTPUT_DIR}/final_model\")\n\nprint(f\"\\n10. Final evaluation metrics:\")\neval_results = trainer.evaluate()\nfor key, value in eval_results.items():\n    print(f\"  {key}: {value:.4f}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Fine-tuning pipeline complete!\")\nprint(\"=\"*60)\nThe same script structure was adapted for TED Talks and YouTube Transcripts by simply changing the DATA_PATH and OUTPUT_DIR variables. This standardization enabled rapid iteration and consistent results across different datasets.\n\n\n\n5.5.2 Model Training Results\n\n5.5.2.1 Wikipedia Model Training\nDataset Statistics: - Total examples: 500 - Training set: 450 (90%) - Validation set: 50 (10%) - Training duration: 8 minutes 29 seconds\nTraining Progress:\n============================================================\nWikipedia LoRA Fine-Tuning Pipeline\n============================================================\n\n1. Loading data from: .../indian_wikipedia/data/wikipedia_qa.jsonl\nTotal examples: 500\nTraining examples: 450\nValidation examples: 50\n\n2. Loading tokenizer...\nTokenizer loaded: PreTrainedTokenizerFast\n\n3. Preparing datasets...\nTraining dataset size: 450\nValidation dataset size: 50\n\n4. Loading base model...\nModel loaded: LlamaForCausalLM\nModel parameters: 8,030,261,248\n\n5. Configuring LoRA...\nLoRA configured successfully\nTrainable parameters: 13,631,488 (0.1695%)\nTotal parameters: 8,043,892,736\n\n6. Setting up training arguments...\nTraining configuration:\n  Epochs: 3\n  Batch size: 2\n  Gradient accumulation: 16\n  Effective batch size: 32\n  Learning rate: 0.0002\n\n8. Starting training...\n============================================================\n{'loss': 2.433, 'grad_norm': 1.730, 'learning_rate': 3.6e-05, 'epoch': 0.71}\n{'loss': 1.945, 'grad_norm': 2.509, 'learning_rate': 7.6e-05, 'epoch': 1.36}\n{'loss': 1.468, 'grad_norm': 4.694, 'learning_rate': 0.000116, 'epoch': 2.0}\n{'loss': 1.281, 'grad_norm': 0.774, 'learning_rate': 0.000156, 'epoch': 2.71}\n\n============================================================\nTraining complete!\n============================================================\n\n10. Final evaluation metrics:\n  eval_loss: 1.2456\n  eval_runtime: 5.8086\n  epoch: 3.0000\nPerformance Analysis:\nThe Wikipedia model achieved the lowest final loss (1.2456) among all three models, indicating: - Excellent convergence on encyclopedic content - Strong pattern recognition for factual Q&A - Effective learning from well-structured Wikipedia articles\nThe loss decreased from 2.433 → 1.281 (47% reduction), demonstrating substantial learning without overfitting (validation loss 1.2456 close to final training loss 1.281).\n\n\n5.5.2.2 YouTube Transcripts Model Training\nDataset Statistics: - Total examples: 512 - Training set: 460 (90%) - Validation set: 52 (10%) - Training duration: 8 minutes 46 seconds\nTraining Progress:\n============================================================\nYouTube Transcripts LoRA Fine-Tuning Pipeline\n============================================================\n\n1. Loading data from: .../youtube_data/data/final_youtube_transcript_data.jsonl\nTotal examples: 512\nTraining examples: 460\nValidation examples: 52\n\n5. Configuring LoRA...\nTrainable parameters: 13,631,488 (0.1695%)\nTotal parameters: 8,043,892,736\n\n8. Starting training...\n============================================================\n{'loss': 3.935, 'grad_norm': 1.849, 'learning_rate': 3.6e-05, 'epoch': 0.7}\n{'loss': 3.363, 'grad_norm': 2.377, 'learning_rate': 7.6e-05, 'epoch': 1.35}\n{'loss': 2.618, 'grad_norm': 2.411, 'learning_rate': 0.000116, 'epoch': 2.0}\n{'loss': 2.324, 'grad_norm': 1.130, 'learning_rate': 0.000156, 'epoch': 2.7}\n\n============================================================\nTraining complete!\n============================================================\n\n10. Final evaluation metrics:\n  eval_loss: 2.2825\n  eval_runtime: 6.0515\n  epoch: 3.0000\nPerformance Analysis:\nThe YouTube model exhibited the highest loss (2.2825) among the three models, suggesting: - Greater complexity in conversational, informal content - Diverse speaking styles and code-mixing (Hinglish) challenging to model - Richer linguistic variation compared to encyclopedic content\nDespite higher loss, the model achieved 41% loss reduction (3.935 → 2.324), indicating successful learning of conversational patterns and cultural idioms prevalent in YouTube content.\n\n\n5.5.2.3 TED Talks Model Training\nDataset Statistics: - Total examples: 596 - Training set: 536 (90%) - Validation set: 60 (10%) - Training duration: 10 minutes 45 seconds (longest due to largest dataset)\nTraining Progress:\n============================================================\nTED Talks LoRA Fine-Tuning Pipeline\n============================================================\n\n1. Loading data from: .../indian_ted_talks/data/ted_talks_qa.jsonl\nTotal examples: 596\nTraining examples: 536\nValidation examples: 60\n\n5. Configuring LoRA...\nTrainable parameters: 13,631,488 (0.1695%)\nTotal parameters: 8,043,892,736\n\n8. Starting training...\n============================================================\n{'loss': 3.451, 'grad_norm': 1.911, 'learning_rate': 3.6e-05, 'epoch': 0.6}\n{'loss': 3.063, 'grad_norm': 1.472, 'learning_rate': 7.6e-05, 'epoch': 1.18}\n{'loss': 2.506, 'grad_norm': 1.670, 'learning_rate': 0.000116, 'epoch': 1.78}\n{'loss': 2.175, 'grad_norm': 0.983, 'learning_rate': 0.000156, 'epoch': 2.36}\n{'loss': 2.022, 'grad_norm': 1.266, 'learning_rate': 0.000196, 'epoch': 2.96}\n\n============================================================\nTraining complete!\n============================================================\n\n10. Final evaluation metrics:\n  eval_loss: 2.0647\n  eval_runtime: 7.0019\n  epoch: 3.0000\nPerformance Analysis:\nThe TED Talks model achieved intermediate loss (2.0647), positioned between Wikipedia’s factual clarity and YouTube’s conversational complexity. This reflects: - Structured presentation style (more formal than YouTube) - Substantive content (less dry than Wikipedia) - Blend of storytelling and information delivery\nLoss reduction of 41% (3.451 → 2.022) matched YouTube’s learning rate, suggesting comparable learning difficulty despite different content styles.\n\n\n5.5.2.4 Comparative Training Analysis\n\n\n\n\n\n\n\n\n\n\nModel\nTrain/Val Split\nTraining Time\nFinal Eval Loss\nLoss Reduction\n\n\n\n\nWikipedia\n450/50\n8:29\n1.2456\n47% (2.433→1.281)\n\n\nTED Talks\n536/60\n10:45\n2.0647\n41% (3.451→2.022)\n\n\nYouTube\n460/52\n8:46\n2.2825\n41% (3.935→2.324)\n\n\n\nKey Observations:\n\nWikipedia’s superiority in loss metrics: The encyclopedic, structured nature of Wikipedia articles enabled tighter convergence\nDataset size correlation: TED Talks (596 examples) took longest to train, but more data didn’t necessarily yield lowest loss\nConsistent learning rates: All models showed 40-47% loss reduction, indicating the training regimen was effective across content types\nValidation alignment: Small gaps between final training loss and validation loss across all models indicate minimal overfitting\n\n\n\n\n5.5.3 Prompt Engineering for Inference\n\n5.5.3.1 System Prompt Design\nTo ensure culturally aware responses during testing, Moses designed a comprehensive system prompt that would be used consistently across all model evaluations:\n\n\n\nPrompt engineering code for culturally aware responses\n\n\nPrompt Structure:\nSYSTEM_PROMPT = \"\"\"You are a culturally aware guide with deep knowledge of \nIndian traditions, Christianity in India, and the beautiful intersections \nbetween faith and culture. When answering questions, draw from your \nunderstanding of Indian regional diversity, historical contexts, contemporary \npractices, and lived experiences. Provide comprehensive, thoughtful responses \n(150-200 words each) that would be valuable for someone doing serious research. \nInclude specific examples, acknowledge regional variations, and demonstrate \nsensitivity to both Hindu and Christian perspectives. Be conversational yet \nsubstantive – imagine you're having a meaningful conversation with someone \ngenuinely curious about these topics.\"\"\"\nDesign Rationale:\nThe prompt was carefully crafted to: - Establish Cultural Expertise: “Deep knowledge of Indian traditions” sets expectations for nuanced answers - Define Scope: “Christianity in India” and “faith-culture intersections” focus the domain - Specify Response Style: “Comprehensive, thoughtful responses (150-200 words)” prevents overly brief answers - Encourage Specificity: “Include specific examples, acknowledge regional variations” promotes detailed responses - Balance Tone: “Conversational yet substantive” avoids academic dryness while maintaining seriousness\n\n\n5.5.3.2 Question Formatting\nEach test question was formatted using the same structure established during training:\ndef format_question(question):\n    prompt = f\"### Question:\\n{question}\\n\\n### Answer:\\n\"\n    return prompt\nThis consistency between training and inference formats is critical for optimal model performance. The ### Question: and ### Answer: delimiters were familiar to the model from fine-tuning, enabling it to recognize when to start generating responses.\n\n\n5.5.3.3 Generation Parameters\nInference used carefully tuned generation parameters:\n\n\n\n\n\n\n\n\nParameter\nValue\nPurpose\n\n\n\n\nmax_new_tokens\n150\nSufficient for detailed answers without rambling\n\n\ntemperature\n0.7\nBalanced creativity and coherence\n\n\ndo_sample\nTrue\nEnable probabilistic sampling\n\n\ntop_p\n0.9\nNucleus sampling for quality control\n\n\npad_token_id\neos_token_id\nProper sequence termination\n\n\n\nTemperature Selection:\nThe temperature of 0.7 was chosen after preliminary testing: - 0.5: Too conservative, repetitive answers - 0.7: Balanced creativity with accuracy - 1.0: Occasional incoherence, overly creative\n\n\n\n5.5.4 Test Question Design and Evaluation Process\n\n5.5.4.1 Rohan’s 20-Question Framework\nTeam member Rohan designed 20 questions spanning multiple cultural dimensions and pillars. These questions were strategically crafted to test:\n\nInterfaith Understanding: Hindu-Christian intersections\nCultural Sensitivity: Handling religious diversity in institutional settings\nPractical Application: Real-world scenarios for Indian Christians\nTheological Integration: Biblical concepts in Indian cultural context\n\nExample Questions:\n\n“How can Diwali and Christmas both symbolize the victory of light over darkness in their respective traditions?”\n\nTests: Values & beliefs, religion & spirituality, comparative understanding\n\n“What themes of renewal in Diwali resonate with the idea of rebirth in Christian theology?”\n\nTests: Theological integration, cultural awareness, abstract reasoning\n\n“How might Indian Christians use Diwali to express biblical messages of hope?”\n\nTests: Practical application, contextualization, creativity\n\n“How can Christian schools acknowledge Diwali without compromising faith boundaries?”\n\nTests: Institutional sensitivity, balance, practical wisdom\n\n“What sensitivities should pastors consider when mentioning Hindu deities in Christmas homilies?”\n\nTests: Cultural sensitivity, religious boundaries, contextual awareness\n\n“How can Christian youth groups create shared Diwali–Christmas community service projects?”\n\nTests: Interfaith collaboration, practical implementation, social organization\n\n\nThese questions deliberately avoid simple factual recall, instead requiring: - Cultural nuance and sensitivity - Integration of multiple perspectives - Practical wisdom - Awareness of regional and social variations\n\n\n5.5.4.2 Testing Workflow\nMoses implemented a systematic testing process:\nStep 1: Model Loading\n# Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n    device_map=\"auto\",\n    torch_dtype=torch.float16\n)\n\n# Load fine-tuned LoRA adapters\nmodel = PeftModel.from_pretrained(base_model, FINETUNED_MODEL_PATH)\nStep 2: Question Iteration\nfor question in test_questions:\n    prompt = f\"### Question:\\n{question}\\n\\n### Answer:\\n\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=150,\n        temperature=0.7,\n        do_sample=True,\n        top_p=0.9,\n        pad_token_id=tokenizer.eos_token_id\n    )\n    \n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    answer = response.split(\"### Answer:\")[-1].strip()\nStep 3: Response Logging\nResponses were saved to structured text files:\n================================================================================\nFine-tuned: YouTube Transcripts\nTest Date: 2025-11-13 22:41:08\n================================================================================\n\n1. How can Diwali and Christmas both symbolize the victory of light over \ndarkness in their respective traditions?\n\nThe victory of light over darkness is what I think is very deeply rooted \nacross religious traditions here. The most obvious example is Diwali and \nits celebration of light defeating the forces of darkness, which is \nassociated with this concept of the divine that's been worshipped in many \nparts of India for thousands of years. At the same time, if you look at \nthe significance of Christmas, there's no doubt that it has an element of \nsymbolism around the victory of good over evil as well, because we celebrate \nthe birth of Jesus Christ who's associated with bringing light and hope to \nhumanity...\n\n\n5.5.4.3 Response Organization for Analysis\nMoses delivered results to Rohan in a standardized format:\nFile Structure:\nresponses/\n├── youtube_model_responses.txt\n├── wikipedia_model_responses.txt\n├── ted_talks_model_responses.txt\n└── baseline_model_responses.txt\nEach file contained: - Model identifier - Test date/time - All 20 questions - Complete model responses - Consistent formatting for comparison\nThis organization enabled Rohan to: - Directly compare responses across models - Track which model excelled at which questions - Identify patterns in cultural awareness - Score models on standardized rubrics\n\n\n\n5.5.5 Baseline Model Testing\n\n5.5.5.1 Control Variable Rationale\nTo establish whether fine-tuning actually improved cultural awareness, Moses tested the unfine-tuned LLaMA-3 8B Instruct model on the same 20 questions. This baseline served as a control variable, representing the model’s cultural knowledge “out of the box” without any Indian cultural training.\nTesting Approach:\n# Load base model WITHOUT LoRA adapters\nbaseline_model = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n    device_map=\"auto\",\n    torch_dtype=torch.float16\n)\n\n# Use same generation parameters as fine-tuned models\n# Same prompt format, same questions\n\n\n5.5.5.2 Expected Baseline Characteristics\nThe baseline model was expected to exhibit:\n\nGeneric Cultural Knowledge: Broad understanding of Diwali and Christianity from pre-training\nLack of Specificity: Missing regional variations, specific Indian Christian practices\nWestern Bias: Potential Anglo-centric perspectives on Christianity\nSurface-Level Connections: Obvious parallels without deep cultural integration\n\nHypothesis: Fine-tuned models would demonstrate superior: - Use of Indian terminology and concepts - Awareness of regional variations - Integration of Hindu-Christian contexts - Practical, lived-experience insights\n\n\n5.5.5.3 Baseline Integration into Evaluation\nThe baseline model responses joined the fine-tuned models in Rohan’s analysis framework, creating a 4-model comparison for Week 4:\n\nBaseline (unfine-tuned LLaMA-3 8B)\nWikipedia fine-tuned\nTED Talks fine-tuned\nYouTube Transcripts fine-tuned\n\nThis comparison would reveal whether fine-tuning provided measurable improvements over the base model’s existing capabilities.\n\n\n\n5.5.6 Combined Model Strategy and Deferral\n\n5.5.6.1 Original Plan: Two Combined Models\nWeek 4 initially planned to create two ensemble models:\n\nTop 3 Datasets Model: Combine the three best-performing datasets based on evaluation results\nAll 5 Datasets Model: Combine all datasets (Superstitions, Constitution, Wikipedia, TED Talks, YouTube)\n\nIntended Workflow:\nWeek 4 Model Evaluation\n    ↓\nIdentify Top 3 Performers\n    ↓\nTrain Combined Model (Top 3)\n    ↓\nTrain Combined Model (All 5)\n    ↓\nCompare Combined vs Individual Models\n\n\n5.5.6.2 Time Constraint Reality\nAs Week 4 progressed, the team encountered a critical timeline issue:\nChallenge: Evaluating four models (baseline + 3 fine-tuned) required: - Rohan analyzing 80 total responses (20 questions × 4 models) - Developing scoring rubrics - Conducting qualitative analysis - Presenting findings to team\nThis evaluation process extended into the final days of Week 4, leaving insufficient time to: 1. Complete evaluation 2. Identify top 3 datasets 3. Prepare combined training data 4. Train combined model 5. Test combined model\nDecision: Defer combined model training to Week 5\n\n\n5.5.6.3 Strategic Pivot to Week 5\nThe team made a strategic decision to prioritize quality over rushing:\nWeek 4 Deliverable: Complete individual model training and testing Week 5 Deliverable: Train “Conlingo 2.0” (combined all 5 datasets) as the flagship model for final comparison\nRationale: - Individual model results provided valuable insights regardless - Combined model training (~10-15 minutes) could fit in Week 5 - Allowed proper evaluation of individual models first - Ensured combined model incorporated lessons learned\nConlingo 2.0 Definition: The final combined model trained on all 5 approved datasets (3,031 Q&A pairs total), representing the team’s comprehensive approach to Indian cultural awareness training.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span>"
    ]
  },
  {
    "objectID": "Week5.html#moses-madale-deliverables",
    "href": "Week5.html#moses-madale-deliverables",
    "title": "6  Week 5: Compare Fine-tuned model with RAG implementation and ChatGPT-5 & Research Paper",
    "section": "6.4 Moses Madale Deliverables",
    "text": "6.4 Moses Madale Deliverables\nAI assistance: Claude Sonnet 4.5 was used to help me restructure the prompt to the AI Llama model which I originally developed myself based on the prompt that was given to the Conlingo RAG model so that it is more LLM friendly. ChatGPT 5.1 Was used to suggest the formula to derive the curved AI model scores from the survey of 50 Indian students from ORU (accessed Nov, 2025).\n\n6.4.1 Human Evaluation and Comparative Analysis\n\n6.4.1.1 Overview\nWeek 5 represented the culmination of the ConLingo 2.0 project: training the final combined model on all five datasets, designing a rigorous human evaluation survey, and determining whether supervised fine-tuning could surpass the existing RAG implementation and state-of-the-art models like ChatGPT 5.1 in capturing Indian cultural nuance. This week required not only technical execution but also careful experimental design to ensure valid, bias-minimized results from human evaluators with authentic Indian cultural expertise.\nPrimary Objectives:\n\nTrain Conlingo 2.0 (combined model with all 3,031 Q&A pairs)\nEngineer culturally appropriate prompts for fair model comparison\nDesign and deploy a Google Forms survey minimizing bias\nRecruit qualified participants with Indian cultural background\nAnalyze results using both raw and weighted scoring methods\nDraw conclusions about fine-tuning effectiveness for cultural awareness\n\n\n\n6.4.1.2 Conlingo 2.0: Training the Combined Model\n\n6.4.1.2.1 Dataset Integration Strategy\nConlingo 2.0 represented the team’s hypothesis that combining diverse Indian cultural data sources would produce a model with comprehensive cultural awareness. The model integrated all five approved datasets from Week 3:\n\n\n\n\n\n\n\n\n\nDataset\nExamples\nPercentage\nCultural Pillars Covered\n\n\n\n\nSuperstitions\n923\n30.5%\nValues & beliefs, norms & customs\n\n\nTED Talks\n596\n19.7%\nArts & literature, social organization\n\n\nYouTube Transcripts\n512\n16.9%\nLanguage, contemporary culture\n\n\nWikipedia\n500\n16.5%\nGovernment, artifacts & technology\n\n\nConstitution\n500\n16.5%\nGovernment, social organization\n\n\nTotal\n3,031\n100%\nAll 8 pillars\n\n\n\nRationale for Balanced Representation:\nThe distribution was not artificially balanced but reflected the natural availability of quality data: - Superstitions (30.5%) provided the richest source of everyday cultural beliefs - TED Talks and YouTube captured contemporary Indian voices and Hinglish usage - Wikipedia and Constitution grounded the model in factual, institutional knowledge\n\n\n6.4.1.2.2 Training Configuration\nConlingo 2.0 used identical LoRA hyperparameters to Week 4’s individual models for consistency:\n#| eval: false\n\n#!/usr/bin/env python3\n\"\"\"\nLoRA Fine-Tuning with ALL Indian Cultural Data\nCombines: YouTube, TED Talks, Wikipedia, Constitution, Superstitions\n\"\"\"\n\nimport torch\nimport json\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling\n)\nfrom peft import LoraConfig, get_peft_model\nfrom datasets import Dataset\nimport os\nfrom sklearn.model_selection import train_test_split\n\nprint(\"=\"*60)\nprint(\"Combined All-Data LoRA Fine-Tuning Pipeline\")\nprint(\"=\"*60)\n\n# Paths\nDATA_PATHS = {\n    \"youtube\": \"/home/mmadale/CSC463/conlingo/youtube_data/data/final_youtube_transcript_data.jsonl\",\n    \"ted_talks\": \"/home/mmadale/CSC463/conlingo/huggingface_data/indian_ted_talks/data/ted_talks_qa.jsonl\",\n    \"wikipedia\": \"/home/mmadale/CSC463/conlingo/huggingface_data/indian_wikipedia/data/wikipedia_qa.jsonl\",\n    \"constitution\": \"/home/mmadale/CSC463/conlingo/huggingface_data/indian_constitution/data/constitution_qa.jsonl\",\n    \"superstitions\": \"/home/mmadale/CSC463/conlingo/superstition_data/data/superstition_qa.jsonl\"\n}\n\nOUTPUT_DIR = \"/home/mmadale/CSC463/conlingo/models/combined-all-data\"\nMODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"\\n1. Loading and combining all datasets...\")\n\nall_examples = []\ndataset_stats = {}\n\nfor dataset_name, data_path in DATA_PATHS.items():\n    print(f\"\\n  Loading {dataset_name}...\")\n    \n    with open(data_path, 'r', encoding='utf-8') as f:\n        data = [json.loads(line) for line in f]\n    \n    count = 0\n    for item in data:\n        # Normalize to question/answer format\n        if \"instruction\" in item and \"response\" in item:\n            question = item[\"instruction\"]\n            answer = item[\"response\"]\n        elif \"question\" in item and \"answer\" in item:\n            question = item[\"question\"]\n            answer = item[\"answer\"]\n        else:\n            continue\n        \n        all_examples.append({\n            \"question\": question,\n            \"answer\": answer,\n            \"source\": dataset_name\n        })\n        count += 1\n    \n    dataset_stats[dataset_name] = count\n    print(f\"    Loaded {count} examples from {dataset_name}\")\n\nprint(f\"\\n  Dataset Statistics:\")\nfor dataset_name, count in dataset_stats.items():\n    percentage = (count / len(all_examples)) * 100\n    print(f\"    {dataset_name}: {count} examples ({percentage:.1f}%)\")\n\nprint(f\"\\n  Total combined examples: {len(all_examples)}\")\n\n# Split into train/validation (90/10)\ntrain_data, val_data = train_test_split(all_examples, test_size=0.1, random_state=42)\n\nprint(f\"\\n  Training examples: {len(train_data)}\")\nprint(f\"  Validation examples: {len(val_data)}\")\n\nprint(f\"\\n2. Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nprint(f\"  Tokenizer loaded: {tokenizer.__class__.__name__}\")\n\nprint(f\"\\n3. Preparing datasets...\")\n\ndef format_instruction(example):\n    \"\"\"Format question-answer pair for training\"\"\"\n    text = f\"### Question:\\n{example['question']}\\n\\n### Answer:\\n{example['answer']}\"\n    return text\n\ndef tokenize_function(example):\n    \"\"\"Tokenize examples with padding and truncation\"\"\"\n    text = format_instruction(example)\n    \n    tokenized = tokenizer(\n        text,\n        truncation=True,\n        max_length=512,\n        padding=\"max_length\",\n        return_tensors=None\n    )\n    \n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized\n\n# Convert to HuggingFace Dataset format\ntrain_dataset = Dataset.from_list(train_data)\nval_dataset = Dataset.from_list(val_data)\n\n# Tokenize\nprint(\"  Tokenizing training data...\")\ntrain_dataset = train_dataset.map(\n    tokenize_function,\n    remove_columns=train_dataset.column_names\n)\n\nprint(\"  Tokenizing validation data...\")\nval_dataset = val_dataset.map(\n    tokenize_function,\n    remove_columns=val_dataset.column_names\n)\n\nprint(f\"  Training dataset size: {len(train_dataset)}\")\nprint(f\"  Validation dataset size: {len(val_dataset)}\")\n\nprint(f\"\\n4. Loading base model...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\n\nmodel.gradient_checkpointing_enable()\n\nprint(f\"  Model loaded: {model.__class__.__name__}\")\nprint(f\"  Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\nprint(f\"\\n5. Configuring LoRA...\")\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\n\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\n\nprint(f\"  LoRA configured successfully\")\nprint(f\"  Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.4f}%)\")\nprint(f\"  Total parameters: {total_params:,}\")\n\nprint(f\"\\n6. Setting up training arguments...\")\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=16,\n    learning_rate=2e-4,\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=100,\n    logging_steps=10,\n    save_steps=100,\n    eval_steps=100,\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    fp16=True,\n    report_to=\"none\",\n    save_total_limit=2,\n    remove_unused_columns=False,\n    gradient_checkpointing=True\n)\n\nprint(\"  Training configuration:\")\nprint(f\"    Epochs: {training_args.num_train_epochs}\")\nprint(f\"    Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"    Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"    Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"    Learning rate: {training_args.learning_rate}\")\n\nprint(f\"\\n7. Initializing trainer...\")\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator\n)\n\nprint(\"  Trainer initialized successfully\")\n\nprint(f\"\\n8. Starting training...\")\nprint(\"=\"*60)\n\ntrainer.train()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Training complete\")\nprint(\"=\"*60)\n\nprint(f\"\\n9. Saving final model...\")\nmodel.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\ntokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n\nprint(f\"  Model saved to: {OUTPUT_DIR}/final_model\")\n\nprint(f\"\\n10. Final evaluation metrics:\")\neval_results = trainer.evaluate()\nfor key, value in eval_results.items():\n    print(f\"    {key}: {value:.4f}\")\n\n\n\n6.4.1.3 Training Results\nTraining Output:\n#| eval: false\n\n============================================================\nCombined All-Data LoRA Fine-Tuning Pipeline\n============================================================\n\n1. Loading and combining all datasets...\n\n  Loading youtube...\n    Loaded 512 examples from youtube\n\n  Loading ted_talks...\n    Loaded 596 examples from ted_talks\n\n  Loading wikipedia...\n    Loaded 500 examples from wikipedia\n\n  Loading constitution...\n    Loaded 500 examples from constitution\n\n  Loading superstitions...\n    Loaded 923 examples from superstitions\n\n  Dataset Statistics:\n    youtube: 512 examples (16.9%)\n    ted_talks: 596 examples (19.7%)\n    wikipedia: 500 examples (16.5%)\n    constitution: 500 examples (16.5%)\n    superstitions: 923 examples (30.5%)\n\n  Total combined examples: 3031\n\n  Training examples: 2727\n  Validation examples: 304\n\n2. Loading tokenizer...\n  Tokenizer loaded: PreTrainedTokenizerFast\n\n3. Preparing datasets...\n  Tokenizing training data...\n  Tokenizing validation data...\n  Training dataset size: 2727\n  Validation dataset size: 304\n\n4. Loading base model...\n  Model loaded: LlamaForCausalLM\n  Model parameters: 8,030,261,248\n\n5. Configuring LoRA...\n  LoRA configured successfully\n  Trainable parameters: 13,631,488 (0.1695%)\n  Total parameters: 8,043,892,736\n\n6. Setting up training arguments...\n  Training configuration:\n    Epochs: 3\n    Batch size: 2\n    Gradient accumulation: 16\n    Effective batch size: 32\n    Learning rate: 0.0002\n    Total training steps: ~255\n\n7. Initializing trainer...\n  Trainer initialized successfully\n\n8. Starting training...\n============================================================\n{'loss': 2.7157, 'grad_norm': 1.824, 'learning_rate': 1.8e-05, 'epoch': 0.12}\n{'loss': 2.5419, 'grad_norm': 1.431, 'learning_rate': 3.8e-05, 'epoch': 0.23}\n{'loss': 2.2675, 'grad_norm': 1.163, 'learning_rate': 5.8e-05, 'epoch': 0.35}\n{'loss': 1.9553, 'grad_norm': 1.023, 'learning_rate': 7.8e-05, 'epoch': 0.47}\n{'loss': 1.7885, 'grad_norm': 1.113, 'learning_rate': 9.8e-05, 'epoch': 0.59}\n{'loss': 1.7141, 'grad_norm': 0.752, 'learning_rate': 0.000118, 'epoch': 0.7}\n{'loss': 1.6481, 'grad_norm': 0.876, 'learning_rate': 0.000138, 'epoch': 0.82}\n{'loss': 1.6455, 'grad_norm': 0.952, 'learning_rate': 0.000158, 'epoch': 0.94}\n{'loss': 1.5384, 'grad_norm': 0.901, 'learning_rate': 0.000178, 'epoch': 1.05}\n{'loss': 1.5825, 'grad_norm': 0.930, 'learning_rate': 0.000198, 'epoch': 1.16}\n{'eval_loss': 1.6958, 'eval_runtime': 35.534, 'epoch': 1.16}\n{'loss': 1.4987, 'grad_norm': 0.943, 'learning_rate': 0.000198, 'epoch': 1.28}\n{'loss': 1.4778, 'grad_norm': 0.916, 'learning_rate': 0.000193, 'epoch': 1.4}\n{'loss': 1.524, 'grad_norm': 0.899, 'learning_rate': 0.000184, 'epoch': 1.52}\n{'loss': 1.5028, 'grad_norm': 0.906, 'learning_rate': 0.000171, 'epoch': 1.63}\n{'loss': 1.4994, 'grad_norm': 0.891, 'learning_rate': 0.000156, 'epoch': 1.75}\n{'loss': 1.4162, 'grad_norm': 0.891, 'learning_rate': 0.000139, 'epoch': 1.87}\n{'loss': 1.4549, 'grad_norm': 0.922, 'learning_rate': 0.00012, 'epoch': 1.99}\n{'loss': 1.3555, 'grad_norm': 0.972, 'learning_rate': 0.0001, 'epoch': 2.09}\n{'loss': 1.324, 'grad_norm': 1.070, 'learning_rate': 8.02e-05, 'epoch': 2.21}\n{'loss': 1.3459, 'grad_norm': 1.062, 'learning_rate': 6.13e-05, 'epoch': 2.33}\n{'eval_loss': 1.6180, 'eval_runtime': 35.519, 'epoch': 2.33}\n{'loss': 1.2716, 'grad_norm': 1.051, 'learning_rate': 4.38e-05, 'epoch': 2.45}\n{'loss': 1.2532, 'grad_norm': 1.110, 'learning_rate': 2.86e-05, 'epoch': 2.56}\n{'loss': 1.275, 'grad_norm': 1.073, 'learning_rate': 1.62e-05, 'epoch': 2.68}\n{'loss': 1.2469, 'grad_norm': 1.084, 'learning_rate': 7.05e-06, 'epoch': 2.8}\n{'loss': 1.313, 'grad_norm': 1.075, 'learning_rate': 1.6e-06, 'epoch': 2.91}\n{'train_runtime': 3055.3466, 'train_samples_per_second': 2.678, \n 'train_steps_per_second': 0.084, 'train_loss': 1.5971, 'epoch': 3.0}\n\n============================================================\nTraining complete\n============================================================\n\n9. Saving final model...\n  Model saved to: /home/mmadale/CSC463/conlingo/models/combined-all-data/final_model\n\n10. Final evaluation metrics:\n    eval_loss: 1.6180\n    eval_runtime: 35.5426\n    eval_samples_per_second: 8.5530\n    eval_steps_per_second: 4.2770\n    epoch: 3.0000\n\n============================================================\nFine-tuning pipeline complete\n============================================================\nSLURM Job Statistics:\n#| eval: false\n\nJobID        JobName       State      Elapsed\n141035       combined_+    COMPLETED  00:51:47\n141035.batch batch         COMPLETED  00:51:47\nPerformance Analysis:\nConlingo 2.0 achieved strong training metrics: - Loss reduction: 2.7157 → 1.313 (52% reduction) - Final evaluation loss: 1.6180 - Training duration: 51 minutes 47 seconds - Trainable parameters: 13,631,488 (0.17% of total) - Dataset scale: 6x larger than individual models (3,031 vs ~500 examples)\nThe evaluation loss of 1.6180 fell between Wikipedia’s excellent 1.2456 and YouTube’s higher 2.2825, suggesting successful integration of diverse content styles. The model learned patterns across encyclopedic facts, conversational transcripts, narrative presentations, legal documents, and cultural beliefs.\n\n\n\n6.4.2 Prompt Engineering for Fair Comparison\n\n6.4.2.1 Studying the RAG Implementation\nBefore generating model responses, Moses studied the RAG implementation’s system prompt to understand what made it effective at cultural contextualization. Key insights:\n\nExpertise framing: Positioned the model as a cultural anthropologist\nSpecific knowledge domains: Listed explicit areas of competence\nRegional awareness: Emphasized North/South/East/West variations\nInterfaith sensitivity: Balanced Hindu and Christian perspectives\nPractical orientation: Focused on actionable, real-world insights\n\n\n\n6.4.2.2 Engineered System Prompt\nMoses designed a comprehensive system prompt that would be supplied identically to both Conlingo 2.0 and ChatGPT 5.1:\n#| eval: false\n\nCULTURAL_SYSTEM_PROMPT = \"\"\"You are a cultural anthropologist and contextual researcher with deep experience studying values, beliefs, customs, and worldview formation across diverse Indian communities.\n\nYour expertise includes:\n- Core cultural values and virtues across Indian regions\n- Family and social structures in Indian society\n- Spiritual and religious norms (Hinduism, Christianity, Islam, Sikhism)\n- Cultural symbols, celebrations, and identity markers\n- Regional variations (North/South/East/West India)\n- Traditional vs modern worldview tensions\n- Caste dynamics and social hierarchies\n- Hindu-Christian dialogue and interfaith relations\n\nWhen answering questions about Indian culture and Christianity in India:\n1. Draw from your deep knowledge of Indian regional diversity, historical contexts, and contemporary practices\n2. Include specific examples from everyday life showing how values and beliefs manifest\n3. Acknowledge regional, religious, and generational variations\n4. Demonstrate sensitivity to both Hindu and Christian perspectives\n5. Focus on worldview - why people believe what they do, not just what they do\n6. Provide practical, actionable insights that show cultural logic\n\nResponse Guidelines:\n- Length: Approximately 100 words\n- Tone: Expert yet conversational, like explaining to someone unfamiliar with the region\n- Structure: Clear and well-organized with natural flow\n- Content: Highly specific to Indian cultural context with real examples\n- Avoid: Generic statements, Western-centric views, oversimplifications\n- Include: Regional nuances, historical context, modern tensions, specific practices\n\"\"\"\nDesign Rationale:\n\nAnthropologist Framing: Positions the responder as having studied Indian communities systematically\nEight Expertise Areas: Covers the cultural pillars identified in Week 3\nSix Response Principles: Guides the model toward nuanced, practical answers\nExplicit Response Guidelines: Constrains length (~100 words) for survey readability\nAvoidance List: Prevents generic, Western-centric, or oversimplified responses\n\n\n\n6.4.2.3 Question Selection\nThree questions were chosen from Rohan’s Week 4 evaluation set based on cultural coverage and sensitivity:\n#| eval: false\n\nQUESTIONS = [\n    \"What sensitivities should pastors consider when mentioning Hindu deities in Christmas homilies?\",\n    \"How can churches ensure caste-neutral seating and participation during worship?\",\n    \"Why might some Christians still use caste surnames, and how should this be discussed?\"\n]\nQuestion Characteristics:\n\n\n\n\n\n\n\n\nQuestion\nCultural Pillars Tested\nWhy Chosen\n\n\n\n\nQ1: Hindu deities in homilies\nValues & beliefs, norms & customs, language, religion & spirituality\nTests interfaith sensitivity and theological boundaries\n\n\nQ2: Caste-neutral worship\nNorms & customs, values & beliefs, social organization, religion & spirituality\nTests practical implementation of equity principles\n\n\nQ3: Caste surnames discussion\nSocial organization, values & beliefs, norms & customs, language\nTests nuanced understanding of identity and heritage\n\n\n\nAll three questions required: - Deep cultural knowledge beyond surface-level facts - Awareness of historical context and modern tensions - Practical wisdom for navigating sensitive topics - Regional and community-specific variations\n\n\n6.4.2.4 Model Response Generation\nProcess:\nAll model responses were generated in a single session to ensure consistency:\n\nRAG Implementation: Already deployed system used by the original ConLingo team\nConlingo 2.0: Loaded from /home/mmadale/CSC463/conlingo/models/combined-all-data/final_model using the same inference parameters as Week 4 (temperature=0.7, max_new_tokens=150)\nChatGPT 5.1: Accessed via web interface at chat.openai.com, with the system prompt provided in the user interface\n\nEach model received: - The identical system prompt - The identical three questions - No additional context or examples\nThis ensured the model itself was the only independent variable in the experiment.\n\n\n\n6.4.3 Survey Design and Implementation\n\n6.4.3.1 Design Objectives\nThe survey needed to:\n\nMinimize Bias: Prevent respondents from developing preferences for specific positions (Model A/B/C)\nEnsure Quality Participants: Filter for respondents with authentic Indian cultural expertise\nBalance Rigor and Accessibility: Collect sufficient data without exhausting participants\nEnable Weighted Scoring: Gather demographic data for exposure-based weighting\n\n\n\n6.4.3.2 Survey Structure\n\n\n\nSurvey design process showing model anonymization and data flow\n\n\nSection 1: Participant Quality Assurance\nFour screening questions characterized respondent expertise:\n\nAre you an Indian citizen? (Yes/No)\nHow many years have you lived in India? (Numeric input)\nWhich levels of your education did you complete in India? (Multi-select: Early education, High school, College/University, None)\nHow familiar are you with Christian traditions and interfaith relationships in India? (4-point scale: Not familiar, Slightly familiar, Somewhat familiar, Very familiar)\n\nRationale: These questions proxy for cultural exposure and competence to judge cultural nuance in responses. Someone who lived in India for 20 years, completed all education there, and is very familiar with interfaith dynamics is better positioned to evaluate cultural sensitivity than someone with limited exposure.\nSection 2: Model Evaluation\nFor each of the three questions, respondents saw: - The question text - Three anonymized responses labeled “Model A”, “Model B”, “Model C” - A single-choice question: “Which response reflects Indian cultural nuance more effectively?”\n\n\n6.4.3.3 Reducing Positional Bias\nA critical design challenge was preventing respondents from unconsciously favoring a specific position (e.g., always choosing Model A or gravitating toward the middle option).\nSolution: Model Mapping Rotation\n\n\n\nModel mapping strategy showing different assignments per question\n\n\nThe internal mapping between “Model A/B/C” labels and actual models was rotated across questions:\n\n\n\n\n\n\n\n\n\nQuestion\nModel A →\nModel B →\nModel C →\n\n\n\n\nQ1: Hindu deities in homilies\nRAG Implementation\nConlingo 2.0\nChatGPT 5.1\n\n\nQ2: Caste-neutral worship\nConlingo 2.0\nChatGPT 5.1\nRAG Implementation\n\n\nQ3: Caste surnames\nChatGPT 5.1\nRAG Implementation\nConlingo 2.0\n\n\n\nThis mapping was invisible to respondents but known to the analysis team. As a result: - A vote for “Model A” on Q1 counted toward RAG - A vote for “Model A” on Q2 counted toward Conlingo 2.0 - A vote for “Model A” on Q3 counted toward ChatGPT 5.1\nBenefit: If a respondent unconsciously preferred the first option, that preference would be distributed equally across all three models rather than systematically favoring one.\n\n\n\n6.4.4 Design Challenges and Solutions\nChallenge 1: Form Length\nParticipants have limited time and attention. How to collect necessary data without creating survey fatigue?\nSolution: - Limited to 3 evaluation questions (not 20) - Combined screening questions (4 items capturing citizenship, years, education, familiarity) - Estimated completion time: 5-7 minutes\nChallenge 2: Recruitment\nHow to reach authentic Indian cultural experts, especially those familiar with Indian Christian contexts?\nSolution: - Leveraged Oral Roberts University’s cultural diversity - Personal outreach to Indian students and staff - Posted in student groups and community channels - Asked friends at work to share the link - Direct recruitment when meeting Indian students on campus\nResult: 52 responses in 48 hours, with 86.5% Indian citizens.\n\n\n6.4.5 Data Collection and Demographics\n\n6.4.5.1 Survey Deployment\nTimeline: - Survey opened: Start of Week 5 evaluation period - Duration: 48 hours - Recruitment strategy: Multi-channel outreach\nOutreach Methods: 1. Personal contacts: Indian students and colleagues Moses knew directly 2. Student groups: Posted survey link in ORU student organizations 3. Social media: Shared in relevant online communities 4. Workplace connections: Asked friends to share with their networks 5. Campus encounters: Direct asks when meeting Indian students\n\n\n6.4.5.2 Response Summary\n\n\n\nGoogle Forms response summary showing 52 total responses\n\n\nTotal Responses: 52\nCitizenship Status: - Indian citizens: 86.5% (45 respondents) - Non-citizens: 13.5% (7 respondents)\nYears Lived in India: - Distribution varied from 0 to 25+ years - Median and mean values indicated substantial Indian exposure among respondents\nEducation Levels Completed in India: - Early education (Primary/Elementary) - High school - College/University - Many respondents checked multiple levels\nFamiliarity with Christian Traditions and Interfaith Relationships: - Very familiar: Highest proportion - Somewhat familiar: Second-highest - Slightly familiar: Present - Not familiar: Minimal\nImplications:\nThe participant pool represented a highly qualified evaluation cohort: - Over 86% held Indian citizenship - Most completed multiple education levels in India - Strong familiarity with Indian Christian and interfaith contexts\nThis demographic profile increased confidence that votes reflected authentic cultural expertise rather than uninformed guesses.\n\n\n\n6.4.6 Data Encoding and Analysis Methodology\n\n6.4.6.1 Raw Data Transformation\nThe exported Google Forms CSV contained mixed data types (Yes/No strings, numeric text, multi-select responses, model letter choices). Before analysis, Moses transformed this into a numeric-friendly format.\nKey Transformations:\n\nCitizenship: “Yes” → 1, “No” → 0\nYears in India: String → float (e.g., “20.0” → 20.0)\nEducation Count: Semi-colon separated text → integer count (e.g., “Early education;High School;College” → 3)\nFamiliarity Score: Text → numeric scale\n\n“Not familiar” → 1\n“Slightly familiar” → 2\n“Somewhat familiar” → 3\n“Very familiar” → 4\n\nModel Choice: Letter → model ID using the mapping table\n\n“Model A” on Q1 → 1 (RAG)\n“Model B” on Q1 → 2 (Conlingo 2.0)\n“Model C” on Q1 → 3 (ChatGPT 5.1)\n(Rotated for Q2 and Q3)\n\n\nOutput: data_transformed.csv with columns: - citizen_numeric - years_in_india - education_count - familiarity_score - q1_model_id, q2_model_id, q3_model_id\n\n\n6.4.6.2 Raw Vote Counting\nMethodology:\nFor each question and each model, count how many respondents selected that model:\nraw_votes[Q1][RAG] = number of respondents who selected Model A on Q1\nraw_votes[Q1][Conlingo 2.0] = number who selected Model B on Q1\nraw_votes[Q1][ChatGPT 5.1] = number who selected Model C on Q1\nSum across all three questions to get total raw votes per model.\n\n\n6.4.6.3 Weighted Scoring Formula\nRationale:\nNot all votes should count equally. A respondent who is an Indian citizen, lived in India for 20 years, completed all education there, and is very familiar with Indian Christian contexts has deeper cultural expertise than someone with minimal exposure. The weighted scoring formula gives more influence to highly qualified respondents.\nWeight Computation:\nFor each respondent:\n\nNormalize components (scale 0 to 1):\n\nyears_norm = min(years_in_india, 25) / 25 (cap at 25 to prevent extreme leverage)\neducation_norm = education_count / 3 (up to 3 levels)\nfamiliarity_norm = familiarity_score / 4 (scores 1-4 → 0.25-1.0)\n\nCombine into core exposure score:\n\ncore = 0.4 * years_norm + 0.3 * education_norm + 0.3 * familiarity_norm\nThis weights years most heavily (40%), with education and familiarity each at 30%\n\nApply citizenship boost:\n\ncitizen_factor = 1.2 if Indian citizen, else 1.0\nCitizens receive a 20% boost\n\nCalculate final weight:\n\nweight = citizen_factor * (0.2 + 0.8 * core)\nThe 0.2 base term ensures even low-exposure respondents contribute somewhat\nThe 0.8 scalar scales influence by exposure level\n\n\nWeight Range: - Minimum (non-citizen, no exposure): 0.2 - Maximum (citizen, 25+ years, all education, very familiar): 1.2\nCurved Score Calculation:\nFor each model, sum the weights of all respondents who voted for that model across all three questions:\ncurved_score[RAG] = sum of weights of all votes for RAG on Q1, Q2, Q3\ncurved_score[Conlingo 2.0] = sum of weights of all votes for Conlingo 2.0 on Q1, Q2, Q3\ncurved_score[ChatGPT 5.1] = sum of weights of all votes for ChatGPT 5.1 on Q1, Q2, Q3\n\n\n6.4.6.4 Analysis Script\nThe complete analysis was performed by analyze_votes.py:\n#| eval: false\n\nimport pandas as pd\nimport numpy as np\n\nINPUT_FILE = \"data_transformed.csv\"\n\n# Column names\nEDU_TEXT_COL = \"Which levels of your education did you complete in India? (Select all that apply)\"\nCITIZEN_COL = \"citizen_numeric\"\nYEARS_COL = \"years_in_india\"\nFAM_COL = \"familiarity_score\"\n\nQ1_COL = \"q1_model_id\"\nQ2_COL = \"q2_model_id\"\nQ3_COL = \"q3_model_id\"\n\n# Model mapping\nMODEL_NAMES = {\n    1: \"RAG implementation\",\n    2: \"Conlingo 2.0\",\n    3: \"GPT 5.1\"\n}\n\ndef compute_weight(row):\n    \"\"\"\n    Compute voter weight based on:\n    - citizenship\n    - years lived in India\n    - education levels completed in India\n    - familiarity with Christian/interfaith context\n    \"\"\"\n    citizen = row.get(CITIZEN_COL, np.nan)\n    years = row.get(YEARS_COL, np.nan)\n    edu_raw = row.get(EDU_TEXT_COL, \"\")\n    fam = row.get(FAM_COL, np.nan)\n\n    # Education: count selected levels\n    edu_count = 0\n    if isinstance(edu_raw, str) and edu_raw.strip():\n        text = edu_raw.replace(\",\", \";\")\n        parts = [p.strip() for p in text.split(\";\") if p.strip()]\n        edu_count = len(parts)\n\n    # Normalize components\n    if pd.isna(years):\n        years_norm = 0.0\n    else:\n        years_norm = max(0.0, min(float(years), 25.0)) / 25.0\n\n    if edu_count &gt; 0:\n        education_norm = edu_count / 3.0\n    else:\n        education_norm = 0.0\n\n    if pd.isna(fam):\n        familiarity_norm = 0.0\n    else:\n        familiarity_norm = float(fam) / 4.0\n\n    core = 0.4 * years_norm + 0.3 * education_norm + 0.3 * familiarity_norm\n    citizen_factor = 1.2 if citizen == 1 else 1.0\n\n    # Add base term for low-exposure participants\n    weight = citizen_factor * (0.2 + 0.8 * core)\n    return weight\n\ndef main():\n    df = pd.read_csv(INPUT_FILE)\n\n    # Compute per-respondent weight\n    df[\"weight\"] = df.apply(compute_weight, axis=1)\n\n    # Raw counts per model per question\n    questions = [Q1_COL, Q2_COL, Q3_COL]\n    raw_per_question = {q: {m: 0 for m in MODEL_NAMES.keys()} for q in questions}\n    raw_total = {m: 0 for m in MODEL_NAMES.keys()}\n\n    for _, row in df.iterrows():\n        for q in questions:\n            model_id = row[q]\n            if model_id in raw_per_question[q]:\n                raw_per_question[q][model_id] += 1\n                raw_total[model_id] += 1\n\n    # Curved scores per model\n    curved_total = {m: 0.0 for m in MODEL_NAMES.keys()}\n\n    for _, row in df.iterrows():\n        w = row[\"weight\"]\n        for q in questions:\n            model_id = row[q]\n            if model_id in curved_total:\n                curved_total[model_id] += w\n\n    # Print raw vote summary per question\n    print(\"Raw vote counts per question\")\n    print(\"-\" * 40)\n    for q in questions:\n        print(f\"{q}:\")\n        for m in sorted(MODEL_NAMES.keys()):\n            print(f\"  Model {m} ({MODEL_NAMES[m]}): {raw_per_question[q][m]}\")\n        print()\n\n    # Print overall summary table\n    print(\"Overall model performance (all questions combined)\")\n    print(\"-\" * 60)\n    header = f\"{'Model ID':&lt;8}  {'Model Name':&lt;22}  {'Raw Votes':&gt;10}  {'Curved Score':&gt;13}\"\n    print(header)\n    print(\"-\" * 60)\n    for m in sorted(MODEL_NAMES.keys()):\n        raw = raw_total[m]\n        curved = curved_total[m]\n        line = f\"{m:&lt;8}  {MODEL_NAMES[m]:&lt;22}  {raw:&gt;10d}  {curved:&gt;13.2f}\"\n        print(line)\n\nif __name__ == \"__main__\":\n    main()\n\n\n\n6.4.7 Results and Findings\n\n6.4.7.1 Raw Vote Results\n\n\n\nRaw popularity showing vote counts and percentages\n\n\nTotal Raw Votes (52 respondents × 3 questions = 156 total votes):\n\n\n\nModel\nRaw Votes\nPercentage\n\n\n\n\nRAG Implementation\n69\n46.00%\n\n\nChatGPT 5.1\n43\n28.67%\n\n\nConlingo 2.0\n38\n25.33%\n\n\n\nInterpretation:\nEven with equal weighting of all votes, the RAG implementation emerged as the clear leader, capturing nearly half of all votes. ChatGPT 5.1 secured second place, while Conlingo 2.0 came in third with just over one-quarter of votes.\nVote Distribution Pattern: - RAG received 81% more votes than Conlingo 2.0 - ChatGPT 5.1 received 13% more votes than Conlingo 2.0 - RAG received 60% more votes than ChatGPT 5.1\nThis raw result suggested that supervised fine-tuning on 3,031 Indian cultural Q&A pairs was insufficient to surpass either the RAG implementation or a state-of-the-art general-purpose model in human evaluators’ judgment of cultural nuance.\n\n\n6.4.7.2 Weighted Curved Scores\n\n\n\nCurved popularity showing weighted scores accounting for participant expertise\n\n\nOverall Model Performance (all questions combined):\n\n\n\nModel ID\nModel Name\nRaw Votes\nCurved Score\n\n\n\n\n1\nRAG implementation\n69\n61.14\n\n\n3\nGPT 5.1\n43\n37.03\n\n\n2\nConlingo 2.0\n38\n33.13\n\n\n\nInterpretation:\nWeighting votes by participant cultural expertise strengthened the RAG implementation’s lead while maintaining the same ranking. The gap between RAG and both competitors widened in the curved scoring:\nCurved Score Comparisons: - RAG scored 84% higher than Conlingo 2.0 (vs 81% higher in raw votes) - ChatGPT 5.1 scored 12% higher than Conlingo 2.0 (vs 13% higher in raw votes) - RAG scored 65% higher than ChatGPT 5.1 (vs 60% higher in raw votes)\nKey Finding: The most culturally qualified respondents—those with the highest years in India, most education there, strongest familiarity with interfaith contexts, and Indian citizenship—disproportionately preferred the RAG implementation. This validated that RAG’s superiority was not due to random chance or low-expertise voters but reflected genuine cultural depth recognized by expert evaluators.\n\n\n6.4.7.3 Comparative Analysis\nWhy RAG Won:\nThe RAG (Retrieval-Augmented Generation) implementation likely succeeded because:\n\nDynamic Knowledge Access: RAG retrieves relevant cultural documents at query time, ensuring responses draw from authentic source material\nVerbatim Cultural Terminology: Can quote or paraphrase exact phrasing from cultural texts\nBreadth of Sources: Access to larger corpus than the 3,031 fine-tuning examples\nReduced Hallucination: Grounded in retrieved passages rather than model memory\nMaintained Base Model Fluency: Leveraged GPT-4’s strong instruction-following while augmenting with cultural knowledge\n\nWhy ChatGPT 5.1 Outperformed Conlingo 2.0:\nDespite having no specialized Indian cultural training, ChatGPT 5.1 scored higher than the fine-tuned model:\n\nLarger Base Model: GPT-5.1 likely has significantly more parameters than LLaMA-3 8B\nExtensive Pre-training: Exposed to massive amounts of text including Indian content\nSuperior Instruction Following: Better at interpreting and responding to the cultural system prompt\nGeneral Cultural Knowledge: Pre-training captured substantial Indian cultural information\nPrompt Engineering Advantage: The carefully designed system prompt may have activated latent knowledge effectively\n\nWhy Conlingo 2.0 Underperformed:\nSeveral factors likely contributed to the fine-tuned model’s third-place finish:\n\nLimited Training Data: 3,031 examples, while substantial for SFT, may be insufficient for deep cultural nuance\nData Quality Variation: Gemini-generated Q&A pairs may have lacked the authenticity of human-authored cultural commentary\nModel Size Constraints: LLaMA-3 8B’s capacity may be too limited compared to much larger models\nLoRA Limitations: Training only 0.17% of parameters may not sufficiently embed cultural knowledge\nOverfitting Risk: Possible memorization of training examples without generalizable cultural reasoning\n\n\n\n\n6.4.8 Conclusion\nWeek 5 brought the ConLingo 2.0 project to a rigorous, well-designed conclusion. While the team’s hypothesis—that supervised fine-tuning on 3,031 culturally specific examples would surpass the RAG implementation—proved incorrect, the project delivered valuable scientific insights and technical achievements.\nFinal Results: - RAG implementation: 61.14 (winner) - ChatGPT 5.1: 37.03 (second place) - Conlingo 2.0: 33.13 (third place)\nKey Contributions: 1. Demonstrated RAG’s superiority over even frontier models for cultural nuance 2. Identified limitations of supervised fine-tuning for cultural knowledge 3. Developed rigorous evaluation methodology for cultural AI 4. Trained and deployed multiple models spanning diverse Indian cultural content 5. Generated 3,000+ Q&A pairs covering 8 cultural pillars\nMost Important Finding:\nThe RAG implementation’s 65% performance advantage over ChatGPT 5.1 validated the original ConLingo team’s architectural choice and provided strong evidence for continuing RAG-based development. While Conlingo 2.0 did not beat RAG, the project strengthened confidence in RAG as the right approach for cultural contextualization.\nMoses’s technical expertese throughout the project—from model selection and environment setup through dataset collection, AI-powered processing, model training, prompt engineering, and human evaluation—demonstrated the full lifecycle of applied AI research. The project’s scientific rigor, even in the face of negative results, exemplified best practices for evaluating cultural AI systems.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5: Compare Fine-tuned model with RAG implementation and ChatGPT-5 & Research Paper</span>"
    ]
  },
  {
    "objectID": "Future_Improvments.html",
    "href": "Future_Improvments.html",
    "title": "7  Future Improvements",
    "section": "",
    "text": "Through our research we learned that we had biases present in our research. The first bias was that we used humans to score the responses. Each person would bring their own biases to the respones of the model. Secondly, the weights used in the CSI score also added a bias of its own.\nDr. V advised us to tackle the bias of the CSI score by doing a multilinear regression model on our dataset to help us understand what weights we need to use to score the CSI score.\nWe had scores from Rohan and his mother. They both had scored the Original model, superstitions model, constitutions model and the youtube model. We then averaeged the scores between these models and used it as a dataset to feed into our multilinear regression model.\nThe below table shows the comparison between old and new weights for each of the different variables. \nThe formula to calculate the Percentage difference is ABS(B2 - C2) / ((B2 + C2) / 2) * 100\nThe graph of all the CSI scores of all the models with the original weights and the CSI scores of all the models using the weights from the regression model can be seen in the below.\n\n\n\nOld Weights Vs New weights graph\n\n\nThe datasets that were used to generate the results can be found in the Appendix section under the RohanAby folder. The ConlingoAllData.xlsx was used to train the model. The code used to generate the new weights is called MLRConlingo.R. AI assistance: ChatGPT was used to generate this code (accessed Dec 4, 2025). The files are Conlingo Average.xlsx, ConlingoAllData.xlsx, Beena_CSI grades.xlsx and the CSI grades_11_14_2025.xlsx.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Future Improvements</span>"
    ]
  },
  {
    "objectID": "Week5.html#suwilanji-mwaanza-deliverables",
    "href": "Week5.html#suwilanji-mwaanza-deliverables",
    "title": "6  Week 5: Compare Fine-tuned model with RAG implementation and ChatGPT-5 & Research Paper",
    "section": "6.5 Suwilanji Mwaanza Deliverables",
    "text": "6.5 Suwilanji Mwaanza Deliverables\n\n6.5.1 Overview of the Week 5 Deliverable\nThis week, my deliverable was to help distribute the survey to as many Indians as possible, with the goal of 50, so that we can obtain a less biased evaluation of our three models. Additionally, we are drafting our research project. Another aspect I investigated was why the RAG implementation seemed to perform better, as the survey responses indicated that the RAG implementation was in the lead.\n\n\n6.5.2 Week 5 Deliverables:\n\nWiden our evaluation to avoid bias.\n\nConduct a survey to gather people’s opinions and compare the results of multiple models.\n\nAdd deliverables 1-4 to the research paper.\nWrite the initial first draft of the Arixv Paper using the findings and visualizations that were carried out in Week 4\n(Bonus) Examine related work for RAG and identify why it outperforms the other models.\n\n\n\n6.5.3 What Was Accomplished:\n\n\n6.5.4 1. Widening our evaluation\nTo ensure we are not introducing bias, Moses and Will were tasked with creating a survey that would be distributed to many Indians to solicit their feedback on the model’s responses.\n\n\n\nTitan GPU environment setup confirmation\n\n\n\n\n6.5.5 2. Adding deliverables to the draft\nAdditionally, I worked on adding my deliverables 1 through 4 to the QMD files.\n\n\n6.5.6 3. Why RAG Outperforms SFT in Cultural Alignment (Bonus)\nAs the results began to come in regarding the survey, the RAG implementation had already seemed to be the model with the higher results. I decided to investigate similar research on culture in LLMs and whether their results could explain why the original model performed better than our supervised model.\nBased on the paper “Rethinking AI cultural alignment” by Bravansky, Trhlik, and Barez (2025), a detailed summary and analysis of the project is provided. I examined this project from the perspective of trying to understand why the RAG (Retrieval-Augmented Generation) model likely outperformed the SFT (Supervised Fine-Tuned) models in the context of Indian culture, followed by the necessary limitations that the paper states.\n\n\n6.5.7 The Case for Context: Why RAG Outperforms SFT in Cultural Alignment\nThe RAG model outperformed the SFT model in our human evaluation, which aligns with the theoretical framework proposed in this paper. While the paper focuses on interaction structures, its core thesis—that cultural alignment is dynamic and context-dependent, rather than static, is a starting point.\n\n\n6.5.8 1. Static vs. Dynamic Cultural Representation\nThe authors critique the prevailing method of cultural alignment, which relies on “embedding predefined cultural values” from standardized surveys (like the World Values Survey) into models. In ConLingo 2.0, the SFT models likely represent this “embedding” approach. By fine-tuning weights on specific Indian cultural datasets, the SFT models attempt to “freeze” cultural knowledge into the model’s parameters.\nHowever, the paper argues that culture is not a fixed repository of facts but a fluid system where values are “enacted in very different ways depending on outside context.”\n\nThe SFT Weakness: SFT models risk treating Indian culture—which is highly pluralistic and diverse—as a monolith or statically. If the model encounters a query that deviates slightly from its training distribution, it relies on static, compressed weights that may produce stereotyped or generic responses.\nThe RAG Advantage: RAG inherently treats knowledge in a more dynamic manner. By retrieving relevant context at inference time, the RAG model mimics the paper’s call for a “context-sensitive” approach. It allows the AI to “align” to the specific nuances of a query by accessing fresh, granular information rather than relying on a generalized “average” of Indian culture baked into its weights.\n\nIn this paper, the researchers did not necessarily implement the RAG or SFT methods; instead, they focused on interaction design and prompting strategies to test cultural alignment. Specifically, they used a case study with GPT-4o using three distinct interaction types:\n\nDirect Classification: Asking the model to classify a response directly.\nChain-of-Thought (CoT): Asking the model to provide reasoning before offering an answer.\nOpen-Ended Scenarios: Asking the model to write content (e.g., a news article or script) based on a specific prompt.\n\nThe reason I thought this paper was applicable was because of the cultural alignment theme throughout the paper, and the fact that they tested their processes on a GPT-4o model, which is exactly the model the RAG implementation uses, except the mini version for cost purposes, and lastly, the idea of cultural alignment being a bidirectional process for stronger results. The original ConLingo model implements this as it has a corpus of data to be accessed, and it is also heavily prompt-engineered.\nTheir core argument is that cultural alignment should be viewed as a “bidirectional process” shaped by how humans interact with the system, rather than just “embedding predefined cultural values” (which is what a retrieval or fine-tuning approach might rely on).\n\n\n6.5.9 2. The Power of “Interaction Structure” and Reasoning\nThe paper demonstrates how a model is prompted significantly changes its cultural alignment. Through their case study using GPT-4o, the authors found that Chain-of-Thought (CoT) prompting yielded significantly higher alignment scores (measured via Wasserstein Similarity) compared to direct classification.\nThis is crucial for explaining the RAG model’s success:\n\nRAG as a form of CoT: RAG systems fundamentally alter the interaction structure. They force the model to process retrieved evidence before generating an answer. This “retrieval \\(\\rightarrow\\) synthesis \\(\\rightarrow\\) generation” pipeline acts as a structural proxy for Chain-of-Thought reasoning.\nEvidence from the Paper (Table 1): The paper’s data specifically highlights India. In the Direct Classification setting, alignment with Indian values was relatively low (0.58). However, when the interaction structure changed to Chain-of-Thought (CoT), the alignment score improved to 0.63.\n\n\n\n\nTitan GPU environment setup confirmation\n\n\n\nImplication: Our RAG model likely outperformed SFT because the retrieval mechanism forced the model to “reason” through specific cultural evidence rather than reacting reflexively (which SFT models often do). The paper demonstrates that providing the model “space” to process context (which RAG does by design) enhances cultural mimicry.\n\n\n\n6.5.10 3. Bidirectional Alignment vs. Imposed Values\nThe authors propose reframing alignment as a “bidirectional process.” They argue that we should not merely impose standardized values on AIs (which SFT does), but rather query values relevant to specific systems through interaction.\nThe RAG model likely succeeded because it operates closer to this bidirectional ideal. When a user asks a question about Indian culture, the RAG system retrieves context-specific information for that interaction. It constructs a cultural framework ad hoc for that specific query. In contrast, the SFT model attempts to force the query to fit into its pre-learned map of Indian culture. As the paper notes, “values that appear nominally identical may be enacted in very different ways depending on outside context”—RAG captures this context; SFT may struggle to.\n\n\n6.5.11 Limitations of the Research\nWhile this paper provides a strong theoretical basis for our results, it is essential to acknowledge the limitations explicitly stated by the authors, which may affect the extent to which we can apply their findings to our specific RAG architecture.\n\nScope of Cultural Theory: The paper acknowledges that it focuses on a “single cultural theory” and relies on specific datasets (GlobalOpinionQA) that privilege binary choices and specific survey-style questions. Indian culture is high-context and often non-binary; the paper’s reliance on quantifiable “distance” metrics (Wasserstein scores) might miss the qualitative nuance that our human evaluators picked up on in the SFT survey.\nModel Specificity: The case study was conducted solely using GPT-4o. The authors admit their work is constrained by this focus. Our SFT models are based on a smaller open-source model (Llama 3); the specific “interaction effects” observed in GPT-4o might not scale linearly. Smaller models might struggle more with the “reasoning” that CoT or RAG requires compared to a frontier model like GPT-4o.\nTask Limitation: The paper experimented with three specific interaction types: classification, CoT, and scenario writing. They did not explicitly test RAG architectures. While we can infer that RAG aligns with their “context-sensitive” conclusions, the paper does not provide empirical data directly comparing Retrieval-based methods against Fine-tuning methods.\nThe “Unclassifiable” Problem: The paper notes that as interaction complexity increases (e.g., in “Scenarios”), the percentage of “unclassifiable outputs” rises significantly (up to 32.27% for India, as seen in Table 1). This suggests that while complex interactions (like RAG) can provide better alignment, they also introduce higher variance and potential instability in the output, which simplistic metrics might fail to capture but human evaluators would notice.\n\n\n\n6.5.12 Summary\nThe paper supports the conclusion that your RAG model won because it treats Indian culture as a context-dependent, reasoning-heavy task rather than a static knowledge-recall task. By avoiding the “imperfect proxies” of baked-in weights (SFT), your RAG system aligned better with the paper’s definition of culture as a fluid, bidirectional phenomenon.\n\n\n6.5.13 AI assistance:\n\n“AI assistance: Copilot was used to summarize the arXiv paper and provide the summary in key insights for the ‘Cultural Alignment’ bonus section.”",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5: Compare Fine-tuned model with RAG implementation and ChatGPT-5 & Research Paper</span>"
    ]
  },
  {
    "objectID": "Week1.html#next-steps",
    "href": "Week1.html#next-steps",
    "title": "2  Week 1: Foundations & Data Collection",
    "section": "2.5 Next Steps:",
    "text": "2.5 Next Steps:\n\n2.5.1 For Week 2\n\nExpand the dataset from 500 to 3,000 examples.\n\nPerform deeper cleaning, annotation, and metadata tagging.\n\nBegin converting all examples into an instruction–response format.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: Foundations & Data Collection</span>"
    ]
  },
  {
    "objectID": "Week1.html#references",
    "href": "Week1.html#references",
    "title": "2  Week 1: Foundations & Data Collection",
    "section": "2.5 References:",
    "text": "2.5 References:\nAI4Bharat. (2024). Sangraha: Large-scale Indic language corpus. https://huggingface.co/datasets/ai4bharat/sangraha\nAI4Bharat. (2024). IndicBERT and IndicBART models. https://ai4bharat.iitm.ac.in/\nHoward Community College. (n.d.). The elements of culture. https://pressbooks.howardcc.edu/soci101/chapter/3-2-the-elements-of-culture/\nPew Research Center. (2021). Religion in India: Tolerance and segregation. https://www.pewresearch.org\nSahgal, N., & Evans, J. (2021). India Survey Dataset [Data set]. Pew Research Center. https://www.pewresearch.org/dataset/india-survey-dataset/\nWorld Values Survey. (2022). India dataset documentation. https://www.worldvaluessurvey.org/WVSDocumentationWV7.jsp\nCommon Crawl. (n.d.). Overview [Web site]. Retrieved November 30, 2025, from https://commoncrawl.org/overview\nNaamapadam Dataset. (2023). Named entity annotated dataset for Indic languages. AI4Bharat.\nIndicGLUE Benchmark. (2023). Benchmark tasks for Indic languages. AI4Bharat.\n“AI assistance: ChatGPT was used to organize and summarize my Word doc of the work I did to put on my QMD for week 1.”\n“AI assistance: ChatGPT was used to organize my data sources into the respective cultural buckets.”",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: Foundations & Data Collection</span>"
    ]
  }
]