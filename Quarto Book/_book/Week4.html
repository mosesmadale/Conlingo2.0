<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>5&nbsp; Week 4: Data Searching, Data Vetting, Dataset Collection, Data Cleaning – ConLingo Quarto Book</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Week5.html" rel="next">
<link href="./Week3.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-485d01fc63b59abcd3ee1bf1e8e2748d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./Week4.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Week 4: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">ConLingo Quarto Book</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">ConLingo 2.0</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Week1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Week 1: Foundations &amp; Data Collection</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Week2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Week 2: Data Cleaning &amp; Pipeline Setup</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Week3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Week4.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Week 4: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Week5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Week 5: Compare Fine-tuned model with RAG implementation and ChatGPT-5 &amp; Research Paper</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Future_Improvments.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Future Improvements</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview"><span class="header-section-number">5.1</span> Overview</a></li>
  <li><a href="#rohan-aby-deliverables" id="toc-rohan-aby-deliverables" class="nav-link" data-scroll-target="#rohan-aby-deliverables"><span class="header-section-number">5.2</span> Rohan Aby Deliverables</a>
  <ul class="collapse">
  <li><a href="#csi-scoring-of-superstitions-and-constitution-dataset" id="toc-csi-scoring-of-superstitions-and-constitution-dataset" class="nav-link" data-scroll-target="#csi-scoring-of-superstitions-and-constitution-dataset"><span class="header-section-number">5.2.1</span> CSI Scoring of superstitions and constitution dataset</a></li>
  </ul></li>
  <li><a href="#william-richards-deliverables" id="toc-william-richards-deliverables" class="nav-link" data-scroll-target="#william-richards-deliverables"><span class="header-section-number">5.3</span> William Richards Deliverables</a>
  <ul class="collapse">
  <li><a href="#individual-fine-tuned-model-testing" id="toc-individual-fine-tuned-model-testing" class="nav-link" data-scroll-target="#individual-fine-tuned-model-testing"><span class="header-section-number">5.3.1</span> Individual Fine-Tuned Model Testing</a></li>
  <li><a href="#question-testing" id="toc-question-testing" class="nav-link" data-scroll-target="#question-testing"><span class="header-section-number">5.3.2</span> Question Testing</a></li>
  <li><a href="#high-five-testing" id="toc-high-five-testing" class="nav-link" data-scroll-target="#high-five-testing"><span class="header-section-number">5.3.3</span> High Five Testing</a></li>
  </ul></li>
  <li><a href="#suwilanji-mwanza-deliverables" id="toc-suwilanji-mwanza-deliverables" class="nav-link" data-scroll-target="#suwilanji-mwanza-deliverables"><span class="header-section-number">5.4</span> Suwilanji Mwanza Deliverables</a>
  <ul class="collapse">
  <li><a href="#overview-of-the-week-4-deliverable" id="toc-overview-of-the-week-4-deliverable" class="nav-link" data-scroll-target="#overview-of-the-week-4-deliverable"><span class="header-section-number">5.4.1</span> Overview of the Week 4 Deliverable</a></li>
  <li><a href="#week-4-deliverables" id="toc-week-4-deliverables" class="nav-link" data-scroll-target="#week-4-deliverables"><span class="header-section-number">5.4.2</span> Week 4 Deliverables:</a></li>
  <li><a href="#what-was-accomplished" id="toc-what-was-accomplished" class="nav-link" data-scroll-target="#what-was-accomplished"><span class="header-section-number">5.4.3</span> What Was Accomplished:</a></li>
  <li><a href="#model-visualizations" id="toc-model-visualizations" class="nav-link" data-scroll-target="#model-visualizations"><span class="header-section-number">5.4.4</span> 1. Model Visualizations</a></li>
  <li><a href="#negative-performing-models" id="toc-negative-performing-models" class="nav-link" data-scroll-target="#negative-performing-models"><span class="header-section-number">5.4.5</span> 2. Negative Performing Models</a></li>
  </ul></li>
  <li><a href="#moses-madale-deliverables" id="toc-moses-madale-deliverables" class="nav-link" data-scroll-target="#moses-madale-deliverables"><span class="header-section-number">5.5</span> Moses Madale Deliverables</a>
  <ul class="collapse">
  <li><a href="#model-training-and-evaluation" id="toc-model-training-and-evaluation" class="nav-link" data-scroll-target="#model-training-and-evaluation"><span class="header-section-number">5.5.1</span> Model Training and Evaluation</a></li>
  <li><a href="#model-training-results" id="toc-model-training-results" class="nav-link" data-scroll-target="#model-training-results"><span class="header-section-number">5.5.2</span> Model Training Results</a></li>
  <li><a href="#prompt-engineering-for-inference" id="toc-prompt-engineering-for-inference" class="nav-link" data-scroll-target="#prompt-engineering-for-inference"><span class="header-section-number">5.5.3</span> Prompt Engineering for Inference</a></li>
  <li><a href="#test-question-design-and-evaluation-process" id="toc-test-question-design-and-evaluation-process" class="nav-link" data-scroll-target="#test-question-design-and-evaluation-process"><span class="header-section-number">5.5.4</span> Test Question Design and Evaluation Process</a></li>
  <li><a href="#baseline-model-testing" id="toc-baseline-model-testing" class="nav-link" data-scroll-target="#baseline-model-testing"><span class="header-section-number">5.5.5</span> Baseline Model Testing</a></li>
  <li><a href="#combined-model-strategy-and-deferral" id="toc-combined-model-strategy-and-deferral" class="nav-link" data-scroll-target="#combined-model-strategy-and-deferral"><span class="header-section-number">5.5.6</span> Combined Model Strategy and Deferral</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Week 4: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="overview" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="overview"><span class="header-section-number">5.1</span> Overview</h2>
<p>In Week 4, the team fine-tuned multiple models on Titan using various datasets, tested them with 20 selected questions, and analyzed performance across metrics like tone, empathy, and accuracy. Rohan led the performance analysis, while Moses and William handled model training, and Suwilanji created visualizations, investigated dataset issues, and began drafting the research paper and Week 4 presentation.</p>
</section>
<section id="rohan-aby-deliverables" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="rohan-aby-deliverables"><span class="header-section-number">5.2</span> Rohan Aby Deliverables</h2>
<section id="csi-scoring-of-superstitions-and-constitution-dataset" class="level3" data-number="5.2.1">
<h3 data-number="5.2.1" class="anchored" data-anchor-id="csi-scoring-of-superstitions-and-constitution-dataset"><span class="header-section-number">5.2.1</span> CSI Scoring of superstitions and constitution dataset</h3>
<p>Rohan was tasked with scoring the output from the superstitions and constitution dataset. The same twenty questions that were used for the other models were selected. The results showed us that the constitution model performed better. It had a score of 92%. The superstitions had a score of 79%. This is contrary to what we expected as we felt that the data that was used to train the superstitions model was more reliable than the constitution dataset. The results can be seen in the image below. <img src="Appendix/RohanAby/Week4RohanAby.png" class="img-fluid" alt="superstitions vs constitution model"></p>
</section>
</section>
<section id="william-richards-deliverables" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="william-richards-deliverables"><span class="header-section-number">5.3</span> William Richards Deliverables</h2>
<section id="individual-fine-tuned-model-testing" class="level3" data-number="5.3.1">
<h3 data-number="5.3.1" class="anchored" data-anchor-id="individual-fine-tuned-model-testing"><span class="header-section-number">5.3.1</span> Individual Fine-Tuned Model Testing</h3>
<p>Wills tasks for this deliverable was to alter Moses’ fine-tuning program for the Superstition and Constitution datasets to create fine-tuned models for each dataset:</p>
<section id="superstitions-dataset" class="level4" data-number="5.3.1.1">
<h4 data-number="5.3.1.1" class="anchored" data-anchor-id="superstitions-dataset"><span class="header-section-number">5.3.1.1</span> Superstitions Dataset:</h4>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Superstitions Fine-Tuning Model Bash Script</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --job-name=super_finetune</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --partition=gpu</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --nodes=1</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --ntasks-per-node=1</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --cpus-per-task=8</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --mem=64G</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --gres=gpu:1</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --time=6:00:00</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --output=logs/super_finetune_%j.out</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --error=logs/super_finetune_%j.err</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --mail-type=</span><span class="re">END</span><span class="co">,FAIL</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --mail-user=starwill16@gmail.com</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="ex">module</span> purge</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="ex">module</span> load Python/3.12.3-GCCcore-13.3.0</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="ex">module</span> load CUDA/12.3.0</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="bu">source</span> /home/gcsc563_01/CSC463/conlingo/llama_env/bin/activate</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">HF_HOME</span><span class="op">=</span>/home/gcsc563_01/.cache/huggingface</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">HUGGINGFACE_HUB_CACHE</span><span class="op">=</span>/home/gcsc563_01/.cache/huggingface</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> /home/gcsc563_01/conlingo</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> scripts/training/finetune_indian_superstitions.py</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="ex">deactivate</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Superstitions Fine-Tuning Model Python Script</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co">#!/usr/bin/env python3</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co">LoRA Fine-Tuning for the Indian Superstitions Data</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co">Fine-tunes LLaMA-3 8B on Indian Superstitions Q&amp;A pairs</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> (</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    AutoTokenizer,</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    AutoModelForCausalLM,</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    TrainingArguments,</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    Trainer,</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    DataCollatorForLanguageModeling</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> peft <span class="im">import</span> LoraConfig, get_peft_model</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> Dataset</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Superstitions LoRA Fine-Tuning Pipeline"</span>)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Paths</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>DATA_PATH <span class="op">=</span> <span class="st">"/home/gcsc563_01/conlingo/data/raw/superstition_qa.jsonl"</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>OUTPUT_DIR <span class="op">=</span> <span class="st">"/home/gcsc563_01/conlingo/models/finetuned/superstition"</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>MODEL_NAME <span class="op">=</span> <span class="st">"meta-llama/Meta-Llama-3-8B-Instruct"</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Create output directory</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>os.makedirs(OUTPUT_DIR, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">1. Loading data from: </span><span class="sc">{</span>DATA_PATH<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Load JSONL data</span></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> []</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(DATA_PATH, <span class="st">'r'</span>, encoding<span class="op">=</span><span class="st">'utf-8'</span>) <span class="im">as</span> f:</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> line <span class="kw">in</span> f:</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>        data.append(json.loads(line))</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total examples: </span><span class="sc">{</span><span class="bu">len</span>(data)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract question-answer pairs</span></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>examples <span class="op">=</span> []</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> item <span class="kw">in</span> data:</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>    examples.append({</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>        <span class="st">"question"</span>: item[<span class="st">"question"</span>],</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>        <span class="st">"answer"</span>: item[<span class="st">"answer"</span>]</span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Split into train/validation (90/10)</span></span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>train_data, val_data <span class="op">=</span> train_test_split(examples, test_size<span class="op">=</span><span class="fl">0.1</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training examples: </span><span class="sc">{</span><span class="bu">len</span>(train_data)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Validation examples: </span><span class="sc">{</span><span class="bu">len</span>(val_data)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">2. Loading tokenizer..."</span>)</span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(MODEL_NAME)</span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>tokenizer.pad_token <span class="op">=</span> tokenizer.eos_token</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a>tokenizer.padding_side <span class="op">=</span> <span class="st">"right"</span></span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Tokenizer loaded: </span><span class="sc">{</span>tokenizer<span class="sc">.</span><span class="va">__class__</span><span class="sc">.</span><span class="va">__name__</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">3. Preparing datasets..."</span>)</span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> format_instruction(example):</span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Format question-answer pair for training"""</span></span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> <span class="ss">f"### Question:</span><span class="ch">\n</span><span class="sc">{</span>example[<span class="st">'question'</span>]<span class="sc">}</span><span class="ch">\n\n</span><span class="ss">### Answer:</span><span class="ch">\n</span><span class="sc">{</span>example[<span class="st">'answer'</span>]<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> text</span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize_function(example):</span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Tokenize examples with padding and truncation"""</span></span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> format_instruction(example)</span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a>    tokenized <span class="op">=</span> tokenizer(</span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a>        text,</span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a>        truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a>        max_length<span class="op">=</span><span class="dv">512</span>,</span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a>        padding<span class="op">=</span><span class="st">"max_length"</span>,</span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a>        return_tensors<span class="op">=</span><span class="va">None</span></span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a>    tokenized[<span class="st">"labels"</span>] <span class="op">=</span> tokenized[<span class="st">"input_ids"</span>].copy()</span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokenized</span>
<span id="cb2-87"><a href="#cb2-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-88"><a href="#cb2-88" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to HuggingFace Dataset format</span></span>
<span id="cb2-89"><a href="#cb2-89" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> Dataset.from_list(train_data)</span>
<span id="cb2-90"><a href="#cb2-90" aria-hidden="true" tabindex="-1"></a>val_dataset <span class="op">=</span> Dataset.from_list(val_data)</span>
<span id="cb2-91"><a href="#cb2-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-92"><a href="#cb2-92" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenize</span></span>
<span id="cb2-93"><a href="#cb2-93" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Tokenizing training data..."</span>)</span>
<span id="cb2-94"><a href="#cb2-94" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> train_dataset.<span class="bu">map</span>(</span>
<span id="cb2-95"><a href="#cb2-95" aria-hidden="true" tabindex="-1"></a>    tokenize_function,</span>
<span id="cb2-96"><a href="#cb2-96" aria-hidden="true" tabindex="-1"></a>    remove_columns<span class="op">=</span>train_dataset.column_names</span>
<span id="cb2-97"><a href="#cb2-97" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-98"><a href="#cb2-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-99"><a href="#cb2-99" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Tokenizing validation data..."</span>)</span>
<span id="cb2-100"><a href="#cb2-100" aria-hidden="true" tabindex="-1"></a>val_dataset <span class="op">=</span> val_dataset.<span class="bu">map</span>(</span>
<span id="cb2-101"><a href="#cb2-101" aria-hidden="true" tabindex="-1"></a>    tokenize_function,</span>
<span id="cb2-102"><a href="#cb2-102" aria-hidden="true" tabindex="-1"></a>    remove_columns<span class="op">=</span>val_dataset.column_names</span>
<span id="cb2-103"><a href="#cb2-103" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-104"><a href="#cb2-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-105"><a href="#cb2-105" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training dataset size: </span><span class="sc">{</span><span class="bu">len</span>(train_dataset)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-106"><a href="#cb2-106" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Validation dataset size: </span><span class="sc">{</span><span class="bu">len</span>(val_dataset)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-107"><a href="#cb2-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-108"><a href="#cb2-108" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">4. Loading base model..."</span>)</span>
<span id="cb2-109"><a href="#cb2-109" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb2-110"><a href="#cb2-110" aria-hidden="true" tabindex="-1"></a>    MODEL_NAME,</span>
<span id="cb2-111"><a href="#cb2-111" aria-hidden="true" tabindex="-1"></a>    device_map<span class="op">=</span><span class="st">"auto"</span>,</span>
<span id="cb2-112"><a href="#cb2-112" aria-hidden="true" tabindex="-1"></a>    torch_dtype<span class="op">=</span>torch.float16,</span>
<span id="cb2-113"><a href="#cb2-113" aria-hidden="true" tabindex="-1"></a>    low_cpu_mem_usage<span class="op">=</span><span class="va">True</span></span>
<span id="cb2-114"><a href="#cb2-114" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-115"><a href="#cb2-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-116"><a href="#cb2-116" aria-hidden="true" tabindex="-1"></a>model.gradient_checkpointing_enable()</span>
<span id="cb2-117"><a href="#cb2-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-118"><a href="#cb2-118" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model loaded: </span><span class="sc">{</span>model<span class="sc">.</span><span class="va">__class__</span><span class="sc">.</span><span class="va">__name__</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-119"><a href="#cb2-119" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model parameters: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters())<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb2-120"><a href="#cb2-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-121"><a href="#cb2-121" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">5. Configuring LoRA..."</span>)</span>
<span id="cb2-122"><a href="#cb2-122" aria-hidden="true" tabindex="-1"></a>lora_config <span class="op">=</span> LoraConfig(</span>
<span id="cb2-123"><a href="#cb2-123" aria-hidden="true" tabindex="-1"></a>    r<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb2-124"><a href="#cb2-124" aria-hidden="true" tabindex="-1"></a>    lora_alpha<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb2-125"><a href="#cb2-125" aria-hidden="true" tabindex="-1"></a>    target_modules<span class="op">=</span>[<span class="st">"q_proj"</span>, <span class="st">"v_proj"</span>, <span class="st">"k_proj"</span>, <span class="st">"o_proj"</span>],</span>
<span id="cb2-126"><a href="#cb2-126" aria-hidden="true" tabindex="-1"></a>    lora_dropout<span class="op">=</span><span class="fl">0.05</span>,</span>
<span id="cb2-127"><a href="#cb2-127" aria-hidden="true" tabindex="-1"></a>    bias<span class="op">=</span><span class="st">"none"</span>,</span>
<span id="cb2-128"><a href="#cb2-128" aria-hidden="true" tabindex="-1"></a>    task_type<span class="op">=</span><span class="st">"CAUSAL_LM"</span></span>
<span id="cb2-129"><a href="#cb2-129" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-130"><a href="#cb2-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-131"><a href="#cb2-131" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> get_peft_model(model, lora_config)</span>
<span id="cb2-132"><a href="#cb2-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-133"><a href="#cb2-133" aria-hidden="true" tabindex="-1"></a>trainable_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters() <span class="cf">if</span> p.requires_grad)</span>
<span id="cb2-134"><a href="#cb2-134" aria-hidden="true" tabindex="-1"></a>total_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters())</span>
<span id="cb2-135"><a href="#cb2-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-136"><a href="#cb2-136" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"LoRA configured successfully"</span>)</span>
<span id="cb2-137"><a href="#cb2-137" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Trainable parameters: </span><span class="sc">{</span>trainable_params<span class="sc">:,}</span><span class="ss"> (</span><span class="sc">{</span><span class="dv">100</span><span class="op">*</span>trainable_params<span class="op">/</span>total_params<span class="sc">:.4f}</span><span class="ss">%)"</span>)</span>
<span id="cb2-138"><a href="#cb2-138" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total parameters: </span><span class="sc">{</span>total_params<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb2-139"><a href="#cb2-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-140"><a href="#cb2-140" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">6. Setting up training arguments..."</span>)</span>
<span id="cb2-141"><a href="#cb2-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-142"><a href="#cb2-142" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments(</span>
<span id="cb2-143"><a href="#cb2-143" aria-hidden="true" tabindex="-1"></a>    output_dir<span class="op">=</span>OUTPUT_DIR,</span>
<span id="cb2-144"><a href="#cb2-144" aria-hidden="true" tabindex="-1"></a>    num_train_epochs<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb2-145"><a href="#cb2-145" aria-hidden="true" tabindex="-1"></a>    per_device_train_batch_size<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb2-146"><a href="#cb2-146" aria-hidden="true" tabindex="-1"></a>    per_device_eval_batch_size<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb2-147"><a href="#cb2-147" aria-hidden="true" tabindex="-1"></a>    gradient_accumulation_steps<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb2-148"><a href="#cb2-148" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">2e-4</span>,</span>
<span id="cb2-149"><a href="#cb2-149" aria-hidden="true" tabindex="-1"></a>    lr_scheduler_type<span class="op">=</span><span class="st">"cosine"</span>,</span>
<span id="cb2-150"><a href="#cb2-150" aria-hidden="true" tabindex="-1"></a>    warmup_steps<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb2-151"><a href="#cb2-151" aria-hidden="true" tabindex="-1"></a>    logging_steps<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb2-152"><a href="#cb2-152" aria-hidden="true" tabindex="-1"></a>    save_steps<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb2-153"><a href="#cb2-153" aria-hidden="true" tabindex="-1"></a>    eval_steps<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb2-154"><a href="#cb2-154" aria-hidden="true" tabindex="-1"></a>    eval_strategy<span class="op">=</span><span class="st">"steps"</span>,</span>
<span id="cb2-155"><a href="#cb2-155" aria-hidden="true" tabindex="-1"></a>    save_strategy<span class="op">=</span><span class="st">"steps"</span>,</span>
<span id="cb2-156"><a href="#cb2-156" aria-hidden="true" tabindex="-1"></a>    load_best_model_at_end<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb2-157"><a href="#cb2-157" aria-hidden="true" tabindex="-1"></a>    metric_for_best_model<span class="op">=</span><span class="st">"eval_loss"</span>,</span>
<span id="cb2-158"><a href="#cb2-158" aria-hidden="true" tabindex="-1"></a>    greater_is_better<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb2-159"><a href="#cb2-159" aria-hidden="true" tabindex="-1"></a>    fp16<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb2-160"><a href="#cb2-160" aria-hidden="true" tabindex="-1"></a>    report_to<span class="op">=</span><span class="st">"none"</span>,</span>
<span id="cb2-161"><a href="#cb2-161" aria-hidden="true" tabindex="-1"></a>    save_total_limit<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb2-162"><a href="#cb2-162" aria-hidden="true" tabindex="-1"></a>    remove_unused_columns<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb2-163"><a href="#cb2-163" aria-hidden="true" tabindex="-1"></a>    gradient_checkpointing<span class="op">=</span><span class="va">True</span></span>
<span id="cb2-164"><a href="#cb2-164" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-165"><a href="#cb2-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-166"><a href="#cb2-166" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training configuration:"</span>)</span>
<span id="cb2-167"><a href="#cb2-167" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Epochs: </span><span class="sc">{</span>training_args<span class="sc">.</span>num_train_epochs<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-168"><a href="#cb2-168" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Batch size: </span><span class="sc">{</span>training_args<span class="sc">.</span>per_device_train_batch_size<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-169"><a href="#cb2-169" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Gradient accumulation: </span><span class="sc">{</span>training_args<span class="sc">.</span>gradient_accumulation_steps<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-170"><a href="#cb2-170" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Effective batch size: </span><span class="sc">{</span>training_args<span class="sc">.</span>per_device_train_batch_size <span class="op">*</span> training_args<span class="sc">.</span>gradient_accumulation_steps<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-171"><a href="#cb2-171" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Learning rate: </span><span class="sc">{</span>training_args<span class="sc">.</span>learning_rate<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-172"><a href="#cb2-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-173"><a href="#cb2-173" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">7. Initializing trainer..."</span>)</span>
<span id="cb2-174"><a href="#cb2-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-175"><a href="#cb2-175" aria-hidden="true" tabindex="-1"></a>data_collator <span class="op">=</span> DataCollatorForLanguageModeling(</span>
<span id="cb2-176"><a href="#cb2-176" aria-hidden="true" tabindex="-1"></a>    tokenizer<span class="op">=</span>tokenizer,</span>
<span id="cb2-177"><a href="#cb2-177" aria-hidden="true" tabindex="-1"></a>    mlm<span class="op">=</span><span class="va">False</span></span>
<span id="cb2-178"><a href="#cb2-178" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-179"><a href="#cb2-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-180"><a href="#cb2-180" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(</span>
<span id="cb2-181"><a href="#cb2-181" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model,</span>
<span id="cb2-182"><a href="#cb2-182" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>training_args,</span>
<span id="cb2-183"><a href="#cb2-183" aria-hidden="true" tabindex="-1"></a>    train_dataset<span class="op">=</span>train_dataset,</span>
<span id="cb2-184"><a href="#cb2-184" aria-hidden="true" tabindex="-1"></a>    eval_dataset<span class="op">=</span>val_dataset,</span>
<span id="cb2-185"><a href="#cb2-185" aria-hidden="true" tabindex="-1"></a>    data_collator<span class="op">=</span>data_collator</span>
<span id="cb2-186"><a href="#cb2-186" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-187"><a href="#cb2-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-188"><a href="#cb2-188" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Trainer initialized successfully!"</span>)</span>
<span id="cb2-189"><a href="#cb2-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-190"><a href="#cb2-190" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">8. Starting training..."</span>)</span>
<span id="cb2-191"><a href="#cb2-191" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb2-192"><a href="#cb2-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-193"><a href="#cb2-193" aria-hidden="true" tabindex="-1"></a>trainer.train()</span>
<span id="cb2-194"><a href="#cb2-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-195"><a href="#cb2-195" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb2-196"><a href="#cb2-196" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training complete!"</span>)</span>
<span id="cb2-197"><a href="#cb2-197" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb2-198"><a href="#cb2-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-199"><a href="#cb2-199" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">9. Saving final model..."</span>)</span>
<span id="cb2-200"><a href="#cb2-200" aria-hidden="true" tabindex="-1"></a>model.save_pretrained(<span class="ss">f"</span><span class="sc">{</span>OUTPUT_DIR<span class="sc">}</span><span class="ss">/final_model"</span>)</span>
<span id="cb2-201"><a href="#cb2-201" aria-hidden="true" tabindex="-1"></a>tokenizer.save_pretrained(<span class="ss">f"</span><span class="sc">{</span>OUTPUT_DIR<span class="sc">}</span><span class="ss">/final_model"</span>)</span>
<span id="cb2-202"><a href="#cb2-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-203"><a href="#cb2-203" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model saved to: </span><span class="sc">{</span>OUTPUT_DIR<span class="sc">}</span><span class="ss">/final_model"</span>)</span>
<span id="cb2-204"><a href="#cb2-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-205"><a href="#cb2-205" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">10. Final evaluation metrics:"</span>)</span>
<span id="cb2-206"><a href="#cb2-206" aria-hidden="true" tabindex="-1"></a>eval_results <span class="op">=</span> trainer.evaluate()</span>
<span id="cb2-207"><a href="#cb2-207" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> key, value <span class="kw">in</span> eval_results.items():</span>
<span id="cb2-208"><a href="#cb2-208" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>key<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>value<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb2-209"><a href="#cb2-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-210"><a href="#cb2-210" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb2-211"><a href="#cb2-211" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Fine-tuning pipeline complete!"</span>)</span>
<span id="cb2-212"><a href="#cb2-212" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="constitutions-dataset" class="level4" data-number="5.3.1.2">
<h4 data-number="5.3.1.2" class="anchored" data-anchor-id="constitutions-dataset"><span class="header-section-number">5.3.1.2</span> Constitutions Dataset:</h4>
<div class="sourceCode" id="cb3"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Constitutions Fine-Tuning Model Bash Script</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --job-name=constitution_finetune</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --partition=gpu</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --nodes=1</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --ntasks-per-node=1</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --cpus-per-task=8</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --mem=64G</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --gres=gpu:1</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --time=6:00:00</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --output=logs/const_finetune_%j.out</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --error=logs/const_finetune_%j.err</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --mail-type=</span><span class="re">END</span><span class="co">,FAIL</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --mail-user=starwill16@gmail.com</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="ex">module</span> purge</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="ex">module</span> load Python/3.12.3-GCCcore-13.3.0</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="ex">module</span> load CUDA/12.3.0</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="bu">source</span> /home/gcsc563_01/CSC463/conlingo/llama_env/bin/activate</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">HF_HOME</span><span class="op">=</span>/home/gcsc563_01/.cache/huggingface</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">HUGGINGFACE_HUB_CACHE</span><span class="op">=</span>/home/gcsc563_01/.cache/huggingface</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> /home/gcsc563_01/conlingo</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> scripts/training/finetune_indian_constitutions.py</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a><span class="ex">deactivate</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Constitutions Fine-Tuning Model Python Script</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co">#!/usr/bin/env python3</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co">LoRA Fine-Tuning for the Indian Constitutions Data</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co">Fine-tunes LLaMA-3 8B on Indian Constitutions Q&amp;A pairs</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> (</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    AutoTokenizer,</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    AutoModelForCausalLM,</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    TrainingArguments,</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    Trainer,</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    DataCollatorForLanguageModeling</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> peft <span class="im">import</span> LoraConfig, get_peft_model</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> Dataset</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Constitutions LoRA Fine-Tuning Pipeline"</span>)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Paths</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>DATA_PATH <span class="op">=</span> <span class="st">"/home/gcsc563_01/conlingo/data/raw/constitution_qa.jsonl"</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>OUTPUT_DIR <span class="op">=</span> <span class="st">"/home/gcsc563_01/conlingo/models/finetuned/constitution"</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>MODEL_NAME <span class="op">=</span> <span class="st">"meta-llama/Meta-Llama-3-8B-Instruct"</span></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Create output directory</span></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>os.makedirs(OUTPUT_DIR, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">1. Loading data from: </span><span class="sc">{</span>DATA_PATH<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Load JSONL data</span></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> []</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(DATA_PATH, <span class="st">'r'</span>, encoding<span class="op">=</span><span class="st">'utf-8'</span>) <span class="im">as</span> f:</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> line <span class="kw">in</span> f:</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>        data.append(json.loads(line))</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total examples: </span><span class="sc">{</span><span class="bu">len</span>(data)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract question-answer pairs</span></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>examples <span class="op">=</span> []</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> item <span class="kw">in</span> data:</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>    examples.append({</span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>        <span class="st">"question"</span>: item[<span class="st">"question"</span>],</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>        <span class="st">"answer"</span>: item[<span class="st">"answer"</span>]</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Split into train/validation (90/10)</span></span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>train_data, val_data <span class="op">=</span> train_test_split(examples, test_size<span class="op">=</span><span class="fl">0.1</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training examples: </span><span class="sc">{</span><span class="bu">len</span>(train_data)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Validation examples: </span><span class="sc">{</span><span class="bu">len</span>(val_data)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">2. Loading tokenizer..."</span>)</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(MODEL_NAME)</span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a>tokenizer.pad_token <span class="op">=</span> tokenizer.eos_token</span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>tokenizer.padding_side <span class="op">=</span> <span class="st">"right"</span></span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Tokenizer loaded: </span><span class="sc">{</span>tokenizer<span class="sc">.</span><span class="va">__class__</span><span class="sc">.</span><span class="va">__name__</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">3. Preparing datasets..."</span>)</span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> format_instruction(example):</span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Format question-answer pair for training"""</span></span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> <span class="ss">f"### Question:</span><span class="ch">\n</span><span class="sc">{</span>example[<span class="st">'question'</span>]<span class="sc">}</span><span class="ch">\n\n</span><span class="ss">### Answer:</span><span class="ch">\n</span><span class="sc">{</span>example[<span class="st">'answer'</span>]<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> text</span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize_function(example):</span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Tokenize examples with padding and truncation"""</span></span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> format_instruction(example)</span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a>    tokenized <span class="op">=</span> tokenizer(</span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a>        text,</span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a>        truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb4-80"><a href="#cb4-80" aria-hidden="true" tabindex="-1"></a>        max_length<span class="op">=</span><span class="dv">512</span>,</span>
<span id="cb4-81"><a href="#cb4-81" aria-hidden="true" tabindex="-1"></a>        padding<span class="op">=</span><span class="st">"max_length"</span>,</span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a>        return_tensors<span class="op">=</span><span class="va">None</span></span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb4-84"><a href="#cb4-84" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-85"><a href="#cb4-85" aria-hidden="true" tabindex="-1"></a>    tokenized[<span class="st">"labels"</span>] <span class="op">=</span> tokenized[<span class="st">"input_ids"</span>].copy()</span>
<span id="cb4-86"><a href="#cb4-86" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokenized</span>
<span id="cb4-87"><a href="#cb4-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-88"><a href="#cb4-88" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to HuggingFace Dataset format</span></span>
<span id="cb4-89"><a href="#cb4-89" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> Dataset.from_list(train_data)</span>
<span id="cb4-90"><a href="#cb4-90" aria-hidden="true" tabindex="-1"></a>val_dataset <span class="op">=</span> Dataset.from_list(val_data)</span>
<span id="cb4-91"><a href="#cb4-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-92"><a href="#cb4-92" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenize</span></span>
<span id="cb4-93"><a href="#cb4-93" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Tokenizing training data..."</span>)</span>
<span id="cb4-94"><a href="#cb4-94" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> train_dataset.<span class="bu">map</span>(</span>
<span id="cb4-95"><a href="#cb4-95" aria-hidden="true" tabindex="-1"></a>    tokenize_function,</span>
<span id="cb4-96"><a href="#cb4-96" aria-hidden="true" tabindex="-1"></a>    remove_columns<span class="op">=</span>train_dataset.column_names</span>
<span id="cb4-97"><a href="#cb4-97" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-98"><a href="#cb4-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-99"><a href="#cb4-99" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Tokenizing validation data..."</span>)</span>
<span id="cb4-100"><a href="#cb4-100" aria-hidden="true" tabindex="-1"></a>val_dataset <span class="op">=</span> val_dataset.<span class="bu">map</span>(</span>
<span id="cb4-101"><a href="#cb4-101" aria-hidden="true" tabindex="-1"></a>    tokenize_function,</span>
<span id="cb4-102"><a href="#cb4-102" aria-hidden="true" tabindex="-1"></a>    remove_columns<span class="op">=</span>val_dataset.column_names</span>
<span id="cb4-103"><a href="#cb4-103" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-104"><a href="#cb4-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-105"><a href="#cb4-105" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training dataset size: </span><span class="sc">{</span><span class="bu">len</span>(train_dataset)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-106"><a href="#cb4-106" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Validation dataset size: </span><span class="sc">{</span><span class="bu">len</span>(val_dataset)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-107"><a href="#cb4-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-108"><a href="#cb4-108" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">4. Loading base model..."</span>)</span>
<span id="cb4-109"><a href="#cb4-109" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb4-110"><a href="#cb4-110" aria-hidden="true" tabindex="-1"></a>    MODEL_NAME,</span>
<span id="cb4-111"><a href="#cb4-111" aria-hidden="true" tabindex="-1"></a>    device_map<span class="op">=</span><span class="st">"auto"</span>,</span>
<span id="cb4-112"><a href="#cb4-112" aria-hidden="true" tabindex="-1"></a>    torch_dtype<span class="op">=</span>torch.float16,</span>
<span id="cb4-113"><a href="#cb4-113" aria-hidden="true" tabindex="-1"></a>    low_cpu_mem_usage<span class="op">=</span><span class="va">True</span></span>
<span id="cb4-114"><a href="#cb4-114" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-115"><a href="#cb4-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-116"><a href="#cb4-116" aria-hidden="true" tabindex="-1"></a>model.gradient_checkpointing_enable()</span>
<span id="cb4-117"><a href="#cb4-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-118"><a href="#cb4-118" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model loaded: </span><span class="sc">{</span>model<span class="sc">.</span><span class="va">__class__</span><span class="sc">.</span><span class="va">__name__</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-119"><a href="#cb4-119" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model parameters: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters())<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb4-120"><a href="#cb4-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-121"><a href="#cb4-121" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">5. Configuring LoRA..."</span>)</span>
<span id="cb4-122"><a href="#cb4-122" aria-hidden="true" tabindex="-1"></a>lora_config <span class="op">=</span> LoraConfig(</span>
<span id="cb4-123"><a href="#cb4-123" aria-hidden="true" tabindex="-1"></a>    r<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb4-124"><a href="#cb4-124" aria-hidden="true" tabindex="-1"></a>    lora_alpha<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb4-125"><a href="#cb4-125" aria-hidden="true" tabindex="-1"></a>    target_modules<span class="op">=</span>[<span class="st">"q_proj"</span>, <span class="st">"v_proj"</span>, <span class="st">"k_proj"</span>, <span class="st">"o_proj"</span>],</span>
<span id="cb4-126"><a href="#cb4-126" aria-hidden="true" tabindex="-1"></a>    lora_dropout<span class="op">=</span><span class="fl">0.05</span>,</span>
<span id="cb4-127"><a href="#cb4-127" aria-hidden="true" tabindex="-1"></a>    bias<span class="op">=</span><span class="st">"none"</span>,</span>
<span id="cb4-128"><a href="#cb4-128" aria-hidden="true" tabindex="-1"></a>    task_type<span class="op">=</span><span class="st">"CAUSAL_LM"</span></span>
<span id="cb4-129"><a href="#cb4-129" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-130"><a href="#cb4-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-131"><a href="#cb4-131" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> get_peft_model(model, lora_config)</span>
<span id="cb4-132"><a href="#cb4-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-133"><a href="#cb4-133" aria-hidden="true" tabindex="-1"></a>trainable_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters() <span class="cf">if</span> p.requires_grad)</span>
<span id="cb4-134"><a href="#cb4-134" aria-hidden="true" tabindex="-1"></a>total_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters())</span>
<span id="cb4-135"><a href="#cb4-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-136"><a href="#cb4-136" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"LoRA configured successfully"</span>)</span>
<span id="cb4-137"><a href="#cb4-137" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Trainable parameters: </span><span class="sc">{</span>trainable_params<span class="sc">:,}</span><span class="ss"> (</span><span class="sc">{</span><span class="dv">100</span><span class="op">*</span>trainable_params<span class="op">/</span>total_params<span class="sc">:.4f}</span><span class="ss">%)"</span>)</span>
<span id="cb4-138"><a href="#cb4-138" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total parameters: </span><span class="sc">{</span>total_params<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb4-139"><a href="#cb4-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-140"><a href="#cb4-140" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">6. Setting up training arguments..."</span>)</span>
<span id="cb4-141"><a href="#cb4-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-142"><a href="#cb4-142" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments(</span>
<span id="cb4-143"><a href="#cb4-143" aria-hidden="true" tabindex="-1"></a>    output_dir<span class="op">=</span>OUTPUT_DIR,</span>
<span id="cb4-144"><a href="#cb4-144" aria-hidden="true" tabindex="-1"></a>    num_train_epochs<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb4-145"><a href="#cb4-145" aria-hidden="true" tabindex="-1"></a>    per_device_train_batch_size<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb4-146"><a href="#cb4-146" aria-hidden="true" tabindex="-1"></a>    per_device_eval_batch_size<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb4-147"><a href="#cb4-147" aria-hidden="true" tabindex="-1"></a>    gradient_accumulation_steps<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb4-148"><a href="#cb4-148" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">2e-4</span>,</span>
<span id="cb4-149"><a href="#cb4-149" aria-hidden="true" tabindex="-1"></a>    lr_scheduler_type<span class="op">=</span><span class="st">"cosine"</span>,</span>
<span id="cb4-150"><a href="#cb4-150" aria-hidden="true" tabindex="-1"></a>    warmup_steps<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb4-151"><a href="#cb4-151" aria-hidden="true" tabindex="-1"></a>    logging_steps<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb4-152"><a href="#cb4-152" aria-hidden="true" tabindex="-1"></a>    save_steps<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb4-153"><a href="#cb4-153" aria-hidden="true" tabindex="-1"></a>    eval_steps<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb4-154"><a href="#cb4-154" aria-hidden="true" tabindex="-1"></a>    eval_strategy<span class="op">=</span><span class="st">"steps"</span>,</span>
<span id="cb4-155"><a href="#cb4-155" aria-hidden="true" tabindex="-1"></a>    save_strategy<span class="op">=</span><span class="st">"steps"</span>,</span>
<span id="cb4-156"><a href="#cb4-156" aria-hidden="true" tabindex="-1"></a>    load_best_model_at_end<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb4-157"><a href="#cb4-157" aria-hidden="true" tabindex="-1"></a>    metric_for_best_model<span class="op">=</span><span class="st">"eval_loss"</span>,</span>
<span id="cb4-158"><a href="#cb4-158" aria-hidden="true" tabindex="-1"></a>    greater_is_better<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb4-159"><a href="#cb4-159" aria-hidden="true" tabindex="-1"></a>    fp16<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb4-160"><a href="#cb4-160" aria-hidden="true" tabindex="-1"></a>    report_to<span class="op">=</span><span class="st">"none"</span>,</span>
<span id="cb4-161"><a href="#cb4-161" aria-hidden="true" tabindex="-1"></a>    save_total_limit<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb4-162"><a href="#cb4-162" aria-hidden="true" tabindex="-1"></a>    remove_unused_columns<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb4-163"><a href="#cb4-163" aria-hidden="true" tabindex="-1"></a>    gradient_checkpointing<span class="op">=</span><span class="va">True</span></span>
<span id="cb4-164"><a href="#cb4-164" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-165"><a href="#cb4-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-166"><a href="#cb4-166" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training configuration:"</span>)</span>
<span id="cb4-167"><a href="#cb4-167" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Epochs: </span><span class="sc">{</span>training_args<span class="sc">.</span>num_train_epochs<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-168"><a href="#cb4-168" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Batch size: </span><span class="sc">{</span>training_args<span class="sc">.</span>per_device_train_batch_size<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-169"><a href="#cb4-169" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Gradient accumulation: </span><span class="sc">{</span>training_args<span class="sc">.</span>gradient_accumulation_steps<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-170"><a href="#cb4-170" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Effective batch size: </span><span class="sc">{</span>training_args<span class="sc">.</span>per_device_train_batch_size <span class="op">*</span> training_args<span class="sc">.</span>gradient_accumulation_steps<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-171"><a href="#cb4-171" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Learning rate: </span><span class="sc">{</span>training_args<span class="sc">.</span>learning_rate<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-172"><a href="#cb4-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-173"><a href="#cb4-173" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">7. Initializing trainer..."</span>)</span>
<span id="cb4-174"><a href="#cb4-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-175"><a href="#cb4-175" aria-hidden="true" tabindex="-1"></a>data_collator <span class="op">=</span> DataCollatorForLanguageModeling(</span>
<span id="cb4-176"><a href="#cb4-176" aria-hidden="true" tabindex="-1"></a>    tokenizer<span class="op">=</span>tokenizer,</span>
<span id="cb4-177"><a href="#cb4-177" aria-hidden="true" tabindex="-1"></a>    mlm<span class="op">=</span><span class="va">False</span></span>
<span id="cb4-178"><a href="#cb4-178" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-179"><a href="#cb4-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-180"><a href="#cb4-180" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(</span>
<span id="cb4-181"><a href="#cb4-181" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model,</span>
<span id="cb4-182"><a href="#cb4-182" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>training_args,</span>
<span id="cb4-183"><a href="#cb4-183" aria-hidden="true" tabindex="-1"></a>    train_dataset<span class="op">=</span>train_dataset,</span>
<span id="cb4-184"><a href="#cb4-184" aria-hidden="true" tabindex="-1"></a>    eval_dataset<span class="op">=</span>val_dataset,</span>
<span id="cb4-185"><a href="#cb4-185" aria-hidden="true" tabindex="-1"></a>    data_collator<span class="op">=</span>data_collator</span>
<span id="cb4-186"><a href="#cb4-186" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-187"><a href="#cb4-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-188"><a href="#cb4-188" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Trainer initialized successfully!"</span>)</span>
<span id="cb4-189"><a href="#cb4-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-190"><a href="#cb4-190" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">8. Starting training..."</span>)</span>
<span id="cb4-191"><a href="#cb4-191" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb4-192"><a href="#cb4-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-193"><a href="#cb4-193" aria-hidden="true" tabindex="-1"></a>trainer.train()</span>
<span id="cb4-194"><a href="#cb4-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-195"><a href="#cb4-195" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb4-196"><a href="#cb4-196" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training complete!"</span>)</span>
<span id="cb4-197"><a href="#cb4-197" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb4-198"><a href="#cb4-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-199"><a href="#cb4-199" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">9. Saving final model..."</span>)</span>
<span id="cb4-200"><a href="#cb4-200" aria-hidden="true" tabindex="-1"></a>model.save_pretrained(<span class="ss">f"</span><span class="sc">{</span>OUTPUT_DIR<span class="sc">}</span><span class="ss">/final_model"</span>)</span>
<span id="cb4-201"><a href="#cb4-201" aria-hidden="true" tabindex="-1"></a>tokenizer.save_pretrained(<span class="ss">f"</span><span class="sc">{</span>OUTPUT_DIR<span class="sc">}</span><span class="ss">/final_model"</span>)</span>
<span id="cb4-202"><a href="#cb4-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-203"><a href="#cb4-203" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model saved to: </span><span class="sc">{</span>OUTPUT_DIR<span class="sc">}</span><span class="ss">/final_model"</span>)</span>
<span id="cb4-204"><a href="#cb4-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-205"><a href="#cb4-205" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">10. Final evaluation metrics:"</span>)</span>
<span id="cb4-206"><a href="#cb4-206" aria-hidden="true" tabindex="-1"></a>eval_results <span class="op">=</span> trainer.evaluate()</span>
<span id="cb4-207"><a href="#cb4-207" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> key, value <span class="kw">in</span> eval_results.items():</span>
<span id="cb4-208"><a href="#cb4-208" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>key<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>value<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb4-209"><a href="#cb4-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-210"><a href="#cb4-210" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb4-211"><a href="#cb4-211" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Fine-tuning pipeline complete!"</span>)</span>
<span id="cb4-212"><a href="#cb4-212" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><br></p>
</section>
</section>
<section id="question-testing" class="level3" data-number="5.3.2">
<h3 data-number="5.3.2" class="anchored" data-anchor-id="question-testing"><span class="header-section-number">5.3.2</span> Question Testing</h3>
<p>Once the fine-tuned models for the two datasets where created, the next was to test each model against the questions provided by Rohan.</p>
<section id="superstitions-dataset-1" class="level4" data-number="5.3.2.1">
<h4 data-number="5.3.2.1" class="anchored" data-anchor-id="superstitions-dataset-1"><span class="header-section-number">5.3.2.1</span> Superstitions Dataset:</h4>
<div class="sourceCode" id="cb5"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Superstitions Model Testing Bash Script</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --job-name=test_super_model</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --partition=gpu</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --nodes=1</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --ntasks-per-node=1</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --cpus-per-task=4</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --mem=32G</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --gres=gpu:1</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --time=1:00:00</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --output=logs/test_super_%j.out</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --error=logs/test_super_%j.err</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --mail-type=</span><span class="re">END</span><span class="co">,FAIL</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --mail-user=starwill16@gmail.com</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="ex">module</span> purge</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="ex">module</span> load Python/3.12.3-GCCcore-13.3.0</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="ex">module</span> load CUDA/12.3.0</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="bu">source</span> /home/gcsc563_01/CSC463/conlingo/llama_env/bin/activate</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">HF_HOME</span><span class="op">=</span>/home/gcsc563_01/.cache/huggingface</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">HUGGINGFACE_HUB_CACHE</span><span class="op">=</span>/home/gcsc563_01/.cache/huggingface</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> /home/gcsc563_01/conlingo</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> scripts/training/run_superstitions_model.py</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a><span class="ex">deactivate</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Superstitions Model Testing Python Script</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co">#!/usr/bin/env python3</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co">Test the fine-tuned TED Talks model</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co">Loads the model and runs sample inference</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModelForCausalLM</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> peft <span class="im">import</span> PeftModel</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Superstitions Model Inference Test"</span>)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Paths</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>BASE_MODEL_NAME <span class="op">=</span> <span class="st">"meta-llama/Meta-Llama-3-8B-Instruct"</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>FINETUNED_MODEL_PATH <span class="op">=</span> <span class="st">"/home/gcsc563_01/conlingo/models/finetuned/superstition/final_model"</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">1. Loading tokenizer..."</span>)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(FINETUNED_MODEL_PATH)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"✅ Tokenizer loaded"</span>)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">2. Loading base model..."</span>)</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>base_model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>    BASE_MODEL_NAME,</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>    device_map<span class="op">=</span><span class="st">"auto"</span>,</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>    torch_dtype<span class="op">=</span>torch.float16,</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>    low_cpu_mem_usage<span class="op">=</span><span class="va">True</span></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"✅ Base model loaded"</span>)</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">3. Loading fine-tuned LoRA adapters..."</span>)</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> PeftModel.from_pretrained(base_model, FINETUNED_MODEL_PATH)</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"✅ Fine-tuned model loaded"</span>)</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">4. Running test inference..."</span>)</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Test questions</span></span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>test_questions <span class="op">=</span> [</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How can Diwali and Christmas both symbolize the victory of light over darkness in their respective traditions?"</span>,</span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>    <span class="st">"What themes of renewal in Diwali resonate with the idea of rebirth in Christian theology?"</span>,</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How might Indian Christians use Diwali to express biblical messages of hope?"</span>,</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How can Christian schools acknowledge Diwali without compromising faith boundaries?"</span>,</span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a>    <span class="st">"What sensitivities should pastors consider when mentioning Hindu deities in Christmas homilies?"</span>,</span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How can Christian youth groups create shared Diwali–Christmas community service projects?"</span>,</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How can churches ensure caste-neutral seating and participation during worship?"</span>,</span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>    <span class="st">"What examples of caste inclusion can be found in the life of Jesus?"</span>,</span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>    <span class="st">"What are sensitive ways to discuss “the least of these” without reinforcing caste bias?"</span>,</span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Why might some Christians still use caste surnames, and how should this be discussed?"</span>,</span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a>    <span class="st">"What is a sensitive way to discuss Jesus’ association with marginalized groups?"</span>,</span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a>    <span class="st">"What is an inclusive way to discuss Krishna and Christ comparisons in academia?"</span>,</span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How can Hindu concepts of karma be reconciled with Christian grace in conversation?"</span>,</span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How can Christian missions emphasize shared moral principles rather than conversion?"</span>,</span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How does food during Christmas vary regionally across India?"</span>,</span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How does regional folklore shape Christian storytelling traditions?"</span>,</span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How do Indian Christian elders respond to youth questioning of denominational traditions?"</span>,</span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How can faith-based education evolve to reach younger, tech-savvy Christians?"</span>,</span>
<span id="cb6-61"><a href="#cb6-61" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How can liturgy adapt to youth participation without losing sacredness?"</span>,</span>
<span id="cb6-62"><a href="#cb6-62" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How do young Christians view service and mission in a modern Indian context?"</span></span>
<span id="cb6-63"><a href="#cb6-63" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb6-64"><a href="#cb6-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-65"><a href="#cb6-65" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, question <span class="kw">in</span> <span class="bu">enumerate</span>(test_questions, <span class="dv">1</span>):</span>
<span id="cb6-66"><a href="#cb6-66" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">[Test </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">]"</span>)</span>
<span id="cb6-67"><a href="#cb6-67" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Question: </span><span class="sc">{</span>question<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-68"><a href="#cb6-68" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-69"><a href="#cb6-69" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Format prompt</span></span>
<span id="cb6-70"><a href="#cb6-70" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">=</span> <span class="ss">f"### Question:</span><span class="ch">\n</span><span class="sc">{</span>question<span class="sc">}</span><span class="ch">\n\n</span><span class="ss">### Answer:</span><span class="ch">\n</span><span class="ss">"</span></span>
<span id="cb6-71"><a href="#cb6-71" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-72"><a href="#cb6-72" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Tokenize</span></span>
<span id="cb6-73"><a href="#cb6-73" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> tokenizer(prompt, return_tensors<span class="op">=</span><span class="st">"pt"</span>).to(model.device)</span>
<span id="cb6-74"><a href="#cb6-74" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-75"><a href="#cb6-75" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate</span></span>
<span id="cb6-76"><a href="#cb6-76" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb6-77"><a href="#cb6-77" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> model.generate(</span>
<span id="cb6-78"><a href="#cb6-78" aria-hidden="true" tabindex="-1"></a>            <span class="op">**</span>inputs,</span>
<span id="cb6-79"><a href="#cb6-79" aria-hidden="true" tabindex="-1"></a>            max_new_tokens<span class="op">=</span><span class="dv">150</span>,</span>
<span id="cb6-80"><a href="#cb6-80" aria-hidden="true" tabindex="-1"></a>            temperature<span class="op">=</span><span class="fl">0.7</span>,</span>
<span id="cb6-81"><a href="#cb6-81" aria-hidden="true" tabindex="-1"></a>            do_sample<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb6-82"><a href="#cb6-82" aria-hidden="true" tabindex="-1"></a>            top_p<span class="op">=</span><span class="fl">0.9</span>,</span>
<span id="cb6-83"><a href="#cb6-83" aria-hidden="true" tabindex="-1"></a>            pad_token_id<span class="op">=</span>tokenizer.eos_token_id</span>
<span id="cb6-84"><a href="#cb6-84" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb6-85"><a href="#cb6-85" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-86"><a href="#cb6-86" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Decode</span></span>
<span id="cb6-87"><a href="#cb6-87" aria-hidden="true" tabindex="-1"></a>    response <span class="op">=</span> tokenizer.decode(outputs[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-88"><a href="#cb6-88" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-89"><a href="#cb6-89" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Extract only the answer part (after "### Answer:")</span></span>
<span id="cb6-90"><a href="#cb6-90" aria-hidden="true" tabindex="-1"></a>    answer <span class="op">=</span> response.split(<span class="st">"### Answer:"</span>)[<span class="op">-</span><span class="dv">1</span>].strip()</span>
<span id="cb6-91"><a href="#cb6-91" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-92"><a href="#cb6-92" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Answer: </span><span class="sc">{</span>answer<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-93"><a href="#cb6-93" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"-"</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb6-94"><a href="#cb6-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-95"><a href="#cb6-95" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb6-96"><a href="#cb6-96" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Inference test complete!"</span>)</span>
<span id="cb6-97"><a href="#cb6-97" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><br></p>
</section>
<section id="constitutions-dataset-1" class="level4" data-number="5.3.2.2">
<h4 data-number="5.3.2.2" class="anchored" data-anchor-id="constitutions-dataset-1"><span class="header-section-number">5.3.2.2</span> Constitutions Dataset:</h4>
<div class="sourceCode" id="cb7"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Constitutions Model Testing Bash Script</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --job-name=test_const_model</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --partition=gpu</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --nodes=1</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --ntasks-per-node=1</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --cpus-per-task=4</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --mem=32G</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --gres=gpu:1</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --time=1:00:00</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --output=logs/test_const_%j.out</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --error=logs/test_const_%j.err</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --mail-type=</span><span class="re">END</span><span class="co">,FAIL</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --mail-user=starwill16@gmail.com</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="ex">module</span> purge</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="ex">module</span> load Python/3.12.3-GCCcore-13.3.0</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="ex">module</span> load CUDA/12.3.0</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a><span class="bu">source</span> /home/gcsc563_01/CSC463/conlingo/llama_env/bin/activate</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">HF_HOME</span><span class="op">=</span>/home/gcsc563_01/.cache/huggingface</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">HUGGINGFACE_HUB_CACHE</span><span class="op">=</span>/home/gcsc563_01/.cache/huggingface</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> /home/gcsc563_01/conlingo</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> scripts/training/run_constitutions_model.py</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a><span class="ex">deactivate</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Constitutions Model Testing Python Script</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co">#!/usr/bin/env python3</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co">Test the fine-tuned Constitutions model</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co">Loads the model and runs sample inference</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModelForCausalLM</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> peft <span class="im">import</span> PeftModel</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Constitutions Model Inference Test"</span>)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Paths</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>BASE_MODEL_NAME <span class="op">=</span> <span class="st">"meta-llama/Meta-Llama-3-8B-Instruct"</span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>FINETUNED_MODEL_PATH <span class="op">=</span> <span class="st">"/home/gcsc563_01/conlingo/models/finetuned/constitution/final_model"</span></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">1. Loading tokenizer..."</span>)</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(FINETUNED_MODEL_PATH)</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"✅ Tokenizer loaded"</span>)</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">2. Loading base model..."</span>)</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>base_model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>    BASE_MODEL_NAME,</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>    device_map<span class="op">=</span><span class="st">"auto"</span>,</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>    torch_dtype<span class="op">=</span>torch.float16,</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>    low_cpu_mem_usage<span class="op">=</span><span class="va">True</span></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"✅ Base model loaded"</span>)</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">3. Loading fine-tuned LoRA adapters..."</span>)</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> PeftModel.from_pretrained(base_model, FINETUNED_MODEL_PATH)</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"✅ Fine-tuned model loaded"</span>)</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">4. Running test inference..."</span>)</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Test questions</span></span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>test_questions <span class="op">=</span> [</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How can Diwali and Christmas both symbolize the victory of light over darkness in their respective traditions?"</span>,</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>    <span class="st">"What themes of renewal in Diwali resonate with the idea of rebirth in Christian theology?"</span>,</span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How might Indian Christians use Diwali to express biblical messages of hope?"</span>,</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How can Christian schools acknowledge Diwali without compromising faith boundaries?"</span>,</span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>    <span class="st">"What sensitivities should pastors consider when mentioning Hindu deities in Christmas homilies?"</span>,</span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How can Christian youth groups create shared Diwali–Christmas community service projects?"</span>,</span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How can churches ensure caste-neutral seating and participation during worship?"</span>,</span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>    <span class="st">"What examples of caste inclusion can be found in the life of Jesus?"</span>,</span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a>    <span class="st">"What are sensitive ways to discuss “the least of these” without reinforcing caste bias?"</span>,</span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Why might some Christians still use caste surnames, and how should this be discussed?"</span>,</span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a>    <span class="st">"What is a sensitive way to discuss Jesus’ association with marginalized groups?"</span>,</span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>    <span class="st">"What is an inclusive way to discuss Krishna and Christ comparisons in academia?"</span>,</span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How can Hindu concepts of karma be reconciled with Christian grace in conversation?"</span>,</span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How can Christian missions emphasize shared moral principles rather than conversion?"</span>,</span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How does food during Christmas vary regionally across India?"</span>,</span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How does regional folklore shape Christian storytelling traditions?"</span>,</span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How do Indian Christian elders respond to youth questioning of denominational traditions?"</span>,</span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How can faith-based education evolve to reach younger, tech-savvy Christians?"</span>,</span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How can liturgy adapt to youth participation without losing sacredness?"</span>,</span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How do young Christians view service and mission in a modern Indian context?"</span></span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-65"><a href="#cb8-65" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, question <span class="kw">in</span> <span class="bu">enumerate</span>(test_questions, <span class="dv">1</span>):</span>
<span id="cb8-66"><a href="#cb8-66" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">[Test </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">]"</span>)</span>
<span id="cb8-67"><a href="#cb8-67" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Question: </span><span class="sc">{</span>question<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-68"><a href="#cb8-68" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-69"><a href="#cb8-69" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Format prompt</span></span>
<span id="cb8-70"><a href="#cb8-70" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">=</span> <span class="ss">f"### Question:</span><span class="ch">\n</span><span class="sc">{</span>question<span class="sc">}</span><span class="ch">\n\n</span><span class="ss">### Answer:</span><span class="ch">\n</span><span class="ss">"</span></span>
<span id="cb8-71"><a href="#cb8-71" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-72"><a href="#cb8-72" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Tokenize</span></span>
<span id="cb8-73"><a href="#cb8-73" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> tokenizer(prompt, return_tensors<span class="op">=</span><span class="st">"pt"</span>).to(model.device)</span>
<span id="cb8-74"><a href="#cb8-74" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-75"><a href="#cb8-75" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate</span></span>
<span id="cb8-76"><a href="#cb8-76" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb8-77"><a href="#cb8-77" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> model.generate(</span>
<span id="cb8-78"><a href="#cb8-78" aria-hidden="true" tabindex="-1"></a>            <span class="op">**</span>inputs,</span>
<span id="cb8-79"><a href="#cb8-79" aria-hidden="true" tabindex="-1"></a>            max_new_tokens<span class="op">=</span><span class="dv">150</span>,</span>
<span id="cb8-80"><a href="#cb8-80" aria-hidden="true" tabindex="-1"></a>            temperature<span class="op">=</span><span class="fl">0.7</span>,</span>
<span id="cb8-81"><a href="#cb8-81" aria-hidden="true" tabindex="-1"></a>            do_sample<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb8-82"><a href="#cb8-82" aria-hidden="true" tabindex="-1"></a>            top_p<span class="op">=</span><span class="fl">0.9</span>,</span>
<span id="cb8-83"><a href="#cb8-83" aria-hidden="true" tabindex="-1"></a>            pad_token_id<span class="op">=</span>tokenizer.eos_token_id</span>
<span id="cb8-84"><a href="#cb8-84" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb8-85"><a href="#cb8-85" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-86"><a href="#cb8-86" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Decode</span></span>
<span id="cb8-87"><a href="#cb8-87" aria-hidden="true" tabindex="-1"></a>    response <span class="op">=</span> tokenizer.decode(outputs[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-88"><a href="#cb8-88" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-89"><a href="#cb8-89" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Extract only the answer part (after "### Answer:")</span></span>
<span id="cb8-90"><a href="#cb8-90" aria-hidden="true" tabindex="-1"></a>    answer <span class="op">=</span> response.split(<span class="st">"### Answer:"</span>)[<span class="op">-</span><span class="dv">1</span>].strip()</span>
<span id="cb8-91"><a href="#cb8-91" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-92"><a href="#cb8-92" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Answer: </span><span class="sc">{</span>answer<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-93"><a href="#cb8-93" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"-"</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb8-94"><a href="#cb8-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-95"><a href="#cb8-95" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb8-96"><a href="#cb8-96" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Inference test complete!"</span>)</span>
<span id="cb8-97"><a href="#cb8-97" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><br></p>
</section>
</section>
<section id="high-five-testing" class="level3" data-number="5.3.3">
<h3 data-number="5.3.3" class="anchored" data-anchor-id="high-five-testing"><span class="header-section-number">5.3.3</span> High Five Testing</h3>
<p>After testing the other 2 models, Wills final requirement was to combine all datasets into one, creating what I called the “High Five” dataset, fine-tuning a model for it, and testing it against Rohan’s questions one more time.</p>
<section id="fine-tuning-model" class="level4" data-number="5.3.3.1">
<h4 data-number="5.3.3.1" class="anchored" data-anchor-id="fine-tuning-model"><span class="header-section-number">5.3.3.1</span> Fine-Tuning Model:</h4>
<div class="sourceCode" id="cb9"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># High Five Fine-Tuning Model Bash Script</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --job-name=high_five_finetune</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --partition=gpu</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --nodes=1</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --ntasks-per-node=1</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --cpus-per-task=8</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --mem=64G</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --gres=gpu:1</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --time=6:00:00</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --output=logs/high_finetune_%j.out</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --error=logs/high_finetune_%j.err</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --mail-type=</span><span class="re">END</span><span class="co">,FAIL</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --mail-user=starwill16@gmail.com</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="ex">module</span> purge</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="ex">module</span> load Python/3.12.3-GCCcore-13.3.0</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a><span class="ex">module</span> load CUDA/12.3.0</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a><span class="bu">source</span> /home/gcsc563_01/CSC463/conlingo/llama_env/bin/activate</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">HF_HOME</span><span class="op">=</span>/home/gcsc563_01/.cache/huggingface</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">HUGGINGFACE_HUB_CACHE</span><span class="op">=</span>/home/gcsc563_01/.cache/huggingface</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> /home/gcsc563_01/conlingo</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> scripts/training/finetune_high_five.py</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a><span class="ex">deactivate</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># High Five Fine-Tuning Model Python Script</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co">#!/usr/bin/env python3</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co">LoRA Fine-Tuning for all 5 Datasets</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co">Fine-tunes LLaMA-3 8B on all 5 Datasets' Q&amp;A pairs</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> (</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    AutoTokenizer,</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    AutoModelForCausalLM,</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    TrainingArguments,</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    Trainer,</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>    DataCollatorForLanguageModeling</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> peft <span class="im">import</span> LoraConfig, get_peft_model</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> Dataset</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"High Five LoRA Fine-Tuning Pipeline"</span>)</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Paths</span></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>DATA_PATH <span class="op">=</span> <span class="st">"/home/gcsc563_01/conlingo/data/raw/highfive_qa.jsonl"</span></span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>OUTPUT_DIR <span class="op">=</span> <span class="st">"/home/gcsc563_01/conlingo/models/finetuned/high_five"</span></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>MODEL_NAME <span class="op">=</span> <span class="st">"meta-llama/Meta-Llama-3-8B-Instruct"</span></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Create output directory</span></span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>os.makedirs(OUTPUT_DIR, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">1. Loading data from: </span><span class="sc">{</span>DATA_PATH<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Load JSONL data</span></span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> []</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(DATA_PATH, <span class="st">'r'</span>, encoding<span class="op">=</span><span class="st">'utf-8'</span>) <span class="im">as</span> f:</span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> line <span class="kw">in</span> f:</span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a>        data.append(json.loads(line))</span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total examples: </span><span class="sc">{</span><span class="bu">len</span>(data)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract question-answer pairs</span></span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a>examples <span class="op">=</span> []</span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> item <span class="kw">in</span> data:</span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a>    examples.append({</span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a>        <span class="st">"question"</span>: item[<span class="st">"question"</span>],</span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a>        <span class="st">"answer"</span>: item[<span class="st">"answer"</span>]</span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Split into train/validation (90/10)</span></span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a>train_data, val_data <span class="op">=</span> train_test_split(examples, test_size<span class="op">=</span><span class="fl">0.1</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training examples: </span><span class="sc">{</span><span class="bu">len</span>(train_data)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Validation examples: </span><span class="sc">{</span><span class="bu">len</span>(val_data)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">2. Loading tokenizer..."</span>)</span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(MODEL_NAME)</span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a>tokenizer.pad_token <span class="op">=</span> tokenizer.eos_token</span>
<span id="cb10-62"><a href="#cb10-62" aria-hidden="true" tabindex="-1"></a>tokenizer.padding_side <span class="op">=</span> <span class="st">"right"</span></span>
<span id="cb10-63"><a href="#cb10-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-64"><a href="#cb10-64" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Tokenizer loaded: </span><span class="sc">{</span>tokenizer<span class="sc">.</span><span class="va">__class__</span><span class="sc">.</span><span class="va">__name__</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-65"><a href="#cb10-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-66"><a href="#cb10-66" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">3. Preparing datasets..."</span>)</span>
<span id="cb10-67"><a href="#cb10-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-68"><a href="#cb10-68" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> format_instruction(example):</span>
<span id="cb10-69"><a href="#cb10-69" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Format question-answer pair for training"""</span></span>
<span id="cb10-70"><a href="#cb10-70" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> <span class="ss">f"### Question:</span><span class="ch">\n</span><span class="sc">{</span>example[<span class="st">'question'</span>]<span class="sc">}</span><span class="ch">\n\n</span><span class="ss">### Answer:</span><span class="ch">\n</span><span class="sc">{</span>example[<span class="st">'answer'</span>]<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb10-71"><a href="#cb10-71" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> text</span>
<span id="cb10-72"><a href="#cb10-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-73"><a href="#cb10-73" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize_function(example):</span>
<span id="cb10-74"><a href="#cb10-74" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Tokenize examples with padding and truncation"""</span></span>
<span id="cb10-75"><a href="#cb10-75" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> format_instruction(example)</span>
<span id="cb10-76"><a href="#cb10-76" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-77"><a href="#cb10-77" aria-hidden="true" tabindex="-1"></a>    tokenized <span class="op">=</span> tokenizer(</span>
<span id="cb10-78"><a href="#cb10-78" aria-hidden="true" tabindex="-1"></a>        text,</span>
<span id="cb10-79"><a href="#cb10-79" aria-hidden="true" tabindex="-1"></a>        truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb10-80"><a href="#cb10-80" aria-hidden="true" tabindex="-1"></a>        max_length<span class="op">=</span><span class="dv">512</span>,</span>
<span id="cb10-81"><a href="#cb10-81" aria-hidden="true" tabindex="-1"></a>        padding<span class="op">=</span><span class="st">"max_length"</span>,</span>
<span id="cb10-82"><a href="#cb10-82" aria-hidden="true" tabindex="-1"></a>        return_tensors<span class="op">=</span><span class="va">None</span></span>
<span id="cb10-83"><a href="#cb10-83" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb10-84"><a href="#cb10-84" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-85"><a href="#cb10-85" aria-hidden="true" tabindex="-1"></a>    tokenized[<span class="st">"labels"</span>] <span class="op">=</span> tokenized[<span class="st">"input_ids"</span>].copy()</span>
<span id="cb10-86"><a href="#cb10-86" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokenized</span>
<span id="cb10-87"><a href="#cb10-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-88"><a href="#cb10-88" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to HuggingFace Dataset format</span></span>
<span id="cb10-89"><a href="#cb10-89" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> Dataset.from_list(train_data)</span>
<span id="cb10-90"><a href="#cb10-90" aria-hidden="true" tabindex="-1"></a>val_dataset <span class="op">=</span> Dataset.from_list(val_data)</span>
<span id="cb10-91"><a href="#cb10-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-92"><a href="#cb10-92" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenize</span></span>
<span id="cb10-93"><a href="#cb10-93" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Tokenizing training data..."</span>)</span>
<span id="cb10-94"><a href="#cb10-94" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> train_dataset.<span class="bu">map</span>(</span>
<span id="cb10-95"><a href="#cb10-95" aria-hidden="true" tabindex="-1"></a>    tokenize_function,</span>
<span id="cb10-96"><a href="#cb10-96" aria-hidden="true" tabindex="-1"></a>    remove_columns<span class="op">=</span>train_dataset.column_names</span>
<span id="cb10-97"><a href="#cb10-97" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-98"><a href="#cb10-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-99"><a href="#cb10-99" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Tokenizing validation data..."</span>)</span>
<span id="cb10-100"><a href="#cb10-100" aria-hidden="true" tabindex="-1"></a>val_dataset <span class="op">=</span> val_dataset.<span class="bu">map</span>(</span>
<span id="cb10-101"><a href="#cb10-101" aria-hidden="true" tabindex="-1"></a>    tokenize_function,</span>
<span id="cb10-102"><a href="#cb10-102" aria-hidden="true" tabindex="-1"></a>    remove_columns<span class="op">=</span>val_dataset.column_names</span>
<span id="cb10-103"><a href="#cb10-103" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-104"><a href="#cb10-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-105"><a href="#cb10-105" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training dataset size: </span><span class="sc">{</span><span class="bu">len</span>(train_dataset)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-106"><a href="#cb10-106" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Validation dataset size: </span><span class="sc">{</span><span class="bu">len</span>(val_dataset)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-107"><a href="#cb10-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-108"><a href="#cb10-108" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">4. Loading base model..."</span>)</span>
<span id="cb10-109"><a href="#cb10-109" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb10-110"><a href="#cb10-110" aria-hidden="true" tabindex="-1"></a>    MODEL_NAME,</span>
<span id="cb10-111"><a href="#cb10-111" aria-hidden="true" tabindex="-1"></a>    device_map<span class="op">=</span><span class="st">"auto"</span>,</span>
<span id="cb10-112"><a href="#cb10-112" aria-hidden="true" tabindex="-1"></a>    torch_dtype<span class="op">=</span>torch.float16,</span>
<span id="cb10-113"><a href="#cb10-113" aria-hidden="true" tabindex="-1"></a>    low_cpu_mem_usage<span class="op">=</span><span class="va">True</span></span>
<span id="cb10-114"><a href="#cb10-114" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-115"><a href="#cb10-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-116"><a href="#cb10-116" aria-hidden="true" tabindex="-1"></a>model.gradient_checkpointing_enable()</span>
<span id="cb10-117"><a href="#cb10-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-118"><a href="#cb10-118" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model loaded: </span><span class="sc">{</span>model<span class="sc">.</span><span class="va">__class__</span><span class="sc">.</span><span class="va">__name__</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-119"><a href="#cb10-119" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model parameters: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters())<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb10-120"><a href="#cb10-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-121"><a href="#cb10-121" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">5. Configuring LoRA..."</span>)</span>
<span id="cb10-122"><a href="#cb10-122" aria-hidden="true" tabindex="-1"></a>lora_config <span class="op">=</span> LoraConfig(</span>
<span id="cb10-123"><a href="#cb10-123" aria-hidden="true" tabindex="-1"></a>    r<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb10-124"><a href="#cb10-124" aria-hidden="true" tabindex="-1"></a>    lora_alpha<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb10-125"><a href="#cb10-125" aria-hidden="true" tabindex="-1"></a>    target_modules<span class="op">=</span>[<span class="st">"q_proj"</span>, <span class="st">"v_proj"</span>, <span class="st">"k_proj"</span>, <span class="st">"o_proj"</span>],</span>
<span id="cb10-126"><a href="#cb10-126" aria-hidden="true" tabindex="-1"></a>    lora_dropout<span class="op">=</span><span class="fl">0.05</span>,</span>
<span id="cb10-127"><a href="#cb10-127" aria-hidden="true" tabindex="-1"></a>    bias<span class="op">=</span><span class="st">"none"</span>,</span>
<span id="cb10-128"><a href="#cb10-128" aria-hidden="true" tabindex="-1"></a>    task_type<span class="op">=</span><span class="st">"CAUSAL_LM"</span></span>
<span id="cb10-129"><a href="#cb10-129" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-130"><a href="#cb10-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-131"><a href="#cb10-131" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> get_peft_model(model, lora_config)</span>
<span id="cb10-132"><a href="#cb10-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-133"><a href="#cb10-133" aria-hidden="true" tabindex="-1"></a>trainable_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters() <span class="cf">if</span> p.requires_grad)</span>
<span id="cb10-134"><a href="#cb10-134" aria-hidden="true" tabindex="-1"></a>total_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters())</span>
<span id="cb10-135"><a href="#cb10-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-136"><a href="#cb10-136" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"LoRA configured successfully"</span>)</span>
<span id="cb10-137"><a href="#cb10-137" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Trainable parameters: </span><span class="sc">{</span>trainable_params<span class="sc">:,}</span><span class="ss"> (</span><span class="sc">{</span><span class="dv">100</span><span class="op">*</span>trainable_params<span class="op">/</span>total_params<span class="sc">:.4f}</span><span class="ss">%)"</span>)</span>
<span id="cb10-138"><a href="#cb10-138" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total parameters: </span><span class="sc">{</span>total_params<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb10-139"><a href="#cb10-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-140"><a href="#cb10-140" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">6. Setting up training arguments..."</span>)</span>
<span id="cb10-141"><a href="#cb10-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-142"><a href="#cb10-142" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments(</span>
<span id="cb10-143"><a href="#cb10-143" aria-hidden="true" tabindex="-1"></a>    output_dir<span class="op">=</span>OUTPUT_DIR,</span>
<span id="cb10-144"><a href="#cb10-144" aria-hidden="true" tabindex="-1"></a>    num_train_epochs<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb10-145"><a href="#cb10-145" aria-hidden="true" tabindex="-1"></a>    per_device_train_batch_size<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb10-146"><a href="#cb10-146" aria-hidden="true" tabindex="-1"></a>    per_device_eval_batch_size<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb10-147"><a href="#cb10-147" aria-hidden="true" tabindex="-1"></a>    gradient_accumulation_steps<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb10-148"><a href="#cb10-148" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">2e-4</span>,</span>
<span id="cb10-149"><a href="#cb10-149" aria-hidden="true" tabindex="-1"></a>    lr_scheduler_type<span class="op">=</span><span class="st">"cosine"</span>,</span>
<span id="cb10-150"><a href="#cb10-150" aria-hidden="true" tabindex="-1"></a>    warmup_steps<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb10-151"><a href="#cb10-151" aria-hidden="true" tabindex="-1"></a>    logging_steps<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb10-152"><a href="#cb10-152" aria-hidden="true" tabindex="-1"></a>    save_steps<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb10-153"><a href="#cb10-153" aria-hidden="true" tabindex="-1"></a>    eval_steps<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb10-154"><a href="#cb10-154" aria-hidden="true" tabindex="-1"></a>    eval_strategy<span class="op">=</span><span class="st">"steps"</span>,</span>
<span id="cb10-155"><a href="#cb10-155" aria-hidden="true" tabindex="-1"></a>    save_strategy<span class="op">=</span><span class="st">"steps"</span>,</span>
<span id="cb10-156"><a href="#cb10-156" aria-hidden="true" tabindex="-1"></a>    load_best_model_at_end<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb10-157"><a href="#cb10-157" aria-hidden="true" tabindex="-1"></a>    metric_for_best_model<span class="op">=</span><span class="st">"eval_loss"</span>,</span>
<span id="cb10-158"><a href="#cb10-158" aria-hidden="true" tabindex="-1"></a>    greater_is_better<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb10-159"><a href="#cb10-159" aria-hidden="true" tabindex="-1"></a>    fp16<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb10-160"><a href="#cb10-160" aria-hidden="true" tabindex="-1"></a>    report_to<span class="op">=</span><span class="st">"none"</span>,</span>
<span id="cb10-161"><a href="#cb10-161" aria-hidden="true" tabindex="-1"></a>    save_total_limit<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb10-162"><a href="#cb10-162" aria-hidden="true" tabindex="-1"></a>    remove_unused_columns<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb10-163"><a href="#cb10-163" aria-hidden="true" tabindex="-1"></a>    gradient_checkpointing<span class="op">=</span><span class="va">True</span></span>
<span id="cb10-164"><a href="#cb10-164" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-165"><a href="#cb10-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-166"><a href="#cb10-166" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training configuration:"</span>)</span>
<span id="cb10-167"><a href="#cb10-167" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Epochs: </span><span class="sc">{</span>training_args<span class="sc">.</span>num_train_epochs<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-168"><a href="#cb10-168" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Batch size: </span><span class="sc">{</span>training_args<span class="sc">.</span>per_device_train_batch_size<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-169"><a href="#cb10-169" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Gradient accumulation: </span><span class="sc">{</span>training_args<span class="sc">.</span>gradient_accumulation_steps<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-170"><a href="#cb10-170" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Effective batch size: </span><span class="sc">{</span>training_args<span class="sc">.</span>per_device_train_batch_size <span class="op">*</span> training_args<span class="sc">.</span>gradient_accumulation_steps<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-171"><a href="#cb10-171" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Learning rate: </span><span class="sc">{</span>training_args<span class="sc">.</span>learning_rate<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-172"><a href="#cb10-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-173"><a href="#cb10-173" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">7. Initializing trainer..."</span>)</span>
<span id="cb10-174"><a href="#cb10-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-175"><a href="#cb10-175" aria-hidden="true" tabindex="-1"></a>data_collator <span class="op">=</span> DataCollatorForLanguageModeling(</span>
<span id="cb10-176"><a href="#cb10-176" aria-hidden="true" tabindex="-1"></a>    tokenizer<span class="op">=</span>tokenizer,</span>
<span id="cb10-177"><a href="#cb10-177" aria-hidden="true" tabindex="-1"></a>    mlm<span class="op">=</span><span class="va">False</span></span>
<span id="cb10-178"><a href="#cb10-178" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-179"><a href="#cb10-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-180"><a href="#cb10-180" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(</span>
<span id="cb10-181"><a href="#cb10-181" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model,</span>
<span id="cb10-182"><a href="#cb10-182" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>training_args,</span>
<span id="cb10-183"><a href="#cb10-183" aria-hidden="true" tabindex="-1"></a>    train_dataset<span class="op">=</span>train_dataset,</span>
<span id="cb10-184"><a href="#cb10-184" aria-hidden="true" tabindex="-1"></a>    eval_dataset<span class="op">=</span>val_dataset,</span>
<span id="cb10-185"><a href="#cb10-185" aria-hidden="true" tabindex="-1"></a>    data_collator<span class="op">=</span>data_collator</span>
<span id="cb10-186"><a href="#cb10-186" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-187"><a href="#cb10-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-188"><a href="#cb10-188" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Trainer initialized successfully!"</span>)</span>
<span id="cb10-189"><a href="#cb10-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-190"><a href="#cb10-190" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">8. Starting training..."</span>)</span>
<span id="cb10-191"><a href="#cb10-191" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb10-192"><a href="#cb10-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-193"><a href="#cb10-193" aria-hidden="true" tabindex="-1"></a>trainer.train()</span>
<span id="cb10-194"><a href="#cb10-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-195"><a href="#cb10-195" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb10-196"><a href="#cb10-196" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training complete!"</span>)</span>
<span id="cb10-197"><a href="#cb10-197" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb10-198"><a href="#cb10-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-199"><a href="#cb10-199" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">9. Saving final model..."</span>)</span>
<span id="cb10-200"><a href="#cb10-200" aria-hidden="true" tabindex="-1"></a>model.save_pretrained(<span class="ss">f"</span><span class="sc">{</span>OUTPUT_DIR<span class="sc">}</span><span class="ss">/final_model"</span>)</span>
<span id="cb10-201"><a href="#cb10-201" aria-hidden="true" tabindex="-1"></a>tokenizer.save_pretrained(<span class="ss">f"</span><span class="sc">{</span>OUTPUT_DIR<span class="sc">}</span><span class="ss">/final_model"</span>)</span>
<span id="cb10-202"><a href="#cb10-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-203"><a href="#cb10-203" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model saved to: </span><span class="sc">{</span>OUTPUT_DIR<span class="sc">}</span><span class="ss">/final_model"</span>)</span>
<span id="cb10-204"><a href="#cb10-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-205"><a href="#cb10-205" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">10. Final evaluation metrics:"</span>)</span>
<span id="cb10-206"><a href="#cb10-206" aria-hidden="true" tabindex="-1"></a>eval_results <span class="op">=</span> trainer.evaluate()</span>
<span id="cb10-207"><a href="#cb10-207" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> key, value <span class="kw">in</span> eval_results.items():</span>
<span id="cb10-208"><a href="#cb10-208" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>key<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>value<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb10-209"><a href="#cb10-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-210"><a href="#cb10-210" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb10-211"><a href="#cb10-211" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Fine-tuning pipeline complete!"</span>)</span>
<span id="cb10-212"><a href="#cb10-212" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="model-testing" class="level4" data-number="5.3.3.2">
<h4 data-number="5.3.3.2" class="anchored" data-anchor-id="model-testing"><span class="header-section-number">5.3.3.2</span> Model Testing:</h4>
<div class="sourceCode" id="cb11"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># High Five Model Testing Bash Script</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --job-name=test_high_five_model</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --partition=gpu</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --nodes=1</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --ntasks-per-node=1</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --cpus-per-task=4</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --mem=32G</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --gres=gpu:1</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --time=1:00:00</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --output=logs/test_high_five_%j.out</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --error=logs/test_high_five_%j.err</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --mail-type=</span><span class="re">END</span><span class="co">,FAIL</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --mail-user=starwill16@gmail.com</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a><span class="ex">module</span> purge</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a><span class="ex">module</span> load Python/3.12.3-GCCcore-13.3.0</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a><span class="ex">module</span> load CUDA/12.3.0</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a><span class="bu">source</span> /home/gcsc563_01/CSC463/conlingo/llama_env/bin/activate</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">HF_HOME</span><span class="op">=</span>/home/gcsc563_01/.cache/huggingface</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">HUGGINGFACE_HUB_CACHE</span><span class="op">=</span>/home/gcsc563_01/.cache/huggingface</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> /home/gcsc563_01/conlingo</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> scripts/training/run_high_five_model.py</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a><span class="ex">deactivate</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># High Five Model Testing Python Script</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co">#!/usr/bin/env python3</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co">Test the fine-tuned High Five model</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="co">Loads the model and runs sample inference</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModelForCausalLM</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> peft <span class="im">import</span> PeftModel</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"High Five Model Inference Test"</span>)</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Paths</span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>BASE_MODEL_NAME <span class="op">=</span> <span class="st">"meta-llama/Meta-Llama-3-8B-Instruct"</span></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>FINETUNED_MODEL_PATH <span class="op">=</span> <span class="st">"/home/gcsc563_01/conlingo/models/finetuned/high_five/final_model"</span></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">1. Loading tokenizer..."</span>)</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(FINETUNED_MODEL_PATH)</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"✅ Tokenizer loaded"</span>)</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">2. Loading base model..."</span>)</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>base_model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>    BASE_MODEL_NAME,</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>    device_map<span class="op">=</span><span class="st">"auto"</span>,</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>    torch_dtype<span class="op">=</span>torch.float16,</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>    low_cpu_mem_usage<span class="op">=</span><span class="va">True</span></span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"✅ Base model loaded"</span>)</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">3. Loading fine-tuned LoRA adapters..."</span>)</span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> PeftModel.from_pretrained(base_model, FINETUNED_MODEL_PATH)</span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"✅ Fine-tuned model loaded"</span>)</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">4. Running test inference..."</span>)</span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Test questions</span></span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a>test_questions <span class="op">=</span> [</span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How can Diwali and Christmas both symbolize the victory of light over darkness in their respective traditions?"</span>,</span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a>    <span class="st">"What themes of renewal in Diwali resonate with the idea of rebirth in Christian theology?"</span>,</span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How might Indian Christians use Diwali to express biblical messages of hope?"</span>,</span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How can Christian schools acknowledge Diwali without compromising faith boundaries?"</span>,</span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a>    <span class="st">"What sensitivities should pastors consider when mentioning Hindu deities in Christmas homilies?"</span>,</span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How can Christian youth groups create shared Diwali–Christmas community service projects?"</span>,</span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How can churches ensure caste-neutral seating and participation during worship?"</span>,</span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a>    <span class="st">"What examples of caste inclusion can be found in the life of Jesus?"</span>,</span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a>    <span class="st">"What are sensitive ways to discuss “the least of these” without reinforcing caste bias?"</span>,</span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Why might some Christians still use caste surnames, and how should this be discussed?"</span>,</span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a>    <span class="st">"What is a sensitive way to discuss Jesus’ association with marginalized groups?"</span>,</span>
<span id="cb12-54"><a href="#cb12-54" aria-hidden="true" tabindex="-1"></a>    <span class="st">"What is an inclusive way to discuss Krishna and Christ comparisons in academia?"</span>,</span>
<span id="cb12-55"><a href="#cb12-55" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How can Hindu concepts of karma be reconciled with Christian grace in conversation?"</span>,</span>
<span id="cb12-56"><a href="#cb12-56" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How can Christian missions emphasize shared moral principles rather than conversion?"</span>,</span>
<span id="cb12-57"><a href="#cb12-57" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How does food during Christmas vary regionally across India?"</span>,</span>
<span id="cb12-58"><a href="#cb12-58" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How does regional folklore shape Christian storytelling traditions?"</span>,</span>
<span id="cb12-59"><a href="#cb12-59" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How do Indian Christian elders respond to youth questioning of denominational traditions?"</span>,</span>
<span id="cb12-60"><a href="#cb12-60" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How can faith-based education evolve to reach younger, tech-savvy Christians?"</span>,</span>
<span id="cb12-61"><a href="#cb12-61" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How can liturgy adapt to youth participation without losing sacredness?"</span>,</span>
<span id="cb12-62"><a href="#cb12-62" aria-hidden="true" tabindex="-1"></a>    <span class="st">"How do young Christians view service and mission in a modern Indian context?"</span></span>
<span id="cb12-63"><a href="#cb12-63" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb12-64"><a href="#cb12-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-65"><a href="#cb12-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-66"><a href="#cb12-66" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, question <span class="kw">in</span> <span class="bu">enumerate</span>(test_questions, <span class="dv">1</span>):</span>
<span id="cb12-67"><a href="#cb12-67" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">[Test </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">]"</span>)</span>
<span id="cb12-68"><a href="#cb12-68" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Question: </span><span class="sc">{</span>question<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-69"><a href="#cb12-69" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-70"><a href="#cb12-70" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Format prompt</span></span>
<span id="cb12-71"><a href="#cb12-71" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">=</span> <span class="ss">f"### Question:</span><span class="ch">\n</span><span class="sc">{</span>question<span class="sc">}</span><span class="ch">\n\n</span><span class="ss">### Answer:</span><span class="ch">\n</span><span class="ss">"</span></span>
<span id="cb12-72"><a href="#cb12-72" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-73"><a href="#cb12-73" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Tokenize</span></span>
<span id="cb12-74"><a href="#cb12-74" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> tokenizer(prompt, return_tensors<span class="op">=</span><span class="st">"pt"</span>).to(model.device)</span>
<span id="cb12-75"><a href="#cb12-75" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-76"><a href="#cb12-76" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate</span></span>
<span id="cb12-77"><a href="#cb12-77" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb12-78"><a href="#cb12-78" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> model.generate(</span>
<span id="cb12-79"><a href="#cb12-79" aria-hidden="true" tabindex="-1"></a>            <span class="op">**</span>inputs,</span>
<span id="cb12-80"><a href="#cb12-80" aria-hidden="true" tabindex="-1"></a>            max_new_tokens<span class="op">=</span><span class="dv">150</span>,</span>
<span id="cb12-81"><a href="#cb12-81" aria-hidden="true" tabindex="-1"></a>            temperature<span class="op">=</span><span class="fl">0.7</span>,</span>
<span id="cb12-82"><a href="#cb12-82" aria-hidden="true" tabindex="-1"></a>            do_sample<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-83"><a href="#cb12-83" aria-hidden="true" tabindex="-1"></a>            top_p<span class="op">=</span><span class="fl">0.9</span>,</span>
<span id="cb12-84"><a href="#cb12-84" aria-hidden="true" tabindex="-1"></a>            pad_token_id<span class="op">=</span>tokenizer.eos_token_id</span>
<span id="cb12-85"><a href="#cb12-85" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb12-86"><a href="#cb12-86" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-87"><a href="#cb12-87" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Decode</span></span>
<span id="cb12-88"><a href="#cb12-88" aria-hidden="true" tabindex="-1"></a>    response <span class="op">=</span> tokenizer.decode(outputs[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-89"><a href="#cb12-89" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-90"><a href="#cb12-90" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Extract only the answer part (after "### Answer:")</span></span>
<span id="cb12-91"><a href="#cb12-91" aria-hidden="true" tabindex="-1"></a>    answer <span class="op">=</span> response.split(<span class="st">"### Answer:"</span>)[<span class="op">-</span><span class="dv">1</span>].strip()</span>
<span id="cb12-92"><a href="#cb12-92" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-93"><a href="#cb12-93" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Answer: </span><span class="sc">{</span>answer<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-94"><a href="#cb12-94" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"-"</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb12-95"><a href="#cb12-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-96"><a href="#cb12-96" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb12-97"><a href="#cb12-97" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Inference test complete!"</span>)</span>
<span id="cb12-98"><a href="#cb12-98" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now, after me and Moses’ analyses and fine-tuning process, evrey model is set for grading and comparison with the original ConLingo model.</p>
<p><br></p>
<p><br></p>
<p><br></p>
<p><br></p>
</section>
</section>
</section>
<section id="suwilanji-mwanza-deliverables" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="suwilanji-mwanza-deliverables"><span class="header-section-number">5.4</span> Suwilanji Mwanza Deliverables</h2>
<section id="overview-of-the-week-4-deliverable" class="level3" data-number="5.4.1">
<h3 data-number="5.4.1" class="anchored" data-anchor-id="overview-of-the-week-4-deliverable"><span class="header-section-number">5.4.1</span> Overview of the Week 4 Deliverable</h3>
<p>This week, my deliverable was to provide visualizations of the different models created for each dataset and then visualize their performance, which was evaluated by Rohan. Additionally, examine the datasets that contributed to a mode’s negative performance, if any, alongside Rohan.</p>
</section>
<section id="week-4-deliverables" class="level3" data-number="5.4.2">
<h3 data-number="5.4.2" class="anchored" data-anchor-id="week-4-deliverables"><span class="header-section-number">5.4.2</span> Week 4 Deliverables:</h3>
<ul>
<li><p>Create comprehensive visualizations about the performance of each of the 7 models, as well as the 2 combined models.</p></li>
<li><p>Examine the datasets that contributed to the model’s poor performance and propose potential reasons for this.</p></li>
<li><p>Create the full presentation that will be used as the Week 4 presentation.</p></li>
</ul>
</section>
<section id="what-was-accomplished" class="level3" data-number="5.4.3">
<h3 data-number="5.4.3" class="anchored" data-anchor-id="what-was-accomplished"><span class="header-section-number">5.4.3</span> What Was Accomplished:</h3>
</section>
<section id="model-visualizations" class="level3" data-number="5.4.4">
<h3 data-number="5.4.4" class="anchored" data-anchor-id="model-visualizations"><span class="header-section-number">5.4.4</span> 1. Model Visualizations</h3>
<p>In accordance with the CSI scoring that Rohan provided earlier, he used it to evaluate the model’s performance. This model scores an AI’s response based on accuracy, tone, context, and empathy. This all contributes to the final CSI score.</p>
<p>The two models’ results, which were ready for me to visualize this week, were the Constitution Model and the Superstitions Model. I used Excel for this process.</p>
<p>Rohan graded the model’s response on a scale of 20 questions that he also created earlier. Each question received a number for it to be easily plotted:</p>
<ol type="1">
<li>How can Diwali and Christmas both symbolize the victory of light over darkness in their respective traditions?&nbsp;</li>
<li>What themes of renewal in Diwali resonate with the idea of rebirth in Christian theology?&nbsp;</li>
<li>How might Indian Christians use Diwali to express biblical messages of hope?&nbsp;</li>
<li>How can Christian schools acknowledge Diwali without compromising faith boundaries?&nbsp;</li>
<li>What sensitivities should pastors consider when mentioning Hindu deities in Christmas homilies?</li>
<li>How can Christian youth groups create shared Diwali–Christmas community service projects?&nbsp;</li>
<li>How can churches ensure caste-neutral seating and participation during worship?&nbsp;</li>
<li>What examples of caste inclusion can be found in the life of Jesus?&nbsp;</li>
<li>What are sensitive ways to discuss “the least of these” without reinforcing caste bias?&nbsp;</li>
<li>Why might some Christians still use caste surnames, and how should this be discussed?&nbsp;</li>
<li>What is a sensitive way to discuss Jesus’ association with marginalized groups?&nbsp;</li>
<li>What is an inclusive way to discuss Krishna and Christ comparisons in academia?&nbsp;</li>
<li>How can Hindu concepts of karma be reconciled with Christian grace in conversation?</li>
<li>How can Christian missions emphasize shared moral principles rather than conversion?</li>
<li>How does food during Christmas vary regionally across India?&nbsp;&nbsp;</li>
<li>How does regional folklore shape Christian storytelling traditions?&nbsp;</li>
<li>How do Indian Christian elders respond to youth questioning of denominational traditions?&nbsp;</li>
<li>How can faith-based education evolve to reach younger, tech-savvy Christians?&nbsp;</li>
<li>How can liturgy adapt to youth participation without losing sacredness?&nbsp;</li>
<li>How do young Christians view service and mission in a modern Indian context?&nbsp;</li>
</ol>
<p><strong>Accuracy</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/week_4_1.png" class="img-fluid figure-img"></p>
<figcaption>Accuracy results</figcaption>
</figure>
</div>
<p>In this graph, the Constitution’s model performs better on average, with a perfect score of 5, and more than half of its answers achieve a perfect score.</p>
<p><strong>Tone</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/week_4_2.png" class="img-fluid figure-img"></p>
<figcaption>Tone results</figcaption>
</figure>
</div>
<p>In this graph, the Constitutions model performs better than the Superstitions data. The Superstitions model has an above-average response.</p>
<p><strong>Context</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/week_4_3.png" class="img-fluid figure-img"></p>
<figcaption>Context results</figcaption>
</figure>
</div>
<p>This graph shows that the Constitution model has a better understanding of the question’s context and thus provides more accurate answers.</p>
<p><strong>Empathy</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/week_4_4.png" class="img-fluid figure-img"></p>
<figcaption>Empathy results</figcaption>
</figure>
</div>
<p>The empathy graph also shows the Constitutions model performing significantly better than the other model, with near-perfect scores for each question.</p>
<p><strong>CSI Score</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/week_4_5.png" class="img-fluid figure-img"></p>
<figcaption>CSI Scoring results</figcaption>
</figure>
</div>
<p>It is not surprising that the overall model shows the Constitutions performed well. This graph is interpreted to mean that each score represents an average of accuracy, tone, context, and empathy for each question.</p>
</section>
<section id="negative-performing-models" class="level3" data-number="5.4.5">
<h3 data-number="5.4.5" class="anchored" data-anchor-id="negative-performing-models"><span class="header-section-number">5.4.5</span> 2. Negative Performing Models</h3>
<p>First and foremost, the fact that Rohan was the only one to grade both the model’s responses meant a heavy bias was present. Although he is from India, his notions of what he sees as accurate are unconsciously influencing his grading.</p>
<p>Additionally, the sample size for this model was too small. Only Rohan. Some areas to improve our perception of a negatively performing model include increasing our sample size to gain a better understanding of which model is truly underperforming.</p>
<p>A major flaw in this grading is that since the models were trained with supervised fine-tuning, they are only able to provide answers based on the fact that the training data, such as that of supervised learning. However, the questions that we are asking the Constitution model do not utilize the knowledge base or dataset of the constitution data, yet the trained model performs better.</p>
<p>What most likely happened is that, although the evaluation questions were unrelated to the SFT knowledge (religious questions versus the Constitution and superstition data), the Constitution-trained model was consistently judged stronger by Rohan. More plainly put, our questions unfortunately did not evaluate knowledge recall but rather cross-domain behavioral transfer. This concept falls under transfer learning, a machine learning technique in which a model trained on one task is repurposed as the foundation for a second task. (GeeksforGeeks, 2025)</p>
<p>In this case, the model was trained for the task of analyzing superstition data or constitutional data; however, during evaluation, the questions soliciting a religious answer posed a new task. What happened here is that the training data sets’ style and reasoning picked up during the training session transferred (cross-domain). SFT not only teaches content but also behavioral patterns.</p>
<p>It is possible that Rohan agreed more with the Constitution’s style that transferred to the responses being evaluated. I’d imagine this data to be more formal, neutral, and structured. In addition, it may have known how to phrase sensitive topics better, which transferred a style of the model being careful with wording, less biased, and more diplomatic, for example. The superstition’s data may have come across as less credible, as the data contains myths, folklore, and mixes belief and fiction. And thus the style transfer is less academic, more storytelling, and less objective.</p>
<p>In conclusion, what would have made this process more effective would have been to have a base model to test against these evaluation questions and truly determine how both models deviate from the base case.</p>
</section>
</section>
<section id="moses-madale-deliverables" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="moses-madale-deliverables"><span class="header-section-number">5.5</span> Moses Madale Deliverables</h2>
<p><strong>AI assistance: Claude Sonnet 4.5 was used to help me troubleshoot issues with setting up an identical virtual environment on Will’s Titan account to ensure that he can also run the fine-tuning from his account smoothly, there were issues with python package version conflicts but with assistance from Claude Sonnet, these issues were resolved more efficiently (accessed Nov, 2025).</strong></p>
<section id="model-training-and-evaluation" class="level3" data-number="5.5.1">
<h3 data-number="5.5.1" class="anchored" data-anchor-id="model-training-and-evaluation"><span class="header-section-number">5.5.1</span> Model Training and Evaluation</h3>
<section id="overview-1" class="level4" data-number="5.5.1.1">
<h4 data-number="5.5.1.1" class="anchored" data-anchor-id="overview-1"><span class="header-section-number">5.5.1.1</span> Overview</h4>
<p>Week 4 transformed the datasets collected in Week 3 into trained models ready for evaluation. The primary focus was training three distinct fine-tuned models—one each for YouTube Transcripts, Wikipedia, and TED Talks datasets—testing them on 20 culturally nuanced questions, and preparing results for comprehensive analysis. This week demonstrated that different cultural data sources produce models with varying capabilities, setting the stage for Week 5’s comparative evaluation against baseline and state-of-the-art models.</p>
</section>
<section id="training-infrastructure-and-workflow" class="level4" data-number="5.5.1.2">
<h4 data-number="5.5.1.2" class="anchored" data-anchor-id="training-infrastructure-and-workflow"><span class="header-section-number">5.5.1.2</span> Training Infrastructure and Workflow</h4>
<section id="fine-tuning-pipeline-architecture" class="level5" data-number="5.5.1.2.1">
<h5 data-number="5.5.1.2.1" class="anchored" data-anchor-id="fine-tuning-pipeline-architecture"><span class="header-section-number">5.5.1.2.1</span> Fine-Tuning Pipeline Architecture</h5>
<p>Building on Week 2’s pipeline, Week 4 established a standardized workflow for training multiple models systematically:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./Appendix/MosesMadale/img/4_1.png" class="img-fluid figure-img"></p>
<figcaption>Fine-tuning pipeline for multiple datasets</figcaption>
</figure>
</div>
<p><strong>Pipeline Stages</strong>:</p>
<ol type="1">
<li><strong>Dataset Loading</strong>: Read JSONL file with Q&amp;A pairs</li>
<li><strong>Train/Validation Split</strong>: 90/10 split with random seed for reproducibility</li>
<li><strong>Tokenization</strong>: Convert text to model-compatible format with padding</li>
<li><strong>Base Model Loading</strong>: Load LLaMA-3 8B Instruct with half-precision</li>
<li><strong>LoRA Configuration</strong>: Add trainable adapters to attention layers</li>
<li><strong>Training</strong>: Run supervised fine-tuning with gradient accumulation</li>
<li><strong>Evaluation</strong>: Assess performance on validation set</li>
<li><strong>Model Saving</strong>: Store LoRA adapters for inference</li>
</ol>
</section>
</section>
<section id="standardized-training-configuration" class="level4" data-number="5.5.1.3">
<h4 data-number="5.5.1.3" class="anchored" data-anchor-id="standardized-training-configuration"><span class="header-section-number">5.5.1.3</span> Standardized Training Configuration</h4>
<p>All three models used identical hyperparameters to ensure fair comparison:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Hyperparameter</th>
<th>Value</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Epochs</strong></td>
<td>3</td>
<td>Sufficient for convergence without overfitting</td>
</tr>
<tr class="even">
<td><strong>Batch Size</strong></td>
<td>2</td>
<td>Maximum fitting in 24 GB VRAM</td>
</tr>
<tr class="odd">
<td><strong>Gradient Accumulation</strong></td>
<td>16</td>
<td>Effective batch size of 32</td>
</tr>
<tr class="even">
<td><strong>Learning Rate</strong></td>
<td>2e-4</td>
<td>Standard for LoRA fine-tuning</td>
</tr>
<tr class="odd">
<td><strong>LR Scheduler</strong></td>
<td>Cosine</td>
<td>Gradual learning rate decay</td>
</tr>
<tr class="even">
<td><strong>Warmup Steps</strong></td>
<td>50</td>
<td>Prevents early training instability</td>
</tr>
<tr class="odd">
<td><strong>Max Sequence Length</strong></td>
<td>512 tokens</td>
<td>Accommodates most Q&amp;A pairs</td>
</tr>
<tr class="even">
<td><strong>LoRA Rank (r)</strong></td>
<td>16</td>
<td>Balance between capacity and efficiency</td>
</tr>
<tr class="odd">
<td><strong>LoRA Alpha</strong></td>
<td>32</td>
<td>Scaling factor for adapter outputs</td>
</tr>
</tbody>
</table>
<p><strong>LoRA Target Modules</strong>: - <code>q_proj</code> (Query projection) - <code>v_proj</code> (Value projection) - <code>k_proj</code> (Key projection) - <code>o_proj</code> (Output projection)</p>
<p>These attention mechanism components were selected because they capture the most information during text generation while keeping trainable parameters to just 0.17% of the total model.</p>
</section>
<section id="training-script-structure" class="level4" data-number="5.5.1.4">
<h4 data-number="5.5.1.4" class="anchored" data-anchor-id="training-script-structure"><span class="header-section-number">5.5.1.4</span> Training Script Structure</h4>
<p>Each fine-tuning script followed a consistent 10-step process. Here is the complete Wikipedia training script as an example:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co">#!/usr/bin/env python3</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co">LoRA Fine-Tuning for Wikipedia Data</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co">Fine-tunes LLaMA-3 8B on Indian Wikipedia Q&amp;A pairs</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> (</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    AutoTokenizer,</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    AutoModelForCausalLM,</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    TrainingArguments,</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    Trainer,</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    DataCollatorForLanguageModeling</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> peft <span class="im">import</span> LoraConfig, get_peft_model</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> Dataset</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Wikipedia LoRA Fine-Tuning Pipeline"</span>)</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Paths</span></span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>DATA_PATH <span class="op">=</span> <span class="st">"/home/mmadale/CSC463/conlingo/huggingface_data/indian_wikipedia/data/wikipedia_qa.jsonl"</span></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>OUTPUT_DIR <span class="op">=</span> <span class="st">"/home/mmadale/CSC463/conlingo/models/wikipedia"</span></span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>MODEL_NAME <span class="op">=</span> <span class="st">"meta-llama/Meta-Llama-3-8B-Instruct"</span></span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Create output directory</span></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>os.makedirs(OUTPUT_DIR, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">1. Loading data from: </span><span class="sc">{</span>DATA_PATH<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Load JSONL data</span></span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> []</span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(DATA_PATH, <span class="st">'r'</span>, encoding<span class="op">=</span><span class="st">'utf-8'</span>) <span class="im">as</span> f:</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> line <span class="kw">in</span> f:</span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a>        data.append(json.loads(line))</span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total examples: </span><span class="sc">{</span><span class="bu">len</span>(data)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract question-answer pairs</span></span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a>examples <span class="op">=</span> []</span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> item <span class="kw">in</span> data:</span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a>    examples.append({</span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a>        <span class="st">"question"</span>: item[<span class="st">"question"</span>],</span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a>        <span class="st">"answer"</span>: item[<span class="st">"answer"</span>]</span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Split into train/validation (90/10)</span></span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a>train_data, val_data <span class="op">=</span> train_test_split(examples, test_size<span class="op">=</span><span class="fl">0.1</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training examples: </span><span class="sc">{</span><span class="bu">len</span>(train_data)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Validation examples: </span><span class="sc">{</span><span class="bu">len</span>(val_data)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-58"><a href="#cb13-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-59"><a href="#cb13-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">2. Loading tokenizer..."</span>)</span>
<span id="cb13-60"><a href="#cb13-60" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(MODEL_NAME)</span>
<span id="cb13-61"><a href="#cb13-61" aria-hidden="true" tabindex="-1"></a>tokenizer.pad_token <span class="op">=</span> tokenizer.eos_token</span>
<span id="cb13-62"><a href="#cb13-62" aria-hidden="true" tabindex="-1"></a>tokenizer.padding_side <span class="op">=</span> <span class="st">"right"</span></span>
<span id="cb13-63"><a href="#cb13-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-64"><a href="#cb13-64" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Tokenizer loaded: </span><span class="sc">{</span>tokenizer<span class="sc">.</span><span class="va">__class__</span><span class="sc">.</span><span class="va">__name__</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-65"><a href="#cb13-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-66"><a href="#cb13-66" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">3. Preparing datasets..."</span>)</span>
<span id="cb13-67"><a href="#cb13-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-68"><a href="#cb13-68" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> format_instruction(example):</span>
<span id="cb13-69"><a href="#cb13-69" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Format question-answer pair for training"""</span></span>
<span id="cb13-70"><a href="#cb13-70" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> <span class="ss">f"### Question:</span><span class="ch">\n</span><span class="sc">{</span>example[<span class="st">'question'</span>]<span class="sc">}</span><span class="ch">\n\n</span><span class="ss">### Answer:</span><span class="ch">\n</span><span class="sc">{</span>example[<span class="st">'answer'</span>]<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb13-71"><a href="#cb13-71" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> text</span>
<span id="cb13-72"><a href="#cb13-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-73"><a href="#cb13-73" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize_function(example):</span>
<span id="cb13-74"><a href="#cb13-74" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Tokenize examples with padding and truncation"""</span></span>
<span id="cb13-75"><a href="#cb13-75" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> format_instruction(example)</span>
<span id="cb13-76"><a href="#cb13-76" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-77"><a href="#cb13-77" aria-hidden="true" tabindex="-1"></a>    tokenized <span class="op">=</span> tokenizer(</span>
<span id="cb13-78"><a href="#cb13-78" aria-hidden="true" tabindex="-1"></a>        text,</span>
<span id="cb13-79"><a href="#cb13-79" aria-hidden="true" tabindex="-1"></a>        truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb13-80"><a href="#cb13-80" aria-hidden="true" tabindex="-1"></a>        max_length<span class="op">=</span><span class="dv">512</span>,</span>
<span id="cb13-81"><a href="#cb13-81" aria-hidden="true" tabindex="-1"></a>        padding<span class="op">=</span><span class="st">"max_length"</span>,</span>
<span id="cb13-82"><a href="#cb13-82" aria-hidden="true" tabindex="-1"></a>        return_tensors<span class="op">=</span><span class="va">None</span></span>
<span id="cb13-83"><a href="#cb13-83" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb13-84"><a href="#cb13-84" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-85"><a href="#cb13-85" aria-hidden="true" tabindex="-1"></a>    tokenized[<span class="st">"labels"</span>] <span class="op">=</span> tokenized[<span class="st">"input_ids"</span>].copy()</span>
<span id="cb13-86"><a href="#cb13-86" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokenized</span>
<span id="cb13-87"><a href="#cb13-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-88"><a href="#cb13-88" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to HuggingFace Dataset format</span></span>
<span id="cb13-89"><a href="#cb13-89" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> Dataset.from_list(train_data)</span>
<span id="cb13-90"><a href="#cb13-90" aria-hidden="true" tabindex="-1"></a>val_dataset <span class="op">=</span> Dataset.from_list(val_data)</span>
<span id="cb13-91"><a href="#cb13-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-92"><a href="#cb13-92" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenize</span></span>
<span id="cb13-93"><a href="#cb13-93" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Tokenizing training data..."</span>)</span>
<span id="cb13-94"><a href="#cb13-94" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> train_dataset.<span class="bu">map</span>(</span>
<span id="cb13-95"><a href="#cb13-95" aria-hidden="true" tabindex="-1"></a>    tokenize_function,</span>
<span id="cb13-96"><a href="#cb13-96" aria-hidden="true" tabindex="-1"></a>    remove_columns<span class="op">=</span>train_dataset.column_names</span>
<span id="cb13-97"><a href="#cb13-97" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-98"><a href="#cb13-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-99"><a href="#cb13-99" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Tokenizing validation data..."</span>)</span>
<span id="cb13-100"><a href="#cb13-100" aria-hidden="true" tabindex="-1"></a>val_dataset <span class="op">=</span> val_dataset.<span class="bu">map</span>(</span>
<span id="cb13-101"><a href="#cb13-101" aria-hidden="true" tabindex="-1"></a>    tokenize_function,</span>
<span id="cb13-102"><a href="#cb13-102" aria-hidden="true" tabindex="-1"></a>    remove_columns<span class="op">=</span>val_dataset.column_names</span>
<span id="cb13-103"><a href="#cb13-103" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-104"><a href="#cb13-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-105"><a href="#cb13-105" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training dataset size: </span><span class="sc">{</span><span class="bu">len</span>(train_dataset)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-106"><a href="#cb13-106" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Validation dataset size: </span><span class="sc">{</span><span class="bu">len</span>(val_dataset)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-107"><a href="#cb13-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-108"><a href="#cb13-108" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">4. Loading base model..."</span>)</span>
<span id="cb13-109"><a href="#cb13-109" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb13-110"><a href="#cb13-110" aria-hidden="true" tabindex="-1"></a>    MODEL_NAME,</span>
<span id="cb13-111"><a href="#cb13-111" aria-hidden="true" tabindex="-1"></a>    device_map<span class="op">=</span><span class="st">"auto"</span>,</span>
<span id="cb13-112"><a href="#cb13-112" aria-hidden="true" tabindex="-1"></a>    torch_dtype<span class="op">=</span>torch.float16,</span>
<span id="cb13-113"><a href="#cb13-113" aria-hidden="true" tabindex="-1"></a>    low_cpu_mem_usage<span class="op">=</span><span class="va">True</span></span>
<span id="cb13-114"><a href="#cb13-114" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-115"><a href="#cb13-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-116"><a href="#cb13-116" aria-hidden="true" tabindex="-1"></a>model.gradient_checkpointing_enable()</span>
<span id="cb13-117"><a href="#cb13-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-118"><a href="#cb13-118" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model loaded: </span><span class="sc">{</span>model<span class="sc">.</span><span class="va">__class__</span><span class="sc">.</span><span class="va">__name__</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-119"><a href="#cb13-119" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model parameters: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters())<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb13-120"><a href="#cb13-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-121"><a href="#cb13-121" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">5. Configuring LoRA..."</span>)</span>
<span id="cb13-122"><a href="#cb13-122" aria-hidden="true" tabindex="-1"></a>lora_config <span class="op">=</span> LoraConfig(</span>
<span id="cb13-123"><a href="#cb13-123" aria-hidden="true" tabindex="-1"></a>    r<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb13-124"><a href="#cb13-124" aria-hidden="true" tabindex="-1"></a>    lora_alpha<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb13-125"><a href="#cb13-125" aria-hidden="true" tabindex="-1"></a>    target_modules<span class="op">=</span>[<span class="st">"q_proj"</span>, <span class="st">"v_proj"</span>, <span class="st">"k_proj"</span>, <span class="st">"o_proj"</span>],</span>
<span id="cb13-126"><a href="#cb13-126" aria-hidden="true" tabindex="-1"></a>    lora_dropout<span class="op">=</span><span class="fl">0.05</span>,</span>
<span id="cb13-127"><a href="#cb13-127" aria-hidden="true" tabindex="-1"></a>    bias<span class="op">=</span><span class="st">"none"</span>,</span>
<span id="cb13-128"><a href="#cb13-128" aria-hidden="true" tabindex="-1"></a>    task_type<span class="op">=</span><span class="st">"CAUSAL_LM"</span></span>
<span id="cb13-129"><a href="#cb13-129" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-130"><a href="#cb13-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-131"><a href="#cb13-131" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> get_peft_model(model, lora_config)</span>
<span id="cb13-132"><a href="#cb13-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-133"><a href="#cb13-133" aria-hidden="true" tabindex="-1"></a>trainable_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters() <span class="cf">if</span> p.requires_grad)</span>
<span id="cb13-134"><a href="#cb13-134" aria-hidden="true" tabindex="-1"></a>total_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters())</span>
<span id="cb13-135"><a href="#cb13-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-136"><a href="#cb13-136" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"LoRA configured successfully"</span>)</span>
<span id="cb13-137"><a href="#cb13-137" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Trainable parameters: </span><span class="sc">{</span>trainable_params<span class="sc">:,}</span><span class="ss"> (</span><span class="sc">{</span><span class="dv">100</span><span class="op">*</span>trainable_params<span class="op">/</span>total_params<span class="sc">:.4f}</span><span class="ss">%)"</span>)</span>
<span id="cb13-138"><a href="#cb13-138" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total parameters: </span><span class="sc">{</span>total_params<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb13-139"><a href="#cb13-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-140"><a href="#cb13-140" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">6. Setting up training arguments..."</span>)</span>
<span id="cb13-141"><a href="#cb13-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-142"><a href="#cb13-142" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments(</span>
<span id="cb13-143"><a href="#cb13-143" aria-hidden="true" tabindex="-1"></a>    output_dir<span class="op">=</span>OUTPUT_DIR,</span>
<span id="cb13-144"><a href="#cb13-144" aria-hidden="true" tabindex="-1"></a>    num_train_epochs<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb13-145"><a href="#cb13-145" aria-hidden="true" tabindex="-1"></a>    per_device_train_batch_size<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb13-146"><a href="#cb13-146" aria-hidden="true" tabindex="-1"></a>    per_device_eval_batch_size<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb13-147"><a href="#cb13-147" aria-hidden="true" tabindex="-1"></a>    gradient_accumulation_steps<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb13-148"><a href="#cb13-148" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">2e-4</span>,</span>
<span id="cb13-149"><a href="#cb13-149" aria-hidden="true" tabindex="-1"></a>    lr_scheduler_type<span class="op">=</span><span class="st">"cosine"</span>,</span>
<span id="cb13-150"><a href="#cb13-150" aria-hidden="true" tabindex="-1"></a>    warmup_steps<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb13-151"><a href="#cb13-151" aria-hidden="true" tabindex="-1"></a>    logging_steps<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb13-152"><a href="#cb13-152" aria-hidden="true" tabindex="-1"></a>    save_steps<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb13-153"><a href="#cb13-153" aria-hidden="true" tabindex="-1"></a>    eval_steps<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb13-154"><a href="#cb13-154" aria-hidden="true" tabindex="-1"></a>    eval_strategy<span class="op">=</span><span class="st">"steps"</span>,</span>
<span id="cb13-155"><a href="#cb13-155" aria-hidden="true" tabindex="-1"></a>    save_strategy<span class="op">=</span><span class="st">"steps"</span>,</span>
<span id="cb13-156"><a href="#cb13-156" aria-hidden="true" tabindex="-1"></a>    load_best_model_at_end<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb13-157"><a href="#cb13-157" aria-hidden="true" tabindex="-1"></a>    metric_for_best_model<span class="op">=</span><span class="st">"eval_loss"</span>,</span>
<span id="cb13-158"><a href="#cb13-158" aria-hidden="true" tabindex="-1"></a>    greater_is_better<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb13-159"><a href="#cb13-159" aria-hidden="true" tabindex="-1"></a>    fp16<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb13-160"><a href="#cb13-160" aria-hidden="true" tabindex="-1"></a>    report_to<span class="op">=</span><span class="st">"none"</span>,</span>
<span id="cb13-161"><a href="#cb13-161" aria-hidden="true" tabindex="-1"></a>    save_total_limit<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb13-162"><a href="#cb13-162" aria-hidden="true" tabindex="-1"></a>    remove_unused_columns<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb13-163"><a href="#cb13-163" aria-hidden="true" tabindex="-1"></a>    gradient_checkpointing<span class="op">=</span><span class="va">True</span></span>
<span id="cb13-164"><a href="#cb13-164" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-165"><a href="#cb13-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-166"><a href="#cb13-166" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training configuration:"</span>)</span>
<span id="cb13-167"><a href="#cb13-167" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Epochs: </span><span class="sc">{</span>training_args<span class="sc">.</span>num_train_epochs<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-168"><a href="#cb13-168" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Batch size: </span><span class="sc">{</span>training_args<span class="sc">.</span>per_device_train_batch_size<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-169"><a href="#cb13-169" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Gradient accumulation: </span><span class="sc">{</span>training_args<span class="sc">.</span>gradient_accumulation_steps<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-170"><a href="#cb13-170" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Effective batch size: </span><span class="sc">{</span>training_args<span class="sc">.</span>per_device_train_batch_size <span class="op">*</span> training_args<span class="sc">.</span>gradient_accumulation_steps<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-171"><a href="#cb13-171" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Learning rate: </span><span class="sc">{</span>training_args<span class="sc">.</span>learning_rate<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-172"><a href="#cb13-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-173"><a href="#cb13-173" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">7. Initializing trainer..."</span>)</span>
<span id="cb13-174"><a href="#cb13-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-175"><a href="#cb13-175" aria-hidden="true" tabindex="-1"></a>data_collator <span class="op">=</span> DataCollatorForLanguageModeling(</span>
<span id="cb13-176"><a href="#cb13-176" aria-hidden="true" tabindex="-1"></a>    tokenizer<span class="op">=</span>tokenizer,</span>
<span id="cb13-177"><a href="#cb13-177" aria-hidden="true" tabindex="-1"></a>    mlm<span class="op">=</span><span class="va">False</span></span>
<span id="cb13-178"><a href="#cb13-178" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-179"><a href="#cb13-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-180"><a href="#cb13-180" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(</span>
<span id="cb13-181"><a href="#cb13-181" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model,</span>
<span id="cb13-182"><a href="#cb13-182" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>training_args,</span>
<span id="cb13-183"><a href="#cb13-183" aria-hidden="true" tabindex="-1"></a>    train_dataset<span class="op">=</span>train_dataset,</span>
<span id="cb13-184"><a href="#cb13-184" aria-hidden="true" tabindex="-1"></a>    eval_dataset<span class="op">=</span>val_dataset,</span>
<span id="cb13-185"><a href="#cb13-185" aria-hidden="true" tabindex="-1"></a>    data_collator<span class="op">=</span>data_collator</span>
<span id="cb13-186"><a href="#cb13-186" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-187"><a href="#cb13-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-188"><a href="#cb13-188" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Trainer initialized successfully!"</span>)</span>
<span id="cb13-189"><a href="#cb13-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-190"><a href="#cb13-190" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">8. Starting training..."</span>)</span>
<span id="cb13-191"><a href="#cb13-191" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb13-192"><a href="#cb13-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-193"><a href="#cb13-193" aria-hidden="true" tabindex="-1"></a>trainer.train()</span>
<span id="cb13-194"><a href="#cb13-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-195"><a href="#cb13-195" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb13-196"><a href="#cb13-196" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training complete!"</span>)</span>
<span id="cb13-197"><a href="#cb13-197" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb13-198"><a href="#cb13-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-199"><a href="#cb13-199" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">9. Saving final model..."</span>)</span>
<span id="cb13-200"><a href="#cb13-200" aria-hidden="true" tabindex="-1"></a>model.save_pretrained(<span class="ss">f"</span><span class="sc">{</span>OUTPUT_DIR<span class="sc">}</span><span class="ss">/final_model"</span>)</span>
<span id="cb13-201"><a href="#cb13-201" aria-hidden="true" tabindex="-1"></a>tokenizer.save_pretrained(<span class="ss">f"</span><span class="sc">{</span>OUTPUT_DIR<span class="sc">}</span><span class="ss">/final_model"</span>)</span>
<span id="cb13-202"><a href="#cb13-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-203"><a href="#cb13-203" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model saved to: </span><span class="sc">{</span>OUTPUT_DIR<span class="sc">}</span><span class="ss">/final_model"</span>)</span>
<span id="cb13-204"><a href="#cb13-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-205"><a href="#cb13-205" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">10. Final evaluation metrics:"</span>)</span>
<span id="cb13-206"><a href="#cb13-206" aria-hidden="true" tabindex="-1"></a>eval_results <span class="op">=</span> trainer.evaluate()</span>
<span id="cb13-207"><a href="#cb13-207" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> key, value <span class="kw">in</span> eval_results.items():</span>
<span id="cb13-208"><a href="#cb13-208" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>key<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>value<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb13-209"><a href="#cb13-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-210"><a href="#cb13-210" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb13-211"><a href="#cb13-211" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Fine-tuning pipeline complete!"</span>)</span>
<span id="cb13-212"><a href="#cb13-212" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The same script structure was adapted for TED Talks and YouTube Transcripts by simply changing the <code>DATA_PATH</code> and <code>OUTPUT_DIR</code> variables. This standardization enabled rapid iteration and consistent results across different datasets.</p>
</section>
</section>
<section id="model-training-results" class="level3" data-number="5.5.2">
<h3 data-number="5.5.2" class="anchored" data-anchor-id="model-training-results"><span class="header-section-number">5.5.2</span> Model Training Results</h3>
<section id="wikipedia-model-training" class="level4" data-number="5.5.2.1">
<h4 data-number="5.5.2.1" class="anchored" data-anchor-id="wikipedia-model-training"><span class="header-section-number">5.5.2.1</span> Wikipedia Model Training</h4>
<p><strong>Dataset Statistics</strong>: - Total examples: 500 - Training set: 450 (90%) - Validation set: 50 (10%) - Training duration: 8 minutes 29 seconds</p>
<p><strong>Training Progress</strong>:</p>
<pre><code>============================================================
Wikipedia LoRA Fine-Tuning Pipeline
============================================================

1. Loading data from: .../indian_wikipedia/data/wikipedia_qa.jsonl
Total examples: 500
Training examples: 450
Validation examples: 50

2. Loading tokenizer...
Tokenizer loaded: PreTrainedTokenizerFast

3. Preparing datasets...
Training dataset size: 450
Validation dataset size: 50

4. Loading base model...
Model loaded: LlamaForCausalLM
Model parameters: 8,030,261,248

5. Configuring LoRA...
LoRA configured successfully
Trainable parameters: 13,631,488 (0.1695%)
Total parameters: 8,043,892,736

6. Setting up training arguments...
Training configuration:
  Epochs: 3
  Batch size: 2
  Gradient accumulation: 16
  Effective batch size: 32
  Learning rate: 0.0002

8. Starting training...
============================================================
{'loss': 2.433, 'grad_norm': 1.730, 'learning_rate': 3.6e-05, 'epoch': 0.71}
{'loss': 1.945, 'grad_norm': 2.509, 'learning_rate': 7.6e-05, 'epoch': 1.36}
{'loss': 1.468, 'grad_norm': 4.694, 'learning_rate': 0.000116, 'epoch': 2.0}
{'loss': 1.281, 'grad_norm': 0.774, 'learning_rate': 0.000156, 'epoch': 2.71}

============================================================
Training complete!
============================================================

10. Final evaluation metrics:
  eval_loss: 1.2456
  eval_runtime: 5.8086
  epoch: 3.0000</code></pre>
<p><strong>Performance Analysis</strong>:</p>
<p>The Wikipedia model achieved the <strong>lowest final loss (1.2456)</strong> among all three models, indicating: - Excellent convergence on encyclopedic content - Strong pattern recognition for factual Q&amp;A - Effective learning from well-structured Wikipedia articles</p>
<p>The loss decreased from 2.433 → 1.281 (47% reduction), demonstrating substantial learning without overfitting (validation loss 1.2456 close to final training loss 1.281).</p>
</section>
<section id="youtube-transcripts-model-training" class="level4" data-number="5.5.2.2">
<h4 data-number="5.5.2.2" class="anchored" data-anchor-id="youtube-transcripts-model-training"><span class="header-section-number">5.5.2.2</span> YouTube Transcripts Model Training</h4>
<p><strong>Dataset Statistics</strong>: - Total examples: 512 - Training set: 460 (90%) - Validation set: 52 (10%) - Training duration: 8 minutes 46 seconds</p>
<p><strong>Training Progress</strong>:</p>
<pre><code>============================================================
YouTube Transcripts LoRA Fine-Tuning Pipeline
============================================================

1. Loading data from: .../youtube_data/data/final_youtube_transcript_data.jsonl
Total examples: 512
Training examples: 460
Validation examples: 52

5. Configuring LoRA...
Trainable parameters: 13,631,488 (0.1695%)
Total parameters: 8,043,892,736

8. Starting training...
============================================================
{'loss': 3.935, 'grad_norm': 1.849, 'learning_rate': 3.6e-05, 'epoch': 0.7}
{'loss': 3.363, 'grad_norm': 2.377, 'learning_rate': 7.6e-05, 'epoch': 1.35}
{'loss': 2.618, 'grad_norm': 2.411, 'learning_rate': 0.000116, 'epoch': 2.0}
{'loss': 2.324, 'grad_norm': 1.130, 'learning_rate': 0.000156, 'epoch': 2.7}

============================================================
Training complete!
============================================================

10. Final evaluation metrics:
  eval_loss: 2.2825
  eval_runtime: 6.0515
  epoch: 3.0000</code></pre>
<p><strong>Performance Analysis</strong>:</p>
<p>The YouTube model exhibited the <strong>highest loss (2.2825)</strong> among the three models, suggesting: - Greater complexity in conversational, informal content - Diverse speaking styles and code-mixing (Hinglish) challenging to model - Richer linguistic variation compared to encyclopedic content</p>
<p>Despite higher loss, the model achieved 41% loss reduction (3.935 → 2.324), indicating successful learning of conversational patterns and cultural idioms prevalent in YouTube content.</p>
</section>
<section id="ted-talks-model-training" class="level4" data-number="5.5.2.3">
<h4 data-number="5.5.2.3" class="anchored" data-anchor-id="ted-talks-model-training"><span class="header-section-number">5.5.2.3</span> TED Talks Model Training</h4>
<p><strong>Dataset Statistics</strong>: - Total examples: 596 - Training set: 536 (90%) - Validation set: 60 (10%) - Training duration: 10 minutes 45 seconds (longest due to largest dataset)</p>
<p><strong>Training Progress</strong>:</p>
<pre><code>============================================================
TED Talks LoRA Fine-Tuning Pipeline
============================================================

1. Loading data from: .../indian_ted_talks/data/ted_talks_qa.jsonl
Total examples: 596
Training examples: 536
Validation examples: 60

5. Configuring LoRA...
Trainable parameters: 13,631,488 (0.1695%)
Total parameters: 8,043,892,736

8. Starting training...
============================================================
{'loss': 3.451, 'grad_norm': 1.911, 'learning_rate': 3.6e-05, 'epoch': 0.6}
{'loss': 3.063, 'grad_norm': 1.472, 'learning_rate': 7.6e-05, 'epoch': 1.18}
{'loss': 2.506, 'grad_norm': 1.670, 'learning_rate': 0.000116, 'epoch': 1.78}
{'loss': 2.175, 'grad_norm': 0.983, 'learning_rate': 0.000156, 'epoch': 2.36}
{'loss': 2.022, 'grad_norm': 1.266, 'learning_rate': 0.000196, 'epoch': 2.96}

============================================================
Training complete!
============================================================

10. Final evaluation metrics:
  eval_loss: 2.0647
  eval_runtime: 7.0019
  epoch: 3.0000</code></pre>
<p><strong>Performance Analysis</strong>:</p>
<p>The TED Talks model achieved <strong>intermediate loss (2.0647)</strong>, positioned between Wikipedia’s factual clarity and YouTube’s conversational complexity. This reflects: - Structured presentation style (more formal than YouTube) - Substantive content (less dry than Wikipedia) - Blend of storytelling and information delivery</p>
<p>Loss reduction of 41% (3.451 → 2.022) matched YouTube’s learning rate, suggesting comparable learning difficulty despite different content styles.</p>
</section>
<section id="comparative-training-analysis" class="level4" data-number="5.5.2.4">
<h4 data-number="5.5.2.4" class="anchored" data-anchor-id="comparative-training-analysis"><span class="header-section-number">5.5.2.4</span> Comparative Training Analysis</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 9%">
<col style="width: 22%">
<col style="width: 21%">
<col style="width: 23%">
<col style="width: 22%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Train/Val Split</th>
<th>Training Time</th>
<th>Final Eval Loss</th>
<th>Loss Reduction</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Wikipedia</strong></td>
<td>450/50</td>
<td>8:29</td>
<td>1.2456</td>
<td>47% (2.433→1.281)</td>
</tr>
<tr class="even">
<td><strong>TED Talks</strong></td>
<td>536/60</td>
<td>10:45</td>
<td>2.0647</td>
<td>41% (3.451→2.022)</td>
</tr>
<tr class="odd">
<td><strong>YouTube</strong></td>
<td>460/52</td>
<td>8:46</td>
<td>2.2825</td>
<td>41% (3.935→2.324)</td>
</tr>
</tbody>
</table>
<p><strong>Key Observations</strong>:</p>
<ol type="1">
<li><strong>Wikipedia’s superiority in loss metrics</strong>: The encyclopedic, structured nature of Wikipedia articles enabled tighter convergence</li>
<li><strong>Dataset size correlation</strong>: TED Talks (596 examples) took longest to train, but more data didn’t necessarily yield lowest loss</li>
<li><strong>Consistent learning rates</strong>: All models showed 40-47% loss reduction, indicating the training regimen was effective across content types</li>
<li><strong>Validation alignment</strong>: Small gaps between final training loss and validation loss across all models indicate minimal overfitting</li>
</ol>
</section>
</section>
<section id="prompt-engineering-for-inference" class="level3" data-number="5.5.3">
<h3 data-number="5.5.3" class="anchored" data-anchor-id="prompt-engineering-for-inference"><span class="header-section-number">5.5.3</span> Prompt Engineering for Inference</h3>
<section id="system-prompt-design" class="level4" data-number="5.5.3.1">
<h4 data-number="5.5.3.1" class="anchored" data-anchor-id="system-prompt-design"><span class="header-section-number">5.5.3.1</span> System Prompt Design</h4>
<p>To ensure culturally aware responses during testing, Moses designed a comprehensive system prompt that would be used consistently across all model evaluations:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./Appendix/MosesMadale/img/4_2.png" class="img-fluid figure-img"></p>
<figcaption>Prompt engineering code for culturally aware responses</figcaption>
</figure>
</div>
<p><strong>Prompt Structure</strong>:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>SYSTEM_PROMPT <span class="op">=</span> <span class="st">"""You are a culturally aware guide with deep knowledge of </span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="st">Indian traditions, Christianity in India, and the beautiful intersections </span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="st">between faith and culture. When answering questions, draw from your </span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="st">understanding of Indian regional diversity, historical contexts, contemporary </span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="st">practices, and lived experiences. Provide comprehensive, thoughtful responses </span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="st">(150-200 words each) that would be valuable for someone doing serious research. </span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="st">Include specific examples, acknowledge regional variations, and demonstrate </span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="st">sensitivity to both Hindu and Christian perspectives. Be conversational yet </span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="st">substantive – imagine you're having a meaningful conversation with someone </span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="st">genuinely curious about these topics."""</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Design Rationale</strong>:</p>
<p>The prompt was carefully crafted to: - <strong>Establish Cultural Expertise</strong>: “Deep knowledge of Indian traditions” sets expectations for nuanced answers - <strong>Define Scope</strong>: “Christianity in India” and “faith-culture intersections” focus the domain - <strong>Specify Response Style</strong>: “Comprehensive, thoughtful responses (150-200 words)” prevents overly brief answers - <strong>Encourage Specificity</strong>: “Include specific examples, acknowledge regional variations” promotes detailed responses - <strong>Balance Tone</strong>: “Conversational yet substantive” avoids academic dryness while maintaining seriousness</p>
</section>
<section id="question-formatting" class="level4" data-number="5.5.3.2">
<h4 data-number="5.5.3.2" class="anchored" data-anchor-id="question-formatting"><span class="header-section-number">5.5.3.2</span> Question Formatting</h4>
<p>Each test question was formatted using the same structure established during training:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> format_question(question):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">=</span> <span class="ss">f"### Question:</span><span class="ch">\n</span><span class="sc">{</span>question<span class="sc">}</span><span class="ch">\n\n</span><span class="ss">### Answer:</span><span class="ch">\n</span><span class="ss">"</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> prompt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This consistency between training and inference formats is critical for optimal model performance. The <code>### Question:</code> and <code>### Answer:</code> delimiters were familiar to the model from fine-tuning, enabling it to recognize when to start generating responses.</p>
</section>
<section id="generation-parameters" class="level4" data-number="5.5.3.3">
<h4 data-number="5.5.3.3" class="anchored" data-anchor-id="generation-parameters"><span class="header-section-number">5.5.3.3</span> Generation Parameters</h4>
<p>Inference used carefully tuned generation parameters:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 40%">
<col style="width: 25%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Value</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>max_new_tokens</code></td>
<td>150</td>
<td>Sufficient for detailed answers without rambling</td>
</tr>
<tr class="even">
<td><code>temperature</code></td>
<td>0.7</td>
<td>Balanced creativity and coherence</td>
</tr>
<tr class="odd">
<td><code>do_sample</code></td>
<td>True</td>
<td>Enable probabilistic sampling</td>
</tr>
<tr class="even">
<td><code>top_p</code></td>
<td>0.9</td>
<td>Nucleus sampling for quality control</td>
</tr>
<tr class="odd">
<td><code>pad_token_id</code></td>
<td><code>eos_token_id</code></td>
<td>Proper sequence termination</td>
</tr>
</tbody>
</table>
<p><strong>Temperature Selection</strong>:</p>
<p>The temperature of 0.7 was chosen after preliminary testing: - <strong>0.5</strong>: Too conservative, repetitive answers - <strong>0.7</strong>: Balanced creativity with accuracy - <strong>1.0</strong>: Occasional incoherence, overly creative</p>
</section>
</section>
<section id="test-question-design-and-evaluation-process" class="level3" data-number="5.5.4">
<h3 data-number="5.5.4" class="anchored" data-anchor-id="test-question-design-and-evaluation-process"><span class="header-section-number">5.5.4</span> Test Question Design and Evaluation Process</h3>
<section id="rohans-20-question-framework" class="level4" data-number="5.5.4.1">
<h4 data-number="5.5.4.1" class="anchored" data-anchor-id="rohans-20-question-framework"><span class="header-section-number">5.5.4.1</span> Rohan’s 20-Question Framework</h4>
<p>Team member Rohan designed 20 questions spanning multiple cultural dimensions and pillars. These questions were strategically crafted to test:</p>
<ul>
<li><strong>Interfaith Understanding</strong>: Hindu-Christian intersections</li>
<li><strong>Cultural Sensitivity</strong>: Handling religious diversity in institutional settings</li>
<li><strong>Practical Application</strong>: Real-world scenarios for Indian Christians</li>
<li><strong>Theological Integration</strong>: Biblical concepts in Indian cultural context</li>
</ul>
<p><strong>Example Questions</strong>:</p>
<ol type="1">
<li>“How can Diwali and Christmas both symbolize the victory of light over darkness in their respective traditions?”
<ul>
<li><strong>Tests</strong>: Values &amp; beliefs, religion &amp; spirituality, comparative understanding</li>
</ul></li>
<li>“What themes of renewal in Diwali resonate with the idea of rebirth in Christian theology?”
<ul>
<li><strong>Tests</strong>: Theological integration, cultural awareness, abstract reasoning</li>
</ul></li>
<li>“How might Indian Christians use Diwali to express biblical messages of hope?”
<ul>
<li><strong>Tests</strong>: Practical application, contextualization, creativity</li>
</ul></li>
<li>“How can Christian schools acknowledge Diwali without compromising faith boundaries?”
<ul>
<li><strong>Tests</strong>: Institutional sensitivity, balance, practical wisdom</li>
</ul></li>
<li>“What sensitivities should pastors consider when mentioning Hindu deities in Christmas homilies?”
<ul>
<li><strong>Tests</strong>: Cultural sensitivity, religious boundaries, contextual awareness</li>
</ul></li>
<li>“How can Christian youth groups create shared Diwali–Christmas community service projects?”
<ul>
<li><strong>Tests</strong>: Interfaith collaboration, practical implementation, social organization</li>
</ul></li>
</ol>
<p>These questions deliberately avoid simple factual recall, instead requiring: - Cultural nuance and sensitivity - Integration of multiple perspectives - Practical wisdom - Awareness of regional and social variations</p>
</section>
<section id="testing-workflow" class="level4" data-number="5.5.4.2">
<h4 data-number="5.5.4.2" class="anchored" data-anchor-id="testing-workflow"><span class="header-section-number">5.5.4.2</span> Testing Workflow</h4>
<p>Moses implemented a systematic testing process:</p>
<p><strong>Step 1: Model Loading</strong></p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load base model</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>base_model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"meta-llama/Meta-Llama-3-8B-Instruct"</span>,</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    device_map<span class="op">=</span><span class="st">"auto"</span>,</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    torch_dtype<span class="op">=</span>torch.float16</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Load fine-tuned LoRA adapters</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> PeftModel.from_pretrained(base_model, FINETUNED_MODEL_PATH)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Step 2: Question Iteration</strong></p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> question <span class="kw">in</span> test_questions:</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">=</span> <span class="ss">f"### Question:</span><span class="ch">\n</span><span class="sc">{</span>question<span class="sc">}</span><span class="ch">\n\n</span><span class="ss">### Answer:</span><span class="ch">\n</span><span class="ss">"</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> tokenizer(prompt, return_tensors<span class="op">=</span><span class="st">"pt"</span>).to(model.device)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model.generate(</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>        <span class="op">**</span>inputs,</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>        max_new_tokens<span class="op">=</span><span class="dv">150</span>,</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>        temperature<span class="op">=</span><span class="fl">0.7</span>,</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>        do_sample<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>        top_p<span class="op">=</span><span class="fl">0.9</span>,</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>        pad_token_id<span class="op">=</span>tokenizer.eos_token_id</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    response <span class="op">=</span> tokenizer.decode(outputs[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>    answer <span class="op">=</span> response.split(<span class="st">"### Answer:"</span>)[<span class="op">-</span><span class="dv">1</span>].strip()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Step 3: Response Logging</strong></p>
<p>Responses were saved to structured text files:</p>
<pre><code>================================================================================
Fine-tuned: YouTube Transcripts
Test Date: 2025-11-13 22:41:08
================================================================================

1. How can Diwali and Christmas both symbolize the victory of light over 
darkness in their respective traditions?

The victory of light over darkness is what I think is very deeply rooted 
across religious traditions here. The most obvious example is Diwali and 
its celebration of light defeating the forces of darkness, which is 
associated with this concept of the divine that's been worshipped in many 
parts of India for thousands of years. At the same time, if you look at 
the significance of Christmas, there's no doubt that it has an element of 
symbolism around the victory of good over evil as well, because we celebrate 
the birth of Jesus Christ who's associated with bringing light and hope to 
humanity...</code></pre>
</section>
<section id="response-organization-for-analysis" class="level4" data-number="5.5.4.3">
<h4 data-number="5.5.4.3" class="anchored" data-anchor-id="response-organization-for-analysis"><span class="header-section-number">5.5.4.3</span> Response Organization for Analysis</h4>
<p>Moses delivered results to Rohan in a standardized format:</p>
<p><strong>File Structure</strong>:</p>
<pre><code>responses/
├── youtube_model_responses.txt
├── wikipedia_model_responses.txt
├── ted_talks_model_responses.txt
└── baseline_model_responses.txt</code></pre>
<p>Each file contained: - Model identifier - Test date/time - All 20 questions - Complete model responses - Consistent formatting for comparison</p>
<p>This organization enabled Rohan to: - Directly compare responses across models - Track which model excelled at which questions - Identify patterns in cultural awareness - Score models on standardized rubrics</p>
</section>
</section>
<section id="baseline-model-testing" class="level3" data-number="5.5.5">
<h3 data-number="5.5.5" class="anchored" data-anchor-id="baseline-model-testing"><span class="header-section-number">5.5.5</span> Baseline Model Testing</h3>
<section id="control-variable-rationale" class="level4" data-number="5.5.5.1">
<h4 data-number="5.5.5.1" class="anchored" data-anchor-id="control-variable-rationale"><span class="header-section-number">5.5.5.1</span> Control Variable Rationale</h4>
<p>To establish whether fine-tuning actually improved cultural awareness, Moses tested the <strong>unfine-tuned LLaMA-3 8B Instruct</strong> model on the same 20 questions. This baseline served as a control variable, representing the model’s cultural knowledge “out of the box” without any Indian cultural training.</p>
<p><strong>Testing Approach</strong>:</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load base model WITHOUT LoRA adapters</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>baseline_model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"meta-llama/Meta-Llama-3-8B-Instruct"</span>,</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    device_map<span class="op">=</span><span class="st">"auto"</span>,</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>    torch_dtype<span class="op">=</span>torch.float16</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Use same generation parameters as fine-tuned models</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Same prompt format, same questions</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="expected-baseline-characteristics" class="level4" data-number="5.5.5.2">
<h4 data-number="5.5.5.2" class="anchored" data-anchor-id="expected-baseline-characteristics"><span class="header-section-number">5.5.5.2</span> Expected Baseline Characteristics</h4>
<p>The baseline model was expected to exhibit:</p>
<ol type="1">
<li><strong>Generic Cultural Knowledge</strong>: Broad understanding of Diwali and Christianity from pre-training</li>
<li><strong>Lack of Specificity</strong>: Missing regional variations, specific Indian Christian practices</li>
<li><strong>Western Bias</strong>: Potential Anglo-centric perspectives on Christianity</li>
<li><strong>Surface-Level Connections</strong>: Obvious parallels without deep cultural integration</li>
</ol>
<p><strong>Hypothesis</strong>: Fine-tuned models would demonstrate superior: - Use of Indian terminology and concepts - Awareness of regional variations - Integration of Hindu-Christian contexts - Practical, lived-experience insights</p>
</section>
<section id="baseline-integration-into-evaluation" class="level4" data-number="5.5.5.3">
<h4 data-number="5.5.5.3" class="anchored" data-anchor-id="baseline-integration-into-evaluation"><span class="header-section-number">5.5.5.3</span> Baseline Integration into Evaluation</h4>
<p>The baseline model responses joined the fine-tuned models in Rohan’s analysis framework, creating a <strong>4-model comparison</strong> for Week 4:</p>
<ol type="1">
<li>Baseline (unfine-tuned LLaMA-3 8B)</li>
<li>Wikipedia fine-tuned</li>
<li>TED Talks fine-tuned</li>
<li>YouTube Transcripts fine-tuned</li>
</ol>
<p>This comparison would reveal whether fine-tuning provided measurable improvements over the base model’s existing capabilities.</p>
</section>
</section>
<section id="combined-model-strategy-and-deferral" class="level3" data-number="5.5.6">
<h3 data-number="5.5.6" class="anchored" data-anchor-id="combined-model-strategy-and-deferral"><span class="header-section-number">5.5.6</span> Combined Model Strategy and Deferral</h3>
<section id="original-plan-two-combined-models" class="level4" data-number="5.5.6.1">
<h4 data-number="5.5.6.1" class="anchored" data-anchor-id="original-plan-two-combined-models"><span class="header-section-number">5.5.6.1</span> Original Plan: Two Combined Models</h4>
<p>Week 4 initially planned to create two ensemble models:</p>
<ol type="1">
<li><strong>Top 3 Datasets Model</strong>: Combine the three best-performing datasets based on evaluation results</li>
<li><strong>All 5 Datasets Model</strong>: Combine all datasets (Superstitions, Constitution, Wikipedia, TED Talks, YouTube)</li>
</ol>
<p><strong>Intended Workflow</strong>:</p>
<pre><code>Week 4 Model Evaluation
    ↓
Identify Top 3 Performers
    ↓
Train Combined Model (Top 3)
    ↓
Train Combined Model (All 5)
    ↓
Compare Combined vs Individual Models</code></pre>
</section>
<section id="time-constraint-reality" class="level4" data-number="5.5.6.2">
<h4 data-number="5.5.6.2" class="anchored" data-anchor-id="time-constraint-reality"><span class="header-section-number">5.5.6.2</span> Time Constraint Reality</h4>
<p>As Week 4 progressed, the team encountered a critical timeline issue:</p>
<p><strong>Challenge</strong>: Evaluating four models (baseline + 3 fine-tuned) required: - Rohan analyzing 80 total responses (20 questions × 4 models) - Developing scoring rubrics - Conducting qualitative analysis - Presenting findings to team</p>
<p>This evaluation process extended into the final days of Week 4, leaving insufficient time to: 1. Complete evaluation 2. Identify top 3 datasets 3. Prepare combined training data 4. Train combined model 5. Test combined model</p>
<p><strong>Decision</strong>: Defer combined model training to Week 5</p>
</section>
<section id="strategic-pivot-to-week-5" class="level4" data-number="5.5.6.3">
<h4 data-number="5.5.6.3" class="anchored" data-anchor-id="strategic-pivot-to-week-5"><span class="header-section-number">5.5.6.3</span> Strategic Pivot to Week 5</h4>
<p>The team made a strategic decision to prioritize quality over rushing:</p>
<p><strong>Week 4 Deliverable</strong>: Complete individual model training and testing <strong>Week 5 Deliverable</strong>: Train “Conlingo 2.0” (combined all 5 datasets) as the flagship model for final comparison</p>
<p><strong>Rationale</strong>: - Individual model results provided valuable insights regardless - Combined model training (~10-15 minutes) could fit in Week 5 - Allowed proper evaluation of individual models first - Ensured combined model incorporated lessons learned</p>
<p><strong>Conlingo 2.0 Definition</strong>: The final combined model trained on all 5 approved datasets (3,031 Q&amp;A pairs total), representing the team’s comprehensive approach to Indian cultural awareness training.</p>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./Week3.html" class="pagination-link" aria-label="Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Week 3: Data Searching, Data Vetting, Dataset Collection, Data Cleaning</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Week5.html" class="pagination-link" aria-label="Week 5: Compare Fine-tuned model with RAG implementation and ChatGPT-5 &amp; Research Paper">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Week 5: Compare Fine-tuned model with RAG implementation and ChatGPT-5 &amp; Research Paper</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>