{"title":"Week 2: Data Cleaning & Pipeline Setup","markdown":{"headingText":"Week 2: Data Cleaning & Pipeline Setup","containsRefs":false,"markdown":"\n## Overview\nThe team expanded the dataset, finalized the fine-tuning pipeline, completed baseline and human evaluation preparations, and built initial tools and documentation to support quality checks and research development.\n\n## Rohan Aby Deliverables\n **AI assistance: ChatGPT was used to develop the 200-question comprehensive test dataset (accessed Nov, 2025).**\n \n### 200-question comprehensive test dataset\n\nThe dataset was expanded to 100 more questions using AI. The questions were divided into\nqusetions on biblical contextualization (25 questions), Cultural Sensitivity (25 questions), Controversial Topics (i.e. cow slaughter, forced conversion) (25 Questions), Regional Variations (25 Questions) \n\n###  RAG baseline evaluation complete (CSI scoring) \n\nRohan selected 20 questions from the list and scored the responses based on the CSI frameowork. He scored the original model ([Download the results of original model's responses](../Appendix/RohanAby/Conlingo_20_question_answers.pdf)) vs the fine tuned model ([Download the results of fine tuned responses](../Appendix/RohanAby/20_Answers_to_Will's_Questions_Moses.pdf)). The result was that the orignal model performed better than the fine tuned model as can be seen in the image below. The original model had a score of 99.2 while the fine tuned model scored 85.55. The responses from the original model was more detailed and contained more information while the fine tuned model was short and did not contain much information. The scoring of the models can be seen below. \n![CSI Score Result](Appendix/RohanAby/CSIScore_result.png)\nFig. 1: CSI Score visualization\n\n\n## William Richards Deliverables\n\nWilliams task was to validate the data by creating visualizations for each set of criteria to ensure the data aligned with them for fine-tuning. These criteria included Cultural Accuracy, Biases or Stereotyping, and Tone or Respect. This was the most time-intensive deliverable, as he reviewed each dataset manually to determine which entries were suitable. After completing his review, he ran the data through the then-new ChatGPT 5.1, which confirmed 95% of his validations.\n\nIn addition, he worked on establishing the future direction for the project by creating a simple outline for the research paper that would ultimately serve as the final report.\n\nData Quality Validation\n\n```{r , eval=FALSE}\nlibrary(tidyverse)\nlibrary(rstudioapi)\n\ndata_path <- \"Appendix/WilliamRichards/data/conlingo_week2_200_labeled.csv\"\ntb_plots <- read.csv(data_path)\nglimpse(tb_plots)\n\n# Cultural Accuracy Plot: How accurate is the data to the respective culture?\n#   Scaling: 0 = Not at all\n#            1 = Simple references\n#            2 = Highly relevant\nggplot(tb_plots, aes(x = \"Cultural Accuracy\", y = cultural_accuracy, color = keep)) +\n  geom_jitter(width = 0.15, alpha = 0.7) +\n  coord_cartesian(ylim = c(-0.1, 2.1)) +\n  labs(\n    title = \"Cultural Accuracy (0–2)\",\n    x = \"\",\n    y = \"Score\",\n    color = \"Keep\"\n  ) +\n  theme_classic()\n\n# Bias / Stereotype Plot: Is than an element of bias or blatant stereotyping?\n#   Scaling: 0 = No\n#            1 = Yes\nstereo_plot <- ggplot(tb_plots, aes(x = \"Stereotype\", y = stereotype, color = keep)) +\n  geom_jitter(width = 0.15, alpha = 0.7) +\n  coord_cartesian(ylim = c(-0.1, 1.1)) +\n  labs(\n    title = \"Bias / Stereotype Flag (0–1)\",\n    x = \"\",\n    y = \"Flag\"\n  ) +\n  theme_classic()\nstereo_plot\n\n# Tone / Respect Plot: Does the data present itself respectfully?\n#   Scaling: 0 = Disrespectful\n#            1 = Neutral\n#            2 = Highly respectful\ntone_plot <- ggplot(tb_plots, aes(x = \"Tone\", y = tone, color = keep)) +\n  geom_jitter(width = 0.15, alpha = 0.7) +\n  coord_cartesian(ylim = c(-0.1, 2.1)) +\n  labs(\n    title = \"Tone / Respect (0–2)\",\n    x = \"\",\n    y = \"Score\"\n  ) +\n  theme_minimal()\ntone_plot\n```\n\nCultural Accuracy Visualization:\n![Cultural Accuracy Visualization:](Appendix/WilliamRichards/Cultural_Plot.png){fig-align=\"center\"}\n\nBias / Stereotype Visualization:\n\n![Bias / Stereotype Visualization](Appendix/WilliamRichards/Stereotype_Plot.png){fig-align=\"center\"}\n\nTone / Respect Visualization:\n![Tone / Respect Visualization](Appendix/WilliamRichards/Tone_Plot.png){fig-align=\"center\"}\n\nResearch Paper Outline\n\nI. Introduction\nA. Problem: Cultural contextualization for biblical content\nB. Thesis\n\nII. Related Work\nA. RAG Systems\nB. Fine-tuning for Cultural tasks\nC. Indian NLP\n\nIII. Methodologies\n\nA. Data Collection\nB. Model Architecture\nC. Evaluation\n\nIV. Results\nA. Comparison Introduction\nB. RAG vs. Fine-tuned\n\nV. Discussion\nA. When to use what approach?\nB. Post-execution reflection\n\nVI. Conclusion\nA. Thesis Restate\nB. Recommendations for OneHope\n\n\n## Suwilanji Mwanza Deliverables\n\n### Overview of the Week 2 Deliverable\n\nWeek 2's Deliverable was to solidify the training datasets for the fine-tuning process and then begin the cleaning process. In addition, ensuring that any unethical data was removed.\n\n###  Week 2 Deliverables:\n\n1.  2,500 raw examples collected (cumulative with Week 1: 3,000 total)\n2.  Initial cleaning applied\n3.  Source distribution documented\n4.  Bias check on collected data\n\n###  What was Accomplished:\n\n###  1. Reaching out to the Reddit API\n\nThis week, I focused on identifying quality data and what it entails. I reached out to Reddit to obtain its API key for more relevant data, as I await its response to see if it would still be possible to use its data. (Filled in a form).\n\n### 2. Finalize Data Sources\n\nI collected data from these 7 sources, all of which perform a specific role or fit into the buckets from last week. These data sets were collected primarily from Kaggle and the rest HuggingFace, or webpages.\n\n1.  IndicQuestionGeneration (hindu) (Source: AI4Bharat HuggingFace)\n2.  Regional Indian Superstitions & Beliefs (Source: Kaggle)\n3.  World Values Survey India (Source: WVS website)\n4.  Top Spotify Podcast Episodes (Updated Daily) India (Source: Kaggle)\n5.  Pew India Survey Dataset (Source: Pew Research Center)\n6.  Hinglish TOP Dataset (Source: Kaggle)\n7.  India News Headlines Dataset (Source: Kaggle)\n\n-   The **IndicQuestionGeneration** dataset has 1,078,297 rows. It is part of the IndicNLG Benchmark that consists of one of AI4Bharat's models. It is a large multilingual collection designed for Natural Language Generation (NLG) tasks and is in 11 Indic languages. It is formatted in Question Generation format, which means that when given a context and an answer, it aims to generate a question that leads to that answer. This was derived from the Stanford Question Answering Dataset (SQuAD). There are \\~98,000 examples per language, totaling around 1.08 **million examples** across all 11 languages. This dataset would be useful for training and evaluating models that generate comprehension questions in Indian languages. This dataset would be easy to use because it is in a question-answer pair format.\n\n-   The **Regional Indian Superstitions & Beliefs** dataset captures regional superstitions and beliefs from all 28 states and 8 union territories of India, showcasing the fascinating and often lesser-known cultural fabric that shapes daily life across the country. It is split into 500+ training entries and 100+ testing entries, each representing a specific superstition or folk belief. It is a data set for developing AI assistants that understand regional nuances.\n\n-   The **World Values Survey – India, Wave 7 (2017–2022)**, is a nationally representative dataset that captures the evolving social, political, economic, and cultural values of Indian society. Conducted through face-to-face interviews across multiple regions and languages, it surveyed approximately **2,400 respondents** aged 18 and above. The dataset comprises nearly 400 variables, encompassing priorities in life (family, work, religion, and politics), child-rearing values, trust in institutions, gender roles, democracy, corruption, globalization, and religious beliefs.\n\n-   The **Top Spotify Podcast Episodes (Updated Daily)** India contains a daily snapshot of Spotify's top 200 podcast episodes for every country. It also includes detailed information about podcast episodes and shows from the Spotify API. Daily data collection began on 2025-10-11; additionally, some data is available from 2024-09-02 to 2024-10-23. This dataset will help take a different angle on culture by comparingpodcast popularity to understand cultural influences on podcast consumption. India is one of the regions.\n\n-   The **Pew India Survey Dataset** is the raw data from a large-scale, face-to-face survey of 29,999 Indian adults (ages 18 and older) conducted by the [Pew Research Center](https://www.pewresearch.org/dataset/india-survey-dataset/) across 26 states and three union territories between November 17, 2019, and March 23, 2020. Conducted in 17 languages using computer-assisted personal interviews (CAPI), the sample was designed with an oversampling strategy to capture a broad view of religious demographics, including interviews with 22,975 Hindus, 3,336 Muslims, 1,782 Sikhs, 1,011 Christians, 719 Buddhists, and 109 Jains. This dataset provides researchers with detailed information on a wide range of social attitudes and beliefs in India, specifically covering topics such as religious identity, beliefs, practices, nationalism, and tolerance in Indian society.\n\n-   The **Hinglish TOP Dataset** comprises a large (10K) human-annotated code-switched semantic parsing dataset, which contains 10,000 real examples of people speaking in Hinglish commands. Humans have manually labeled or \"tagged\" the computer-readable meaning for each one. Additionally, the 170K generated utterances using the CST5 augmentation technique introduced in the paper were utilized, which significantly expanded the dataset. To achieve this, researchers employed a special trick called **CST5** to create **170,000 new, synthetic** (computer-generated) Hinglish commands based on the existing data. Queries are derived from TOPv2, a multi-domain task-oriented semantic parsing dataset.\n\n-   The **India News Headlines Dataset** is a comprehensive historical archive of notable events in the Indian subcontinent from 2001 to Q2 2023, recorded in real-time by journalists in India. It contains approximately 3.8 million events published by The Times of India. There are 3,876,557 rows in the CSV. Due to the heavy daily volume (avg. 600 articles) over multiple years, this data offers a deep insight into Indian society, its priorities, events, issues, and talking points, and how they have unfolded over time.\n\nThese seven sources will be used to train our data. However, because these sets combined are millions of data points, we will only use 6,000 of them to start. I will also be manually validating them as much as I can for any toxicity.\n\nIn this instance, **toxicity** is defined as data that may be considered harmful or detrimental. Sexual, indecent behaviour, curse words, and the like.\n\nAn **Excel sheet** with the following categories of data was collected and shared with the team.\n\n###  3. Distribution of Data\n\nAfter deciding on these datasets, due to the large amount of data in each, I had to narrow down the dataset sizes to ensure we have 6,000 high-quality examples to train our model. We can gather these examples by numbering each data point and doing a random sample, with the no. of examples as the goal.\n\nThe breakdown of the datasets and their approximate number of examples are:\n\n| Subset                                             | No. of Examples |\n|----------------------------------------------------|-----------------|\n| IndicQuestionGeneration                            | 1000            |\n| Regional Indian Superstitions & Beliefs            | 660             |\n| World Values Survey India                          | 1000            |\n| Top Spotify Podcast Episodes (Updated Daily) India | 500             |\n| Pew India Survey Dataset                           | 1000            |\n| Hinglish TOP Dataset                               | 1000            |\n| India News Headlines Dataset                       | 840             |\n| **Total:**                                         | **6,000**       |\n\nThis shows the distribution of our data set piece:\n\n![Titan GPU environment setup confirmation](img/week_2.png)\n\nThe bottleneck in this process is that the examples must be handpicked.\n\n###  4. Cleaning Script\n\nThe initial cleaning script will be better created. I integrated Claude into its development, so I can understand what automating that part looks like and the different functions. I will then create my own based on the final recommendation from Moses on how he wants the data to be structured.\n\n```{md}\n# Fine-Tuning Pipeline for ConLingo 2.0\n# 10/30/25\n# ETL Pipeline\n\n# Datasets included:\n# - India News Headlines Datasets 2001-01-01 ; End Date: 2023-06-30 (for example)\n\n\"\"\"\nData Cleaning & Preprocessing Pipeline for Fine-tuning\n\nSteps:\n1. Load datasets\n2. Clean text (remove URLs, excessive punctuation, spam)\n3. Deduplicate (cosine similarity > 0.9)\n4. Flag toxic content (Detoxify)\n\"\"\"\n\nimport os\nimport re\nimport string\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom detoxify import Detoxify\nfrom tqdm import tqdm\n\n# ---------------------------\n# CONFIGURATION\n# ---------------------------\nDATASETS = {\n    \"IndicQuestionGeneration_hi\": \"path/to/IndicQuestionGeneration.csv\",\n    \"RegionalIndianSuperstitions\": \"path/to/RegionalIndianSuperstitions.csv\",\n    \"WorldValuesSurveyIndia\": \"path/to/WorldValuesSurveyIndia.csv\",\n    \"TopSpotifyPodcastEpisodes\": \"path/to/TopSpotifyPodcastEpisodes.csv\",\n    \"IndiaPewSurvey\": \"path/to/IndiaPewSurvey.csv\",\n    \"HinglishTOP\": \"path/to/HinglishTOP.csv\",\n    \"IndiaNewsHeadlines\": \"path/to/IndiaNewsHeadlines.csv\"\n}\n\nTEXT_COLUMN = \"text\"   # Change this if your datasets have a different column name\n\n# ---------------------------\n# TEXT CLEANING FUNCTIONS\n# ---------------------------\ndef clean_text(text: str) -> str:\n    \"\"\"Remove URLs, excessive punctuation, and obvious spam.\"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # remove URLs\n    text = re.sub(r\"\\s+\", \" \", text)  # collapse whitespace\n    text = re.sub(rf\"[{re.escape(string.punctuation)}]\", \" \", text)  # remove punctuation\n    text = re.sub(r\"\\b(buy now|click here|free money|subscribe)\\b\", \"\", text, flags=re.I)\n    return text.strip()\n\n# ---------------------------\n# DEDUPLICATION\n# ---------------------------\ndef deduplicate_texts(df: pd.DataFrame, column: str, threshold=0.9) -> pd.DataFrame:\n    \"\"\"Remove near-duplicates using cosine similarity.\"\"\"\n    texts = df[column].fillna(\"\").tolist()\n    vectorizer = TfidfVectorizer().fit_transform(texts)\n    similarity = cosine_similarity(vectorizer)\n    to_drop = set()\n    for i in range(len(texts)):\n        for j in range(i + 1, len(texts)):\n            if similarity[i, j] > threshold:\n                to_drop.add(j)\n    return df.drop(df.index[list(to_drop)])\n\n# ---------------------------\n# TOXICITY FLAGGING\n# ---------------------------\ndef flag_toxicity(df: pd.DataFrame, column: str) -> pd.DataFrame:\n    \"\"\"Add a column with toxicity scores using Detoxify.\"\"\"\n    model = Detoxify('original')\n    toxicity_scores = []\n    for text in tqdm(df[column], desc=\"Toxicity Scoring\"):\n        score = model.predict(str(text))\n        toxicity_scores.append(score[\"toxicity\"])\n    df[\"toxicity_score\"] = toxicity_scores\n    df[\"is_toxic_flag\"] = df[\"toxicity_score\"] > 0.5  # manual review threshold\n    return df\n\n# ---------------------------\n# PIPELINE EXECUTION\n# ---------------------------\ndef process_dataset(name, path):\n    print(f\"\\nProcessing {name}...\")\n    df = pd.read_csv(path)\n    if TEXT_COLUMN not in df.columns:\n        raise ValueError(f\"Column '{TEXT_COLUMN}' not found in {path}\")\n\n    # Step 1: Clean\n    df[TEXT_COLUMN] = df[TEXT_COLUMN].apply(clean_text)\n\n    # Step 2: Deduplicate\n    df = deduplicate_texts(df, TEXT_COLUMN)\n\n    # Step 3: Flag toxicity\n    df = flag_toxicity(df, TEXT_COLUMN)\n\n    # Save processed version\n    out_path = f\"cleaned_{name}.csv\"\n    df.to_csv(out_path, index=False)\n    print(f\"Saved cleaned dataset to {out_path}\")\n\n# ---------------------------\n# MAIN SCRIPT\n# ---------------------------\nif __name__ == \"__main__\":\n    for name, path in DATASETS.items():\n        if os.path.exists(path):\n            process_dataset(name, path)\n        else:\n            print(f\" Skipping {name}: file not found at {path}\")\n\n```\n\n-   This script will aid in Removing URLs, excessive punctuation, and obvious spam. Deduplicate or Near-duplicate detection (cosine similarity \\> 0.9) and lastly flag any toxic content using Detoxify classifier (which will require additional manual review).\n\n### AI assistance:\n\n-   \"AI assistance: Gemini was used to summarize the data source pages of some of the sources for me to write my paragraphs.\"\n\n-   \"AI assistance: Claude was used to generate an example script for a very simple backbone pipeline that would be expanded upon.\"\n\n\n## Moses Mandale Deliverables\n\nAI assistance:Claude Sonnet 4.5 was used to optimize the initial finetune_lora.py to use LoRA fine tuning instead of the normal fine tuning to that the fine tuning is more efficient.\nClaude Sonnet 4.5 was used to troubleshoot “memory exceeded” errors and optimize the training parameters like number of epochs and batch size to fit the 24GB RAM that is available on Titan. This an error that was really troubling me because the script was already optimized with LoRA and was wondering what exactly was missing (accessed Nov, 2025).\n\n\n### Overview\n\nWeek 2 focused on developing the complete fine-tuning pipeline for the LLaMA-3 8B model. The primary deliverables included creating data preprocessing scripts, implementing the LoRA fine-tuning workflow, training the first model on Indian superstition data, and validating the training process. This week transformed the configured environment from Week 1 into a functional training system.\n\n### Fine-Tuning Pipeline Architecture\n\n#### Pipeline Overview\n\nThe fine-tuning pipeline consists of two main stages: data preprocessing and model training. The architecture was designed to be modular, allowing for easy iteration on different datasets while maintaining consistent formatting and training procedures.\n\n![Fine-tuning pipeline architecture](Appendix/MosesMadale/img/2_1.png)\n\n**Pipeline Stages**:\n\n1. **Data Preprocessing** (`preprocess_data.py`): Converts raw CSV data into instruction-response format suitable for LLaMA-3 fine-tuning\n2. **Model Fine-Tuning** (`finetune_lora.py`): Trains LoRA adapters on the preprocessed data while keeping base model weights frozen\n\n### Data Flow\n\n```\nRaw CSV (Indian Superstitions)\n    ↓\npreprocess_data.py\n    ↓\nJSON instruction-response pairs\n    ↓\nTrain/Validation Split (90/10)\n    ↓\nfinetune_lora.py\n    ↓\nFine-tuned Model with LoRA Adapters\n```\n\n### Data Preprocessing\n\n#### Input Dataset\n\nThe preprocessing pipeline began with a dataset of Indian superstitions collected by team member Suwilanji from Kaggle. The raw dataset contained:\n\n- **Total Rows**: 660 entries\n- **Key Columns**: `superstition_name`, `description`, `region`, `category`, `origin_theory`, `modern_status`, `is_harmful`, `source`, `user_contributed`\n- **Format**: CSV file\n\n#### Preprocessing Script Design\n\nThe `preprocess_data.py` script implements a systematic transformation from raw tabular data to instruction-response format suitable for LLM fine-tuning.\n\n**Key Processing Steps**:\n\n1. **Column Detection**: Automatically identifies name and description columns\n2. **Data Cleaning**: Filters out invalid entries (null values, insufficient length)\n3. **Unicode Normalization**: Converts Unicode quotation marks to ASCII equivalents\n4. **Question Generation**: Transforms superstition names into natural questions\n5. **JSON Structuring**: Creates instruction-response pairs with metadata\n6. **Train/Validation Split**: Implements 90/10 split with random shuffling\n\n**Data Transformation Example**:\n\n```python\n# Input (CSV row)\nsuperstition_name: \"Mirror breaks, 7 years bad luck\"\ndescription: \"Believed to bring misfortune if a mirror breaks.\"\n\n# Output (JSON)\n{\n  \"instruction\": \"What is the cultural significance and meaning of the superstition: 'Mirror breaks, 7 years bad luck'?\",\n  \"response\": \"Believed to bring misfortune if a mirror breaks.\",\n  \"metadata\": {\n    \"source\": \"superstition_dataset\",\n    \"category\": \"cultural_beliefs\",\n    \"original_name\": \"Mirror breaks, 7 years bad luck\"\n  }\n}\n```\n\n![Data preprocessing transformation from CSV to JSON](Appendix/MosesMadale/img/2_2.png)\n\n#### Preprocessing Results\n\nThe preprocessing pipeline successfully transformed the raw dataset:\n\n**Input Statistics**:\n- Raw CSV entries: 660\n- Valid entries after filtering: 658\n\n**Output Statistics**:\n- Training examples: 592 (90%)\n- Validation examples: 66 (10%)\n- Total processed: 658\n\n**Data Quality Measures**:\n- Filtered entries: 2 (entries with null values or insufficient text length)\n- Unicode normalization: Applied to all entries for ASCII compatibility\n- Question format: Consistent across all examples\n\n### Fine-Tuning Implementation\n\n#### Training Configuration\n\nThe fine-tuning script (`finetune_lora.py`) implements LoRA-based parameter-efficient fine-tuning with the following configuration:\n\n**LoRA Hyperparameters**:\n\n| Parameter | Value | Purpose |\n|-----------|-------|---------|\n| Rank (r) | 16 | Dimensionality of adapter matrices |\n| Alpha | 32 | Scaling factor for adapter outputs |\n| Target Modules | q_proj, v_proj, k_proj, o_proj | Attention projection layers |\n| Dropout | 0.05 | Regularization to prevent overfitting |\n| Bias | None | No additional bias terms in adapters |\n\n**Training Hyperparameters**:\n\n| Parameter | Value | Explanation |\n|-----------|-------|-------------|\n| Epochs | 3 | Number of passes through training data |\n| Per-device Batch Size | 4 | Examples processed simultaneously |\n| Gradient Accumulation Steps | 8 | Effective batch size = 4 × 8 = 32 |\n| Learning Rate | 2e-4 | Step size for weight updates |\n| Max Sequence Length | 512 | Maximum tokens per example |\n| Evaluation Strategy | Steps | Evaluate periodically during training |\n| Save Steps | 100 | Checkpoint frequency |\n\n**Effective Training Configuration**:\n- Effective batch size: 32 examples per update\n- Total training steps: ~54 steps (592 examples / 32 batch size × 3 epochs)\n- Gradient checkpointing: Enabled for memory efficiency\n\n### Model Architecture\n\nThe fine-tuning process adds LoRA adapters to specific layers of the LLaMA-3 8B model:\n\n**Parameter Breakdown**:\n- Base model parameters (frozen): 8,030,261,248\n- LoRA trainable parameters: 13,631,488\n- Total parameters: 8,043,892,736\n- **Trainable percentage: 0.17%**\n\nThis parameter-efficient approach enables training with limited GPU memory while maintaining model quality.\n\n![Training configuration output](Appendix/MosesMadale/img/2_3.png)\n\n#### Tokenization and Data Formatting\n\nThe training script implements a custom formatting function to structure data for instruction-following:\n\n```python\ndef format_instruction(example):\n    \"\"\"Format question-answer pair for training\"\"\"\n    text = f\"### Question:\\n{example['instruction']}\\n\\n### Answer:\\n{example['response']}\"\n    return text\n```\n\nThis format provides clear delineation between the instruction (question) and expected response, helping the model learn the question-answering structure.\n\n### Training Process and Results\n\n#### Training Progress\n\nThe model was trained for 3 epochs on the Indian superstition dataset. The training process exhibited expected behavior with decreasing loss values:\n\n**Loss Progression**:\n\n| Epoch | Training Loss | Gradient Norm | Learning Rate |\n|-------|---------------|---------------|---------------|\n| 0.6 | 3.4506 | 1.9107 | 3.6e-05 |\n| 1.18 | 3.0625 | 1.4718 | 7.6e-05 |\n| 1.78 | 2.5055 | 1.6695 | 1.16e-04 |\n| 2.36 | 2.1748 | 0.9833 | 1.56e-04 |\n| 2.96 | 2.0223 | 1.2655 | 1.96e-04 |\n\n**Final Metrics**:\n- Training loss: 2.6282\n- Validation loss: 2.0647\n- Training time: Approximately 10 minutes\n- GPU memory usage: ~16 GB during training\n\n![Training progress showing loss decrease over epochs](Appendix/MosesMadale/img/2_4.png)\n\n#### Loss Analysis\n\nThe training loss decreased from 3.45 to 2.02, indicating successful learning:\n\n- **Initial loss (3.45)**: Model has limited knowledge of the specific cultural content\n- **Mid-training (2.51 at epoch 1.78)**: Model begins recognizing patterns in superstition descriptions\n- **Final loss (2.02)**: Model has learned to generate appropriate responses for cultural questions\n\nThe validation loss of 2.06 is slightly higher than the final training loss, indicating minimal overfitting. This small gap suggests the model generalizes reasonably well to unseen examples.\n\n#### Model Checkpointing\n\nThe fine-tuning pipeline automatically saves model checkpoints:\n\n**Saved Artifacts**:\n- Final model: `/home/mmadale/CSC463/conlingo/models/finetuned/final_model/`\n- Checkpoint size: Approximately 1.3 GB (includes LoRA adapters only)\n- Configuration files: `adapter_config.json`, `adapter_model.bin`\n\nThe saved model contains only the LoRA adapters, not the full base model. This significantly reduces storage requirements while preserving the fine-tuning results.\n\n### Code Implementation Highlights\n\n#### Data Preprocessing\n\nKey implementation details from `preprocess_data.py`:\n\n```python\n# Unicode normalization for ASCII compatibility\nsuperstition_name = superstition_name.replace('\\u2019', \"'\").replace('\\u2018', \"'\")\nsuperstition_name = superstition_name.replace('\\u201c', '\"').replace('\\u201d', '\"')\n\n# Question generation\nquestion = f\"What is the cultural significance and meaning of the superstition: '{superstition_name}'?\"\n\n# Structured output\ninstruction_response = {\n    \"instruction\": question,\n    \"response\": description,\n    \"metadata\": {\n        \"source\": \"superstition_dataset\",\n        \"category\": \"cultural_beliefs\",\n        \"original_name\": superstition_name\n    }\n}\n```\n\n#### Fine-Tuning Configuration\n\nKey implementation details from `finetune_lora.py`:\n\n```python\n# LoRA configuration\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=8,\n    learning_rate=2e-4,\n    evaluation_strategy=\"steps\",\n    save_steps=100\n)\n```\n\n![Code snippet showing training configuration](Appendix/MosesMadale/img/2_5.png)\n\n### Technical Challenges and Solutions\n\n#### Memory Management\n\n**Challenge**: Training with batch size of 8 caused out-of-memory errors on the 24.6 GB GPU.\n\n**Solution**: Implemented gradient accumulation with batch size 4 and accumulation steps 8, achieving effective batch size of 32 while staying within memory constraints.\n\n#### Unicode Character Handling\n\n**Challenge**: Original dataset contained Unicode quotation marks (e.g., `\\u2019`, `\\u201c`) that could cause tokenization issues.\n\n**Solution**: Implemented systematic Unicode-to-ASCII conversion in preprocessing script, ensuring consistent character encoding throughout the dataset.\n\n#### Data Quality\n\n**Challenge**: Some CSV entries contained null values or insufficient text.\n\n**Solution**: Added validation checks in preprocessing script to filter entries with:\n- Null/missing superstition names or descriptions\n- Superstition names shorter than 3 characters\n- Descriptions shorter than 10 characters\n\n### Validation and Quality Assurance\n\n#### Data Validation\n\nThe preprocessing script includes multiple validation steps:\n\n1. **Column Detection**: Automatically identifies relevant columns, adapting to CSV structure\n2. **Null Filtering**: Removes entries with missing critical information\n3. **Length Validation**: Ensures minimum content length for meaningful training\n4. **Unicode Normalization**: Standardizes character encoding\n\n**Validation Results**:\n- Entries processed: 660\n- Entries passed validation: 658 (99.7% retention rate)\n- Entries filtered: 2 (0.3%)\n\n#### Training Validation\n\nThe training process includes continuous validation:\n\n- **Gradient Norm Monitoring**: Tracked to detect instability (all values < 2.0, indicating stable training)\n- **Validation Loss**: Evaluated every 100 steps to monitor generalization\n- **Checkpoint Saving**: Automatic preservation of model state for recovery\n\n### Key Achievements\n\nWeek 2 successfully delivered a complete fine-tuning pipeline and initial trained model:\n\n1. **Data Preprocessing Pipeline**: Created robust script converting CSV to instruction-response format\n2. **658 Training Examples**: Processed and validated dataset of Indian superstitions\n3. **Fine-Tuning Script**: Implemented LoRA-based training with proper configuration\n4. **Trained Model**: Successfully fine-tuned LLaMA-3 8B on cultural content\n5. **Loss Reduction**: Achieved 41% decrease in training loss (3.45 → 2.02)\n6. **Model Artifacts**: Generated reusable LoRA adapters (~1.3 GB) for inference\n7. **Reproducible Pipeline**: Created modular scripts adaptable to new datasets\n\n### Lessons Learned\n\n#### Pipeline Design\n\nThe modular pipeline architecture proved effective for rapid iteration. Separating preprocessing and training into distinct scripts allowed independent testing and debugging of each component.\n\n#### Batch Size Optimization\n\nFinding the optimal batch size required balancing:\n- GPU memory constraints (24.6 GB available)\n- Training stability (larger batches → more stable gradients)\n- Training speed (larger batches → fewer updates per epoch)\n\nThe final configuration (batch size 4 with 8 accumulation steps) effectively balanced these factors.\n\n#### Data Quality Impact\n\nThe high validation rate (99.7% of entries passed filtering) indicated good initial data quality. Manual inspection of filtered entries confirmed they were legitimately problematic (null values), validating the filtering criteria.\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":false,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"Week2.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.33","jupyter":false,"bibliography":["references.bib"],"theme":["cosmo","brand"]},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":false,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"Week2.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"block-headings":true,"jupyter":false,"bibliography":["references.bib"],"documentclass":"scrreprt"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}