{"title":"Week 5: Compare Fine-tuned model with RAG implementation and ChatGPT-5 & Research Paper","markdown":{"headingText":"Week 5: Compare Fine-tuned model with RAG implementation and ChatGPT-5 & Research Paper","containsRefs":false,"markdown":"\n## Overview\nIn Week 5, the team focused on comparing the fine-tuned models with the RAG implementation and ChatGPT-5, including running a Turing test to evaluate whether the AI responses were indistinguishable from those of a Native Indian Expert. Rohan conducted the survey and integrated deliverables into the research paper, Moses fine-tuned the model using RAG data and prepared for testing, Suwilanji drafted the initial paper and organized participants for the Turing test, and William edited and finalized the research paper.\n\n## Rohan Aby Deliverables\nRohan worked on recuriting multiple to do the longer survey that dealt with the extensive grading proecss. Only his mother completed the task. Then he recruited multiple people from churches in Tulsa, Kuwait and India to fill out the survey that Moses and William designed. \n\nRohan then worked on the research paper and this qurto book. \n\n## William Richards Deliverables\n\n\nThis quarto document will be quite short given me and Moses worked on this together and Will was more focused on reaching out to others.\n\nOur deliverable this week was to form the grading process into a yin and yang, where Rohan's grading would be individualistic and personal, while ours would be clustered and technical.\n\nWe chose 3 different models and a short set of questions (courtesy of Rohan) and Moses implemented them into a google form for us to send out to anyone from India. A final grading process that emphasizes and utilizes the cultural diversity of ORU, Wills job was to get this to as many people as possible.\n\nThe google form requires one to have been an Indian citizen and decently knowledgeable of Indian culture, teseting to see which model Q/A pairs the community at large (via polling) decided was most accurate.\n\nBetween:\n\n-   ConLingo Base Model (RAG Implementation)\n\n-   **ConLingo 2.0 Model**\n\n-   ChatGPT 5.1\n\nWill used his position on Student Association, he was able to send this survey out to as many people as possible. Along with Moses', Suwilanji's, and Rohan's massive help, we were able to receive 50 responses in 24 hours!\n\nAfter analyzing it appears that many decided the RAG Implementation was just barely better than ConLingo 2.0 and ChatGPT 5.1. However despite the fact our project might not have been as impactful as we hoped, the journey itself is a fantastic groundwork with a new way to approach improving model's such as these. By interviewing and surveying more personally and emotionally than simply grading with another AI model.\n\n\n## Moses Madale Deliverables\n\n **AI assistance: Claude Sonnet 4.5 was used to help me restructure the prompt to the AI Llama model which I originally developed myself based on the prompt that was given to the Conlingo RAG model so that it is more LLM friendly.\nChatGPT 5.1 Was used to suggest the formula to derive the curved AI model scores from the survey of 50 Indian students from ORU (accessed Nov, 2025).**\n \n### Human Evaluation and Comparative Analysis\n\n####  Overview\n\nWeek 5 represented the culmination of the ConLingo 2.0 project: training the final combined model on all five datasets, designing a rigorous human evaluation survey, and determining whether supervised fine-tuning could surpass the existing RAG implementation and state-of-the-art models like ChatGPT 5.1 in capturing Indian cultural nuance. This week required not only technical execution but also careful experimental design to ensure valid, bias-minimized results from human evaluators with authentic Indian cultural expertise.\n\n**Primary Objectives**:\n\n1. Train Conlingo 2.0 (combined model with all 3,031 Q&A pairs)\n2. Engineer culturally appropriate prompts for fair model comparison\n3. Design and deploy a Google Forms survey minimizing bias\n4. Recruit qualified participants with Indian cultural background\n5. Analyze results using both raw and weighted scoring methods\n6. Draw conclusions about fine-tuning effectiveness for cultural awareness\n\n#### Conlingo 2.0: Training the Combined Model\n\n##### Dataset Integration Strategy\n\nConlingo 2.0 represented the team's hypothesis that combining diverse Indian cultural data sources would produce a model with comprehensive cultural awareness. The model integrated all five approved datasets from Week 3:\n\n| Dataset | Examples | Percentage | Cultural Pillars Covered |\n|---------|----------|------------|--------------------------|\n| **Superstitions** | 923 | 30.5% | Values & beliefs, norms & customs |\n| **TED Talks** | 596 | 19.7% | Arts & literature, social organization |\n| **YouTube Transcripts** | 512 | 16.9% | Language, contemporary culture |\n| **Wikipedia** | 500 | 16.5% | Government, artifacts & technology |\n| **Constitution** | 500 | 16.5% | Government, social organization |\n| **Total** | 3,031 | 100% | All 8 pillars |\n\n**Rationale for Balanced Representation**:\n\nThe distribution was not artificially balanced but reflected the natural availability of quality data:\n- Superstitions (30.5%) provided the richest source of everyday cultural beliefs\n- TED Talks and YouTube captured contemporary Indian voices and Hinglish usage\n- Wikipedia and Constitution grounded the model in factual, institutional knowledge\n\n#####  Training Configuration\n\nConlingo 2.0 used identical LoRA hyperparameters to Week 4's individual models for consistency:\n\n```python\n#| eval: false\n\n#!/usr/bin/env python3\n\"\"\"\nLoRA Fine-Tuning with ALL Indian Cultural Data\nCombines: YouTube, TED Talks, Wikipedia, Constitution, Superstitions\n\"\"\"\n\nimport torch\nimport json\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling\n)\nfrom peft import LoraConfig, get_peft_model\nfrom datasets import Dataset\nimport os\nfrom sklearn.model_selection import train_test_split\n\nprint(\"=\"*60)\nprint(\"Combined All-Data LoRA Fine-Tuning Pipeline\")\nprint(\"=\"*60)\n\n# Paths\nDATA_PATHS = {\n    \"youtube\": \"/home/mmadale/CSC463/conlingo/youtube_data/data/final_youtube_transcript_data.jsonl\",\n    \"ted_talks\": \"/home/mmadale/CSC463/conlingo/huggingface_data/indian_ted_talks/data/ted_talks_qa.jsonl\",\n    \"wikipedia\": \"/home/mmadale/CSC463/conlingo/huggingface_data/indian_wikipedia/data/wikipedia_qa.jsonl\",\n    \"constitution\": \"/home/mmadale/CSC463/conlingo/huggingface_data/indian_constitution/data/constitution_qa.jsonl\",\n    \"superstitions\": \"/home/mmadale/CSC463/conlingo/superstition_data/data/superstition_qa.jsonl\"\n}\n\nOUTPUT_DIR = \"/home/mmadale/CSC463/conlingo/models/combined-all-data\"\nMODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"\\n1. Loading and combining all datasets...\")\n\nall_examples = []\ndataset_stats = {}\n\nfor dataset_name, data_path in DATA_PATHS.items():\n    print(f\"\\n  Loading {dataset_name}...\")\n    \n    with open(data_path, 'r', encoding='utf-8') as f:\n        data = [json.loads(line) for line in f]\n    \n    count = 0\n    for item in data:\n        # Normalize to question/answer format\n        if \"instruction\" in item and \"response\" in item:\n            question = item[\"instruction\"]\n            answer = item[\"response\"]\n        elif \"question\" in item and \"answer\" in item:\n            question = item[\"question\"]\n            answer = item[\"answer\"]\n        else:\n            continue\n        \n        all_examples.append({\n            \"question\": question,\n            \"answer\": answer,\n            \"source\": dataset_name\n        })\n        count += 1\n    \n    dataset_stats[dataset_name] = count\n    print(f\"    Loaded {count} examples from {dataset_name}\")\n\nprint(f\"\\n  Dataset Statistics:\")\nfor dataset_name, count in dataset_stats.items():\n    percentage = (count / len(all_examples)) * 100\n    print(f\"    {dataset_name}: {count} examples ({percentage:.1f}%)\")\n\nprint(f\"\\n  Total combined examples: {len(all_examples)}\")\n\n# Split into train/validation (90/10)\ntrain_data, val_data = train_test_split(all_examples, test_size=0.1, random_state=42)\n\nprint(f\"\\n  Training examples: {len(train_data)}\")\nprint(f\"  Validation examples: {len(val_data)}\")\n\nprint(f\"\\n2. Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nprint(f\"  Tokenizer loaded: {tokenizer.__class__.__name__}\")\n\nprint(f\"\\n3. Preparing datasets...\")\n\ndef format_instruction(example):\n    \"\"\"Format question-answer pair for training\"\"\"\n    text = f\"### Question:\\n{example['question']}\\n\\n### Answer:\\n{example['answer']}\"\n    return text\n\ndef tokenize_function(example):\n    \"\"\"Tokenize examples with padding and truncation\"\"\"\n    text = format_instruction(example)\n    \n    tokenized = tokenizer(\n        text,\n        truncation=True,\n        max_length=512,\n        padding=\"max_length\",\n        return_tensors=None\n    )\n    \n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized\n\n# Convert to HuggingFace Dataset format\ntrain_dataset = Dataset.from_list(train_data)\nval_dataset = Dataset.from_list(val_data)\n\n# Tokenize\nprint(\"  Tokenizing training data...\")\ntrain_dataset = train_dataset.map(\n    tokenize_function,\n    remove_columns=train_dataset.column_names\n)\n\nprint(\"  Tokenizing validation data...\")\nval_dataset = val_dataset.map(\n    tokenize_function,\n    remove_columns=val_dataset.column_names\n)\n\nprint(f\"  Training dataset size: {len(train_dataset)}\")\nprint(f\"  Validation dataset size: {len(val_dataset)}\")\n\nprint(f\"\\n4. Loading base model...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\n\nmodel.gradient_checkpointing_enable()\n\nprint(f\"  Model loaded: {model.__class__.__name__}\")\nprint(f\"  Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\nprint(f\"\\n5. Configuring LoRA...\")\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\n\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\n\nprint(f\"  LoRA configured successfully\")\nprint(f\"  Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.4f}%)\")\nprint(f\"  Total parameters: {total_params:,}\")\n\nprint(f\"\\n6. Setting up training arguments...\")\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=16,\n    learning_rate=2e-4,\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=100,\n    logging_steps=10,\n    save_steps=100,\n    eval_steps=100,\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    fp16=True,\n    report_to=\"none\",\n    save_total_limit=2,\n    remove_unused_columns=False,\n    gradient_checkpointing=True\n)\n\nprint(\"  Training configuration:\")\nprint(f\"    Epochs: {training_args.num_train_epochs}\")\nprint(f\"    Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"    Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"    Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"    Learning rate: {training_args.learning_rate}\")\n\nprint(f\"\\n7. Initializing trainer...\")\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator\n)\n\nprint(\"  Trainer initialized successfully\")\n\nprint(f\"\\n8. Starting training...\")\nprint(\"=\"*60)\n\ntrainer.train()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Training complete\")\nprint(\"=\"*60)\n\nprint(f\"\\n9. Saving final model...\")\nmodel.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\ntokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n\nprint(f\"  Model saved to: {OUTPUT_DIR}/final_model\")\n\nprint(f\"\\n10. Final evaluation metrics:\")\neval_results = trainer.evaluate()\nfor key, value in eval_results.items():\n    print(f\"    {key}: {value:.4f}\")\n```\n\n#### Training Results\n\n**Training Output**:\n\n```bash\n#| eval: false\n\n============================================================\nCombined All-Data LoRA Fine-Tuning Pipeline\n============================================================\n\n1. Loading and combining all datasets...\n\n  Loading youtube...\n    Loaded 512 examples from youtube\n\n  Loading ted_talks...\n    Loaded 596 examples from ted_talks\n\n  Loading wikipedia...\n    Loaded 500 examples from wikipedia\n\n  Loading constitution...\n    Loaded 500 examples from constitution\n\n  Loading superstitions...\n    Loaded 923 examples from superstitions\n\n  Dataset Statistics:\n    youtube: 512 examples (16.9%)\n    ted_talks: 596 examples (19.7%)\n    wikipedia: 500 examples (16.5%)\n    constitution: 500 examples (16.5%)\n    superstitions: 923 examples (30.5%)\n\n  Total combined examples: 3031\n\n  Training examples: 2727\n  Validation examples: 304\n\n2. Loading tokenizer...\n  Tokenizer loaded: PreTrainedTokenizerFast\n\n3. Preparing datasets...\n  Tokenizing training data...\n  Tokenizing validation data...\n  Training dataset size: 2727\n  Validation dataset size: 304\n\n4. Loading base model...\n  Model loaded: LlamaForCausalLM\n  Model parameters: 8,030,261,248\n\n5. Configuring LoRA...\n  LoRA configured successfully\n  Trainable parameters: 13,631,488 (0.1695%)\n  Total parameters: 8,043,892,736\n\n6. Setting up training arguments...\n  Training configuration:\n    Epochs: 3\n    Batch size: 2\n    Gradient accumulation: 16\n    Effective batch size: 32\n    Learning rate: 0.0002\n    Total training steps: ~255\n\n7. Initializing trainer...\n  Trainer initialized successfully\n\n8. Starting training...\n============================================================\n{'loss': 2.7157, 'grad_norm': 1.824, 'learning_rate': 1.8e-05, 'epoch': 0.12}\n{'loss': 2.5419, 'grad_norm': 1.431, 'learning_rate': 3.8e-05, 'epoch': 0.23}\n{'loss': 2.2675, 'grad_norm': 1.163, 'learning_rate': 5.8e-05, 'epoch': 0.35}\n{'loss': 1.9553, 'grad_norm': 1.023, 'learning_rate': 7.8e-05, 'epoch': 0.47}\n{'loss': 1.7885, 'grad_norm': 1.113, 'learning_rate': 9.8e-05, 'epoch': 0.59}\n{'loss': 1.7141, 'grad_norm': 0.752, 'learning_rate': 0.000118, 'epoch': 0.7}\n{'loss': 1.6481, 'grad_norm': 0.876, 'learning_rate': 0.000138, 'epoch': 0.82}\n{'loss': 1.6455, 'grad_norm': 0.952, 'learning_rate': 0.000158, 'epoch': 0.94}\n{'loss': 1.5384, 'grad_norm': 0.901, 'learning_rate': 0.000178, 'epoch': 1.05}\n{'loss': 1.5825, 'grad_norm': 0.930, 'learning_rate': 0.000198, 'epoch': 1.16}\n{'eval_loss': 1.6958, 'eval_runtime': 35.534, 'epoch': 1.16}\n{'loss': 1.4987, 'grad_norm': 0.943, 'learning_rate': 0.000198, 'epoch': 1.28}\n{'loss': 1.4778, 'grad_norm': 0.916, 'learning_rate': 0.000193, 'epoch': 1.4}\n{'loss': 1.524, 'grad_norm': 0.899, 'learning_rate': 0.000184, 'epoch': 1.52}\n{'loss': 1.5028, 'grad_norm': 0.906, 'learning_rate': 0.000171, 'epoch': 1.63}\n{'loss': 1.4994, 'grad_norm': 0.891, 'learning_rate': 0.000156, 'epoch': 1.75}\n{'loss': 1.4162, 'grad_norm': 0.891, 'learning_rate': 0.000139, 'epoch': 1.87}\n{'loss': 1.4549, 'grad_norm': 0.922, 'learning_rate': 0.00012, 'epoch': 1.99}\n{'loss': 1.3555, 'grad_norm': 0.972, 'learning_rate': 0.0001, 'epoch': 2.09}\n{'loss': 1.324, 'grad_norm': 1.070, 'learning_rate': 8.02e-05, 'epoch': 2.21}\n{'loss': 1.3459, 'grad_norm': 1.062, 'learning_rate': 6.13e-05, 'epoch': 2.33}\n{'eval_loss': 1.6180, 'eval_runtime': 35.519, 'epoch': 2.33}\n{'loss': 1.2716, 'grad_norm': 1.051, 'learning_rate': 4.38e-05, 'epoch': 2.45}\n{'loss': 1.2532, 'grad_norm': 1.110, 'learning_rate': 2.86e-05, 'epoch': 2.56}\n{'loss': 1.275, 'grad_norm': 1.073, 'learning_rate': 1.62e-05, 'epoch': 2.68}\n{'loss': 1.2469, 'grad_norm': 1.084, 'learning_rate': 7.05e-06, 'epoch': 2.8}\n{'loss': 1.313, 'grad_norm': 1.075, 'learning_rate': 1.6e-06, 'epoch': 2.91}\n{'train_runtime': 3055.3466, 'train_samples_per_second': 2.678, \n 'train_steps_per_second': 0.084, 'train_loss': 1.5971, 'epoch': 3.0}\n\n============================================================\nTraining complete\n============================================================\n\n9. Saving final model...\n  Model saved to: /home/mmadale/CSC463/conlingo/models/combined-all-data/final_model\n\n10. Final evaluation metrics:\n    eval_loss: 1.6180\n    eval_runtime: 35.5426\n    eval_samples_per_second: 8.5530\n    eval_steps_per_second: 4.2770\n    epoch: 3.0000\n\n============================================================\nFine-tuning pipeline complete\n============================================================\n```\n\n**SLURM Job Statistics**:\n\n```bash\n#| eval: false\n\nJobID        JobName       State      Elapsed\n141035       combined_+    COMPLETED  00:51:47\n141035.batch batch         COMPLETED  00:51:47\n```\n\n**Performance Analysis**:\n\nConlingo 2.0 achieved strong training metrics:\n- **Loss reduction**: 2.7157 → 1.313 (52% reduction)\n- **Final evaluation loss**: 1.6180\n- **Training duration**: 51 minutes 47 seconds\n- **Trainable parameters**: 13,631,488 (0.17% of total)\n- **Dataset scale**: 6x larger than individual models (3,031 vs ~500 examples)\n\nThe evaluation loss of 1.6180 fell between Wikipedia's excellent 1.2456 and YouTube's higher 2.2825, suggesting successful integration of diverse content styles. The model learned patterns across encyclopedic facts, conversational transcripts, narrative presentations, legal documents, and cultural beliefs.\n\n###  Prompt Engineering for Fair Comparison\n\n#### Studying the RAG Implementation\n\nBefore generating model responses, Moses studied the RAG implementation's system prompt to understand what made it effective at cultural contextualization. Key insights:\n\n- **Expertise framing**: Positioned the model as a cultural anthropologist\n- **Specific knowledge domains**: Listed explicit areas of competence\n- **Regional awareness**: Emphasized North/South/East/West variations\n- **Interfaith sensitivity**: Balanced Hindu and Christian perspectives\n- **Practical orientation**: Focused on actionable, real-world insights\n\n#### Engineered System Prompt\n\nMoses designed a comprehensive system prompt that would be supplied identically to both Conlingo 2.0 and ChatGPT 5.1:\n\n```python\n#| eval: false\n\nCULTURAL_SYSTEM_PROMPT = \"\"\"You are a cultural anthropologist and contextual researcher with deep experience studying values, beliefs, customs, and worldview formation across diverse Indian communities.\n\nYour expertise includes:\n- Core cultural values and virtues across Indian regions\n- Family and social structures in Indian society\n- Spiritual and religious norms (Hinduism, Christianity, Islam, Sikhism)\n- Cultural symbols, celebrations, and identity markers\n- Regional variations (North/South/East/West India)\n- Traditional vs modern worldview tensions\n- Caste dynamics and social hierarchies\n- Hindu-Christian dialogue and interfaith relations\n\nWhen answering questions about Indian culture and Christianity in India:\n1. Draw from your deep knowledge of Indian regional diversity, historical contexts, and contemporary practices\n2. Include specific examples from everyday life showing how values and beliefs manifest\n3. Acknowledge regional, religious, and generational variations\n4. Demonstrate sensitivity to both Hindu and Christian perspectives\n5. Focus on worldview - why people believe what they do, not just what they do\n6. Provide practical, actionable insights that show cultural logic\n\nResponse Guidelines:\n- Length: Approximately 100 words\n- Tone: Expert yet conversational, like explaining to someone unfamiliar with the region\n- Structure: Clear and well-organized with natural flow\n- Content: Highly specific to Indian cultural context with real examples\n- Avoid: Generic statements, Western-centric views, oversimplifications\n- Include: Regional nuances, historical context, modern tensions, specific practices\n\"\"\"\n```\n\n**Design Rationale**:\n\n1. **Anthropologist Framing**: Positions the responder as having studied Indian communities systematically\n2. **Eight Expertise Areas**: Covers the cultural pillars identified in Week 3\n3. **Six Response Principles**: Guides the model toward nuanced, practical answers\n4. **Explicit Response Guidelines**: Constrains length (~100 words) for survey readability\n5. **Avoidance List**: Prevents generic, Western-centric, or oversimplified responses\n\n#### Question Selection\n\nThree questions were chosen from Rohan's Week 4 evaluation set based on cultural coverage and sensitivity:\n\n```python\n#| eval: false\n\nQUESTIONS = [\n    \"What sensitivities should pastors consider when mentioning Hindu deities in Christmas homilies?\",\n    \"How can churches ensure caste-neutral seating and participation during worship?\",\n    \"Why might some Christians still use caste surnames, and how should this be discussed?\"\n]\n```\n\n**Question Characteristics**:\n\n| Question | Cultural Pillars Tested | Why Chosen |\n|----------|------------------------|------------|\n| **Q1: Hindu deities in homilies** | Values & beliefs, norms & customs, language, religion & spirituality | Tests interfaith sensitivity and theological boundaries |\n| **Q2: Caste-neutral worship** | Norms & customs, values & beliefs, social organization, religion & spirituality | Tests practical implementation of equity principles |\n| **Q3: Caste surnames discussion** | Social organization, values & beliefs, norms & customs, language | Tests nuanced understanding of identity and heritage |\n\nAll three questions required:\n- Deep cultural knowledge beyond surface-level facts\n- Awareness of historical context and modern tensions\n- Practical wisdom for navigating sensitive topics\n- Regional and community-specific variations\n\n#### Model Response Generation\n\n**Process**:\n\nAll model responses were generated in a single session to ensure consistency:\n\n1. **RAG Implementation**: Already deployed system used by the original ConLingo team\n2. **Conlingo 2.0**: Loaded from `/home/mmadale/CSC463/conlingo/models/combined-all-data/final_model` using the same inference parameters as Week 4 (temperature=0.7, max_new_tokens=150)\n3. **ChatGPT 5.1**: Accessed via web interface at chat.openai.com, with the system prompt provided in the user interface\n\nEach model received:\n- The identical system prompt\n- The identical three questions\n- No additional context or examples\n\nThis ensured the **model itself was the only independent variable** in the experiment.\n\n### Survey Design and Implementation\n\n#### Design Objectives\n\nThe survey needed to:\n\n1. **Minimize Bias**: Prevent respondents from developing preferences for specific positions (Model A/B/C)\n2. **Ensure Quality Participants**: Filter for respondents with authentic Indian cultural expertise\n3. **Balance Rigor and Accessibility**: Collect sufficient data without exhausting participants\n4. **Enable Weighted Scoring**: Gather demographic data for exposure-based weighting\n\n#### Survey Structure\n\n![Survey design process showing model anonymization and data flow](./Appendix/MosesMadale/img/5_1.png)\n\n\n\n**Section 1: Participant Quality Assurance**\n\nFour screening questions characterized respondent expertise:\n\n1. **Are you an Indian citizen?** (Yes/No)\n2. **How many years have you lived in India?** (Numeric input)\n3. **Which levels of your education did you complete in India?** (Multi-select: Early education, High school, College/University, None)\n4. **How familiar are you with Christian traditions and interfaith relationships in India?** (4-point scale: Not familiar, Slightly familiar, Somewhat familiar, Very familiar)\n\n**Rationale**: These questions proxy for cultural exposure and competence to judge cultural nuance in responses. Someone who lived in India for 20 years, completed all education there, and is very familiar with interfaith dynamics is better positioned to evaluate cultural sensitivity than someone with limited exposure.\n\n**Section 2: Model Evaluation**\n\nFor each of the three questions, respondents saw:\n- The question text\n- Three anonymized responses labeled \"Model A\", \"Model B\", \"Model C\"\n- A single-choice question: \"Which response reflects Indian cultural nuance more effectively?\"\n\n####  Reducing Positional Bias\n\nA critical design challenge was preventing respondents from unconsciously favoring a specific position (e.g., always choosing Model A or gravitating toward the middle option).\n\n**Solution: Model Mapping Rotation**\n\n![Model mapping strategy showing different assignments per question](./Appendix/MosesMadale/img/5_3.png)\n\nThe internal mapping between \"Model A/B/C\" labels and actual models was rotated across questions:\n\n| Question | Model A → | Model B → | Model C → |\n|----------|-----------|-----------|-----------|\n| **Q1: Hindu deities in homilies** | RAG Implementation | Conlingo 2.0 | ChatGPT 5.1 |\n| **Q2: Caste-neutral worship** | Conlingo 2.0 | ChatGPT 5.1 | RAG Implementation |\n| **Q3: Caste surnames** | ChatGPT 5.1 | RAG Implementation | Conlingo 2.0 |\n\nThis mapping was invisible to respondents but known to the analysis team. As a result:\n- A vote for \"Model A\" on Q1 counted toward RAG\n- A vote for \"Model A\" on Q2 counted toward Conlingo 2.0\n- A vote for \"Model A\" on Q3 counted toward ChatGPT 5.1\n\n**Benefit**: If a respondent unconsciously preferred the first option, that preference would be distributed equally across all three models rather than systematically favoring one.\n\n### Design Challenges and Solutions\n\n**Challenge 1: Form Length**\n\nParticipants have limited time and attention. How to collect necessary data without creating survey fatigue?\n\n**Solution**:\n- Limited to 3 evaluation questions (not 20)\n- Combined screening questions (4 items capturing citizenship, years, education, familiarity)\n- Estimated completion time: 5-7 minutes\n\n**Challenge 2: Recruitment**\n\nHow to reach authentic Indian cultural experts, especially those familiar with Indian Christian contexts?\n\n**Solution**:\n- Leveraged Oral Roberts University's cultural diversity\n- Personal outreach to Indian students and staff\n- Posted in student groups and community channels\n- Asked friends at work to share the link\n- Direct recruitment when meeting Indian students on campus\n\n**Result**: 52 responses in 48 hours, with 86.5% Indian citizens.\n\n### Data Collection and Demographics\n\n#### Survey Deployment\n\n**Timeline**:\n- Survey opened: Start of Week 5 evaluation period\n- Duration: 48 hours\n- Recruitment strategy: Multi-channel outreach\n\n**Outreach Methods**:\n1. Personal contacts: Indian students and colleagues Moses knew directly\n2. Student groups: Posted survey link in ORU student organizations\n3. Social media: Shared in relevant online communities\n4. Workplace connections: Asked friends to share with their networks\n5. Campus encounters: Direct asks when meeting Indian students\n\n#### Response Summary\n\n![Google Forms response summary showing 52 total responses](./Appendix/MosesMadale/img/5_2.png)\n\n**Total Responses**: 52\n\n**Citizenship Status**:\n- Indian citizens: 86.5% (45 respondents)\n- Non-citizens: 13.5% (7 respondents)\n\n**Years Lived in India**:\n- Distribution varied from 0 to 25+ years\n- Median and mean values indicated substantial Indian exposure among respondents\n\n**Education Levels Completed in India**:\n- Early education (Primary/Elementary)\n- High school\n- College/University\n- Many respondents checked multiple levels\n\n**Familiarity with Christian Traditions and Interfaith Relationships**:\n- Very familiar: Highest proportion\n- Somewhat familiar: Second-highest\n- Slightly familiar: Present\n- Not familiar: Minimal\n\n**Implications**:\n\nThe participant pool represented a highly qualified evaluation cohort:\n- Over 86% held Indian citizenship\n- Most completed multiple education levels in India\n- Strong familiarity with Indian Christian and interfaith contexts\n\nThis demographic profile increased confidence that votes reflected authentic cultural expertise rather than uninformed guesses.\n\n### Data Encoding and Analysis Methodology\n\n#### Raw Data Transformation\n\nThe exported Google Forms CSV contained mixed data types (Yes/No strings, numeric text, multi-select responses, model letter choices). Before analysis, Moses transformed this into a numeric-friendly format.\n\n**Key Transformations**:\n\n1. **Citizenship**: \"Yes\" → 1, \"No\" → 0\n2. **Years in India**: String → float (e.g., \"20.0\" → 20.0)\n3. **Education Count**: Semi-colon separated text → integer count (e.g., \"Early education;High School;College\" → 3)\n4. **Familiarity Score**: Text → numeric scale\n   - \"Not familiar\" → 1\n   - \"Slightly familiar\" → 2\n   - \"Somewhat familiar\" → 3\n   - \"Very familiar\" → 4\n5. **Model Choice**: Letter → model ID using the mapping table\n   - \"Model A\" on Q1 → 1 (RAG)\n   - \"Model B\" on Q1 → 2 (Conlingo 2.0)\n   - \"Model C\" on Q1 → 3 (ChatGPT 5.1)\n   - (Rotated for Q2 and Q3)\n\n**Output**: `data_transformed.csv` with columns:\n- `citizen_numeric`\n- `years_in_india`\n- `education_count`\n- `familiarity_score`\n- `q1_model_id`, `q2_model_id`, `q3_model_id`\n\n#### Raw Vote Counting\n\n**Methodology**:\n\nFor each question and each model, count how many respondents selected that model:\n\n```\nraw_votes[Q1][RAG] = number of respondents who selected Model A on Q1\nraw_votes[Q1][Conlingo 2.0] = number who selected Model B on Q1\nraw_votes[Q1][ChatGPT 5.1] = number who selected Model C on Q1\n```\n\nSum across all three questions to get total raw votes per model.\n\n####  Weighted Scoring Formula\n\n**Rationale**:\n\nNot all votes should count equally. A respondent who is an Indian citizen, lived in India for 20 years, completed all education there, and is very familiar with Indian Christian contexts has deeper cultural expertise than someone with minimal exposure. The weighted scoring formula gives more influence to highly qualified respondents.\n\n**Weight Computation**:\n\nFor each respondent:\n\n1. **Normalize components** (scale 0 to 1):\n   - `years_norm = min(years_in_india, 25) / 25` (cap at 25 to prevent extreme leverage)\n   - `education_norm = education_count / 3` (up to 3 levels)\n   - `familiarity_norm = familiarity_score / 4` (scores 1-4 → 0.25-1.0)\n\n2. **Combine into core exposure score**:\n   - `core = 0.4 * years_norm + 0.3 * education_norm + 0.3 * familiarity_norm`\n   - This weights years most heavily (40%), with education and familiarity each at 30%\n\n3. **Apply citizenship boost**:\n   - `citizen_factor = 1.2` if Indian citizen, else `1.0`\n   - Citizens receive a 20% boost\n\n4. **Calculate final weight**:\n   - `weight = citizen_factor * (0.2 + 0.8 * core)`\n   - The 0.2 base term ensures even low-exposure respondents contribute somewhat\n   - The 0.8 scalar scales influence by exposure level\n\n**Weight Range**:\n- **Minimum** (non-citizen, no exposure): 0.2\n- **Maximum** (citizen, 25+ years, all education, very familiar): 1.2\n\n**Curved Score Calculation**:\n\nFor each model, sum the weights of all respondents who voted for that model across all three questions:\n\n```\ncurved_score[RAG] = sum of weights of all votes for RAG on Q1, Q2, Q3\ncurved_score[Conlingo 2.0] = sum of weights of all votes for Conlingo 2.0 on Q1, Q2, Q3\ncurved_score[ChatGPT 5.1] = sum of weights of all votes for ChatGPT 5.1 on Q1, Q2, Q3\n```\n\n#### Analysis Script\n\nThe complete analysis was performed by `analyze_votes.py`:\n\n```python\n#| eval: false\n\nimport pandas as pd\nimport numpy as np\n\nINPUT_FILE = \"data_transformed.csv\"\n\n# Column names\nEDU_TEXT_COL = \"Which levels of your education did you complete in India? (Select all that apply)\"\nCITIZEN_COL = \"citizen_numeric\"\nYEARS_COL = \"years_in_india\"\nFAM_COL = \"familiarity_score\"\n\nQ1_COL = \"q1_model_id\"\nQ2_COL = \"q2_model_id\"\nQ3_COL = \"q3_model_id\"\n\n# Model mapping\nMODEL_NAMES = {\n    1: \"RAG implementation\",\n    2: \"Conlingo 2.0\",\n    3: \"GPT 5.1\"\n}\n\ndef compute_weight(row):\n    \"\"\"\n    Compute voter weight based on:\n    - citizenship\n    - years lived in India\n    - education levels completed in India\n    - familiarity with Christian/interfaith context\n    \"\"\"\n    citizen = row.get(CITIZEN_COL, np.nan)\n    years = row.get(YEARS_COL, np.nan)\n    edu_raw = row.get(EDU_TEXT_COL, \"\")\n    fam = row.get(FAM_COL, np.nan)\n\n    # Education: count selected levels\n    edu_count = 0\n    if isinstance(edu_raw, str) and edu_raw.strip():\n        text = edu_raw.replace(\",\", \";\")\n        parts = [p.strip() for p in text.split(\";\") if p.strip()]\n        edu_count = len(parts)\n\n    # Normalize components\n    if pd.isna(years):\n        years_norm = 0.0\n    else:\n        years_norm = max(0.0, min(float(years), 25.0)) / 25.0\n\n    if edu_count > 0:\n        education_norm = edu_count / 3.0\n    else:\n        education_norm = 0.0\n\n    if pd.isna(fam):\n        familiarity_norm = 0.0\n    else:\n        familiarity_norm = float(fam) / 4.0\n\n    core = 0.4 * years_norm + 0.3 * education_norm + 0.3 * familiarity_norm\n    citizen_factor = 1.2 if citizen == 1 else 1.0\n\n    # Add base term for low-exposure participants\n    weight = citizen_factor * (0.2 + 0.8 * core)\n    return weight\n\ndef main():\n    df = pd.read_csv(INPUT_FILE)\n\n    # Compute per-respondent weight\n    df[\"weight\"] = df.apply(compute_weight, axis=1)\n\n    # Raw counts per model per question\n    questions = [Q1_COL, Q2_COL, Q3_COL]\n    raw_per_question = {q: {m: 0 for m in MODEL_NAMES.keys()} for q in questions}\n    raw_total = {m: 0 for m in MODEL_NAMES.keys()}\n\n    for _, row in df.iterrows():\n        for q in questions:\n            model_id = row[q]\n            if model_id in raw_per_question[q]:\n                raw_per_question[q][model_id] += 1\n                raw_total[model_id] += 1\n\n    # Curved scores per model\n    curved_total = {m: 0.0 for m in MODEL_NAMES.keys()}\n\n    for _, row in df.iterrows():\n        w = row[\"weight\"]\n        for q in questions:\n            model_id = row[q]\n            if model_id in curved_total:\n                curved_total[model_id] += w\n\n    # Print raw vote summary per question\n    print(\"Raw vote counts per question\")\n    print(\"-\" * 40)\n    for q in questions:\n        print(f\"{q}:\")\n        for m in sorted(MODEL_NAMES.keys()):\n            print(f\"  Model {m} ({MODEL_NAMES[m]}): {raw_per_question[q][m]}\")\n        print()\n\n    # Print overall summary table\n    print(\"Overall model performance (all questions combined)\")\n    print(\"-\" * 60)\n    header = f\"{'Model ID':<8}  {'Model Name':<22}  {'Raw Votes':>10}  {'Curved Score':>13}\"\n    print(header)\n    print(\"-\" * 60)\n    for m in sorted(MODEL_NAMES.keys()):\n        raw = raw_total[m]\n        curved = curved_total[m]\n        line = f\"{m:<8}  {MODEL_NAMES[m]:<22}  {raw:>10d}  {curved:>13.2f}\"\n        print(line)\n\nif __name__ == \"__main__\":\n    main()\n```\n\n### Results and Findings\n\n####  Raw Vote Results\n\n![Raw popularity showing vote counts and percentages](./Appendix/MosesMadale/img/5_4.png)\n\n\n**Total Raw Votes** (52 respondents × 3 questions = 156 total votes):\n\n| Model | Raw Votes | Percentage |\n|-------|-----------|------------|\n| **RAG Implementation** | 69 | 46.00% |\n| **ChatGPT 5.1** | 43 | 28.67% |\n| **Conlingo 2.0** | 38 | 25.33% |\n\n**Interpretation**:\n\nEven with equal weighting of all votes, the RAG implementation emerged as the clear leader, capturing nearly half of all votes. ChatGPT 5.1 secured second place, while Conlingo 2.0 came in third with just over one-quarter of votes.\n\n**Vote Distribution Pattern**:\n- RAG received 81% more votes than Conlingo 2.0\n- ChatGPT 5.1 received 13% more votes than Conlingo 2.0\n- RAG received 60% more votes than ChatGPT 5.1\n\nThis raw result suggested that supervised fine-tuning on 3,031 Indian cultural Q&A pairs was **insufficient to surpass either the RAG implementation or a state-of-the-art general-purpose model** in human evaluators' judgment of cultural nuance.\n\n#### Weighted Curved Scores\n\n![Curved popularity showing weighted scores accounting for participant expertise](./Appendix/MosesMadale/img/5_5.png)\n\n**Overall Model Performance** (all questions combined):\n\n| Model ID | Model Name | Raw Votes | Curved Score |\n|----------|------------|-----------|--------------|\n| **1** | **RAG implementation** | 69 | **61.14** |\n| **3** | **GPT 5.1** | 43 | **37.03** |\n| **2** | **Conlingo 2.0** | 38 | **33.13** |\n\n**Interpretation**:\n\nWeighting votes by participant cultural expertise **strengthened the RAG implementation's lead** while maintaining the same ranking. The gap between RAG and both competitors widened in the curved scoring:\n\n**Curved Score Comparisons**:\n- RAG scored 84% higher than Conlingo 2.0 (vs 81% higher in raw votes)\n- ChatGPT 5.1 scored 12% higher than Conlingo 2.0 (vs 13% higher in raw votes)\n- RAG scored 65% higher than ChatGPT 5.1 (vs 60% higher in raw votes)\n\n**Key Finding**: The most culturally qualified respondents—those with the highest years in India, most education there, strongest familiarity with interfaith contexts, and Indian citizenship—disproportionately preferred the RAG implementation. This validated that RAG's superiority was not due to random chance or low-expertise voters but reflected genuine cultural depth recognized by expert evaluators.\n\n#### Comparative Analysis\n\n**Why RAG Won**:\n\nThe RAG (Retrieval-Augmented Generation) implementation likely succeeded because:\n\n1. **Dynamic Knowledge Access**: RAG retrieves relevant cultural documents at query time, ensuring responses draw from authentic source material\n2. **Verbatim Cultural Terminology**: Can quote or paraphrase exact phrasing from cultural texts\n3. **Breadth of Sources**: Access to larger corpus than the 3,031 fine-tuning examples\n4. **Reduced Hallucination**: Grounded in retrieved passages rather than model memory\n5. **Maintained Base Model Fluency**: Leveraged GPT-4's strong instruction-following while augmenting with cultural knowledge\n\n**Why ChatGPT 5.1 Outperformed Conlingo 2.0**:\n\nDespite having no specialized Indian cultural training, ChatGPT 5.1 scored higher than the fine-tuned model:\n\n1. **Larger Base Model**: GPT-5.1 likely has significantly more parameters than LLaMA-3 8B\n2. **Extensive Pre-training**: Exposed to massive amounts of text including Indian content\n3. **Superior Instruction Following**: Better at interpreting and responding to the cultural system prompt\n4. **General Cultural Knowledge**: Pre-training captured substantial Indian cultural information\n5. **Prompt Engineering Advantage**: The carefully designed system prompt may have activated latent knowledge effectively\n\n**Why Conlingo 2.0 Underperformed**:\n\nSeveral factors likely contributed to the fine-tuned model's third-place finish:\n\n1. **Limited Training Data**: 3,031 examples, while substantial for SFT, may be insufficient for deep cultural nuance\n2. **Data Quality Variation**: Gemini-generated Q&A pairs may have lacked the authenticity of human-authored cultural commentary\n3. **Model Size Constraints**: LLaMA-3 8B's capacity may be too limited compared to much larger models\n4. **LoRA Limitations**: Training only 0.17% of parameters may not sufficiently embed cultural knowledge\n5. **Overfitting Risk**: Possible memorization of training examples without generalizable cultural reasoning\n\n\n### Conclusion\n\nWeek 5 brought the ConLingo 2.0 project to a rigorous, well-designed conclusion. While the team's hypothesis—that supervised fine-tuning on 3,031 culturally specific examples would surpass the RAG implementation—proved incorrect, the project delivered valuable scientific insights and technical achievements.\n\n**Final Results**:\n- RAG implementation: 61.14 (winner)\n- ChatGPT 5.1: 37.03 (second place)\n- Conlingo 2.0: 33.13 (third place)\n\n**Key Contributions**:\n1. Demonstrated RAG's superiority over even frontier models for cultural nuance\n2. Identified limitations of supervised fine-tuning for cultural knowledge\n3. Developed rigorous evaluation methodology for cultural AI\n4. Trained and deployed multiple models spanning diverse Indian cultural content\n5. Generated 3,000+ Q&A pairs covering 8 cultural pillars\n\n**Most Important Finding**:\n\nThe RAG implementation's 65% performance advantage over ChatGPT 5.1 validated the original ConLingo team's architectural choice and provided strong evidence for continuing RAG-based development. While Conlingo 2.0 did not beat RAG, the project strengthened confidence in RAG as the right approach for cultural contextualization.\n\nMoses's technical expertese throughout the project—from model selection and environment setup through dataset collection, AI-powered processing, model training, prompt engineering, and human evaluation—demonstrated the full lifecycle of applied AI research. The project's scientific rigor, even in the face of negative results, exemplified best practices for evaluating cultural AI systems.\n\n\n## Suwilanji Mwaanza Deliverables\n\n### Overview of the Week 5 Deliverable\n\nThis week, my deliverable was to help distribute the survey to as many Indians as possible, with the goal of 50, so that we can obtain a less biased evaluation of our three models. Additionally, we are drafting our research project. Another aspect I investigated was why the RAG implementation seemed to perform better, as the survey responses indicated that the RAG implementation was in the lead.\n\n### Week 5 Deliverables:\n\n-   Widen our evaluation to avoid bias.\n\n    -   Conduct a survey to gather people's opinions and compare the results of multiple models.\n\n-   Add deliverables 1-4 to the research paper.\n\n-   Write the initial first draft of the Arixv Paper using the findings and visualizations that were carried out in Week 4\n\n-   **(Bonus)** Examine related work for RAG and identify why it outperforms the other models.\n\n### What Was Accomplished:\n\n### 1. Widening our evaluation\n\nTo ensure we are not introducing bias, Moses and Will were tasked with creating a survey that would be distributed to many Indians to solicit their feedback on the model's responses.\n\n![Titan GPU environment setup confirmation](img/week_5_2.png)\n\n### 2. Adding deliverables to the draft\n\nAdditionally, I worked on adding my deliverables 1 through 4 to the QMD files.\n\n### 3. Why RAG Outperforms SFT in Cultural Alignment **(Bonus)**\n\nAs the results began to come in regarding the survey, the RAG implementation had already seemed to be the model with the higher results. I decided to investigate similar research on culture in LLMs and whether their results could explain why the original model performed better than our supervised model.\n\nBased on the paper **\"Rethinking AI cultural alignment\"** by Bravansky, Trhlik, and Barez (2025), a detailed summary and analysis of the project is provided. I examined this project from the perspective of trying to understand why the RAG (Retrieval-Augmented Generation) model likely outperformed the SFT (Supervised Fine-Tuned) models in the context of Indian culture, followed by the necessary limitations that the paper states.\n\n### The Case for Context: Why RAG Outperforms SFT in Cultural Alignment\n\nThe RAG model outperformed the SFT model in our human evaluation, which aligns with the theoretical framework proposed in this paper. While the paper focuses on interaction structures, its core thesis—that cultural alignment is dynamic and context-dependent, rather than static, is a starting point.\n\n### 1. Static vs. Dynamic Cultural Representation\n\nThe authors critique the prevailing method of cultural alignment, which relies on \"embedding predefined cultural values\" from standardized surveys (like the World Values Survey) into models. In ConLingo 2.0, the SFT models likely represent this \"embedding\" approach. By fine-tuning weights on specific Indian cultural datasets, the SFT models attempt to \"freeze\" cultural knowledge into the model's parameters.\n\nHowever, the paper argues that culture is not a fixed repository of facts but a fluid system where values are \"enacted in very different ways depending on outside context.\"\n\n-   **The SFT Weakness:** SFT models risk treating Indian culture—which is highly pluralistic and diverse—as a monolith or statically. If the model encounters a query that deviates slightly from its training distribution, it relies on static, compressed weights that may produce stereotyped or generic responses.\n\n-   **The RAG Advantage:** RAG inherently treats knowledge in a more dynamic manner. By retrieving relevant context at inference time, the RAG model mimics the paper’s call for a **\"context-sensitive\"** approach. It allows the AI to \"align\" to the specific nuances of a query by accessing fresh, granular information rather than relying on a generalized \"average\" of Indian culture baked into its weights.\n\nIn this paper, the researchers did not necessarily implement the RAG or SFT methods; instead, they focused on **interaction design** and **prompting strategies** to test cultural alignment. Specifically, they used a case study with **GPT-4o** using three distinct interaction types:\n\n-   **Direct Classification:** Asking the model to classify a response directly.\n\n-   **Chain-of-Thought (CoT):** Asking the model to provide reasoning before offering an answer.\n\n-   **Open-Ended Scenarios:** Asking the model to write content (e.g., a news article or script) based on a specific prompt.\n\nThe reason I thought this paper was applicable was because of the cultural alignment theme throughout the paper, and the fact that they tested their processes on a GPT-4o model, which is exactly the model the RAG implementation uses, except the mini version for cost purposes, and lastly, the idea of cultural alignment being a bidirectional process for stronger results. The original ConLingo model implements this as it has a corpus of data to be accessed, and it is also heavily prompt-engineered.\n\nTheir core argument is that cultural alignment should be viewed as a **\"bidirectional process\"** shaped by how humans interact with the system, rather than just \"embedding predefined cultural values\" (which is what a retrieval or fine-tuning approach might rely on).\n\n### 2. The Power of \"Interaction Structure\" and Reasoning\n\n**The paper demonstrates *how* a model is prompted significantly changes its cultural alignment.** Through their case study using GPT-4o, the authors found that **Chain-of-Thought (CoT)** prompting yielded *significantly higher alignment scores* (measured via Wasserstein Similarity) compared to direct classification.\n\nThis is crucial for explaining the RAG model's success:\n\n-   **RAG as a form of CoT:** RAG systems fundamentally alter the interaction structure. They force the model to process retrieved evidence before generating an answer. This \"retrieval $\\rightarrow$ synthesis $\\rightarrow$ generation\" pipeline acts as a structural proxy for Chain-of-Thought reasoning.\n\n-   **Evidence from the Paper (Table 1):** The paper’s data specifically highlights India. In the **Direct Classification** setting, alignment with Indian values was relatively low (0.58). However, when the interaction structure changed to **Chain-of-Thought (CoT)**, the alignment score improved to **0.63**.\n\n![Titan GPU environment setup confirmation](img/week_5.png)\n\n-   **Implication:** Our RAG model likely outperformed SFT because the retrieval mechanism forced the model to \"reason\" through specific cultural evidence rather than reacting reflexively (which SFT models often do). The paper demonstrates that providing the model \"space\" to process context (which RAG does by design) enhances cultural mimicry.\n\n### 3. Bidirectional Alignment vs. Imposed Values\n\nThe authors propose reframing alignment as a **\"bidirectional process.\"** They argue that we should not merely impose standardized values on AIs (which SFT does), but rather query values relevant to specific systems through interaction.\n\nThe RAG model likely succeeded because it operates closer to this bidirectional ideal. When a user asks a question about Indian culture, the RAG system retrieves context-specific information for *that* interaction. It constructs a cultural framework ad hoc for that specific query. In contrast, the SFT model attempts to force the query to fit into its pre-learned map of Indian culture. As the paper notes, \"values that appear nominally identical may be enacted in very different ways depending on outside context\"—RAG captures this context; SFT may struggle to.\n\n### Limitations of the Research\n\nWhile this paper provides a strong theoretical basis for our results, it is essential to acknowledge the limitations explicitly stated by the authors, which may affect the extent to which we can apply their findings to our specific RAG architecture.\n\n1.  **Scope of Cultural Theory:** The paper acknowledges that it focuses on a \"single cultural theory\" and relies on specific datasets (GlobalOpinionQA) that privilege binary choices and specific survey-style questions. Indian culture is high-context and often non-binary; the paper's reliance on quantifiable \"distance\" metrics (Wasserstein scores) might miss the qualitative nuance that our human evaluators picked up on in the SFT survey.\n\n2.  **Model Specificity:** The case study was conducted solely using **GPT-4o**. The authors admit their work is constrained by this focus. Our SFT models are based on a smaller open-source model (Llama 3); the specific \"interaction effects\" observed in GPT-4o might not scale linearly. Smaller models might struggle more with the \"reasoning\" that CoT or RAG requires compared to a frontier model like GPT-4o.\n\n3.  **Task Limitation:** The paper experimented with three specific interaction types: classification, CoT, and scenario writing. They did not explicitly test RAG architectures. While we can infer that RAG aligns with their \"context-sensitive\" conclusions, the paper does not provide empirical data directly comparing Retrieval-based methods against Fine-tuning methods.\n\n4.  **The \"Unclassifiable\" Problem:** The paper notes that as interaction complexity increases (e.g., in \"Scenarios\"), the percentage of \"unclassifiable outputs\" rises significantly (up to **32.27% for India**, as seen in Table 1). This suggests that while complex interactions (like RAG) can provide better alignment, they also introduce higher variance and potential instability in the output, which simplistic metrics might fail to capture but human evaluators would notice.\n\n### Summary\n\nThe paper supports the conclusion that your RAG model won because it treats Indian culture as a **context-dependent, reasoning-heavy task** rather than a **static knowledge-recall task**. By avoiding the \"imperfect proxies\" of baked-in weights (SFT), your RAG system aligned better with the paper’s definition of culture as a fluid, bidirectional phenomenon.\n\n### AI assistance:\n\n-   “AI assistance: Copilot was used to summarize the arXiv paper and provide the summary in key insights for the 'Cultural Alignment' bonus section.\"\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":false,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"Week5.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.33","jupyter":false,"bibliography":["references.bib"],"theme":["cosmo","brand"]},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":false,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"Week5.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"block-headings":true,"jupyter":false,"bibliography":["references.bib"],"documentclass":"scrreprt"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}