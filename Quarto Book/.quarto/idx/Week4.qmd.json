{"title":"Week 4: Data Searching, Data Vetting, Dataset Collection, Data Cleaning","markdown":{"yaml":{"execute":{"echo":false,"eval":false,"warning":false}},"headingText":"Week 4: Data Searching, Data Vetting, Dataset Collection, Data Cleaning","containsRefs":false,"markdown":"\n\n\n\n## Overview\nIn Week 4, the team fine-tuned multiple models on Titan using various datasets, tested them with 20 selected questions, and analyzed performance across metrics like tone, empathy, and accuracy. Rohan led the performance analysis, while Moses and William handled model training, and Suwilanji created visualizations, investigated dataset issues, and began drafting the research paper and Week 4 presentation.\n\n## Rohan Aby Deliverables\n### CSI Scoring of superstitions and constitution dataset\nRohan was tasked with scoring the output from the superstitions and constitution dataset. The same twenty questions that were used for the other models were selected. The results showed us that the constitution model performed better. It had a score of 92%. The superstitions had a score of 79%. This is contrary to what we expected as we felt that the data that was used to train the superstitions model was more reliable than the constitution dataset. The results can be seen in the image below. \n![superstitions vs constitution model](Appendix/RohanAby/Week4RohanAby.png)\n\n\n## William Richards Deliverables\n\n### Individual Fine-Tuned Model Testing\n\nWills tasks for this deliverable was to alter Moses' fine-tuning program for the Superstition and Constitution datasets to create fine-tuned models for each dataset:\n\n#### Superstitions Dataset:\n\n```bash\n# Superstitions Fine-Tuning Model Bash Script\n\n#!/bin/bash\n#SBATCH --job-name=super_finetune\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=64G\n#SBATCH --gres=gpu:1\n#SBATCH --time=6:00:00\n#SBATCH --output=logs/super_finetune_%j.out\n#SBATCH --error=logs/super_finetune_%j.err\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=starwill16@gmail.com\n\nmodule purge\nmodule load Python/3.12.3-GCCcore-13.3.0\nmodule load CUDA/12.3.0\n\nsource /home/gcsc563_01/CSC463/conlingo/llama_env/bin/activate\n\nexport HF_HOME=/home/gcsc563_01/.cache/huggingface\nexport HUGGINGFACE_HUB_CACHE=/home/gcsc563_01/.cache/huggingface\n\ncd /home/gcsc563_01/conlingo\n\npython scripts/training/finetune_indian_superstitions.py\n\ndeactivate\n```\n\n```python\n# Superstitions Fine-Tuning Model Python Script\n\n#!/usr/bin/env python3\n\"\"\"\nLoRA Fine-Tuning for the Indian Superstitions Data\nFine-tunes LLaMA-3 8B on Indian Superstitions Q&A pairs\n\"\"\"\n\nimport torch\nimport json\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling\n)\nfrom peft import LoraConfig, get_peft_model\nfrom datasets import Dataset\nimport os\nfrom sklearn.model_selection import train_test_split\n\nprint(\"=\"*60)\nprint(\"Superstitions LoRA Fine-Tuning Pipeline\")\nprint(\"=\"*60)\n\n# Paths\nDATA_PATH = \"/home/gcsc563_01/conlingo/data/raw/superstition_qa.jsonl\"\nOUTPUT_DIR = \"/home/gcsc563_01/conlingo/models/finetuned/superstition\"\nMODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\n# Create output directory\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"\\n1. Loading data from: {DATA_PATH}\")\n\n# Load JSONL data\ndata = []\nwith open(DATA_PATH, 'r', encoding='utf-8') as f:\n    for line in f:\n        data.append(json.loads(line))\n\nprint(f\"Total examples: {len(data)}\")\n\n# Extract question-answer pairs\nexamples = []\nfor item in data:\n    examples.append({\n        \"question\": item[\"question\"],\n        \"answer\": item[\"answer\"]\n    })\n\n# Split into train/validation (90/10)\ntrain_data, val_data = train_test_split(examples, test_size=0.1, random_state=42)\n\nprint(f\"Training examples: {len(train_data)}\")\nprint(f\"Validation examples: {len(val_data)}\")\n\nprint(f\"\\n2. Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nprint(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n\nprint(f\"\\n3. Preparing datasets...\")\n\ndef format_instruction(example):\n    \"\"\"Format question-answer pair for training\"\"\"\n    text = f\"### Question:\\n{example['question']}\\n\\n### Answer:\\n{example['answer']}\"\n    return text\n\ndef tokenize_function(example):\n    \"\"\"Tokenize examples with padding and truncation\"\"\"\n    text = format_instruction(example)\n    \n    tokenized = tokenizer(\n        text,\n        truncation=True,\n        max_length=512,\n        padding=\"max_length\",\n        return_tensors=None\n    )\n    \n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized\n\n# Convert to HuggingFace Dataset format\ntrain_dataset = Dataset.from_list(train_data)\nval_dataset = Dataset.from_list(val_data)\n\n# Tokenize\nprint(\"Tokenizing training data...\")\ntrain_dataset = train_dataset.map(\n    tokenize_function,\n    remove_columns=train_dataset.column_names\n)\n\nprint(\"Tokenizing validation data...\")\nval_dataset = val_dataset.map(\n    tokenize_function,\n    remove_columns=val_dataset.column_names\n)\n\nprint(f\"Training dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(val_dataset)}\")\n\nprint(f\"\\n4. Loading base model...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\n\nmodel.gradient_checkpointing_enable()\n\nprint(f\"Model loaded: {model.__class__.__name__}\")\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\nprint(f\"\\n5. Configuring LoRA...\")\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\n\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\n\nprint(f\"LoRA configured successfully\")\nprint(f\"Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.4f}%)\")\nprint(f\"Total parameters: {total_params:,}\")\n\nprint(f\"\\n6. Setting up training arguments...\")\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=16,\n    learning_rate=2e-4,\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=50,\n    logging_steps=10,\n    save_steps=50,\n    eval_steps=50,\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    fp16=True,\n    report_to=\"none\",\n    save_total_limit=2,\n    remove_unused_columns=False,\n    gradient_checkpointing=True\n)\n\nprint(\"Training configuration:\")\nprint(f\"  Epochs: {training_args.num_train_epochs}\")\nprint(f\"  Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  Learning rate: {training_args.learning_rate}\")\n\nprint(f\"\\n7. Initializing trainer...\")\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator\n)\n\nprint(\"Trainer initialized successfully!\")\n\nprint(f\"\\n8. Starting training...\")\nprint(\"=\"*60)\n\ntrainer.train()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Training complete!\")\nprint(\"=\"*60)\n\nprint(f\"\\n9. Saving final model...\")\nmodel.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\ntokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n\nprint(f\"Model saved to: {OUTPUT_DIR}/final_model\")\n\nprint(f\"\\n10. Final evaluation metrics:\")\neval_results = trainer.evaluate()\nfor key, value in eval_results.items():\n    print(f\"  {key}: {value:.4f}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Fine-tuning pipeline complete!\")\nprint(\"=\"*60)\n\n```\n\n#### Constitutions Dataset:\n\n``` bash\n# Constitutions Fine-Tuning Model Bash Script\n\n#!/bin/bash\n#SBATCH --job-name=constitution_finetune\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=64G\n#SBATCH --gres=gpu:1\n#SBATCH --time=6:00:00\n#SBATCH --output=logs/const_finetune_%j.out\n#SBATCH --error=logs/const_finetune_%j.err\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=starwill16@gmail.com\n\nmodule purge\nmodule load Python/3.12.3-GCCcore-13.3.0\nmodule load CUDA/12.3.0\n\nsource /home/gcsc563_01/CSC463/conlingo/llama_env/bin/activate\n\nexport HF_HOME=/home/gcsc563_01/.cache/huggingface\nexport HUGGINGFACE_HUB_CACHE=/home/gcsc563_01/.cache/huggingface\n\ncd /home/gcsc563_01/conlingo\n\npython scripts/training/finetune_indian_constitutions.py\n\ndeactivate\n```\n\n```python\n# Constitutions Fine-Tuning Model Python Script\n\n#!/usr/bin/env python3\n\"\"\"\nLoRA Fine-Tuning for the Indian Constitutions Data\nFine-tunes LLaMA-3 8B on Indian Constitutions Q&A pairs\n\"\"\"\n\nimport torch\nimport json\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling\n)\nfrom peft import LoraConfig, get_peft_model\nfrom datasets import Dataset\nimport os\nfrom sklearn.model_selection import train_test_split\n\nprint(\"=\"*60)\nprint(\"Constitutions LoRA Fine-Tuning Pipeline\")\nprint(\"=\"*60)\n\n# Paths\nDATA_PATH = \"/home/gcsc563_01/conlingo/data/raw/constitution_qa.jsonl\"\nOUTPUT_DIR = \"/home/gcsc563_01/conlingo/models/finetuned/constitution\"\nMODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\n# Create output directory\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"\\n1. Loading data from: {DATA_PATH}\")\n\n# Load JSONL data\ndata = []\nwith open(DATA_PATH, 'r', encoding='utf-8') as f:\n    for line in f:\n        data.append(json.loads(line))\n\nprint(f\"Total examples: {len(data)}\")\n\n# Extract question-answer pairs\nexamples = []\nfor item in data:\n    examples.append({\n        \"question\": item[\"question\"],\n        \"answer\": item[\"answer\"]\n    })\n\n# Split into train/validation (90/10)\ntrain_data, val_data = train_test_split(examples, test_size=0.1, random_state=42)\n\nprint(f\"Training examples: {len(train_data)}\")\nprint(f\"Validation examples: {len(val_data)}\")\n\nprint(f\"\\n2. Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nprint(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n\nprint(f\"\\n3. Preparing datasets...\")\n\ndef format_instruction(example):\n    \"\"\"Format question-answer pair for training\"\"\"\n    text = f\"### Question:\\n{example['question']}\\n\\n### Answer:\\n{example['answer']}\"\n    return text\n\ndef tokenize_function(example):\n    \"\"\"Tokenize examples with padding and truncation\"\"\"\n    text = format_instruction(example)\n    \n    tokenized = tokenizer(\n        text,\n        truncation=True,\n        max_length=512,\n        padding=\"max_length\",\n        return_tensors=None\n    )\n    \n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized\n\n# Convert to HuggingFace Dataset format\ntrain_dataset = Dataset.from_list(train_data)\nval_dataset = Dataset.from_list(val_data)\n\n# Tokenize\nprint(\"Tokenizing training data...\")\ntrain_dataset = train_dataset.map(\n    tokenize_function,\n    remove_columns=train_dataset.column_names\n)\n\nprint(\"Tokenizing validation data...\")\nval_dataset = val_dataset.map(\n    tokenize_function,\n    remove_columns=val_dataset.column_names\n)\n\nprint(f\"Training dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(val_dataset)}\")\n\nprint(f\"\\n4. Loading base model...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\n\nmodel.gradient_checkpointing_enable()\n\nprint(f\"Model loaded: {model.__class__.__name__}\")\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\nprint(f\"\\n5. Configuring LoRA...\")\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\n\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\n\nprint(f\"LoRA configured successfully\")\nprint(f\"Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.4f}%)\")\nprint(f\"Total parameters: {total_params:,}\")\n\nprint(f\"\\n6. Setting up training arguments...\")\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=16,\n    learning_rate=2e-4,\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=50,\n    logging_steps=10,\n    save_steps=50,\n    eval_steps=50,\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    fp16=True,\n    report_to=\"none\",\n    save_total_limit=2,\n    remove_unused_columns=False,\n    gradient_checkpointing=True\n)\n\nprint(\"Training configuration:\")\nprint(f\"  Epochs: {training_args.num_train_epochs}\")\nprint(f\"  Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  Learning rate: {training_args.learning_rate}\")\n\nprint(f\"\\n7. Initializing trainer...\")\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator\n)\n\nprint(\"Trainer initialized successfully!\")\n\nprint(f\"\\n8. Starting training...\")\nprint(\"=\"*60)\n\ntrainer.train()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Training complete!\")\nprint(\"=\"*60)\n\nprint(f\"\\n9. Saving final model...\")\nmodel.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\ntokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n\nprint(f\"Model saved to: {OUTPUT_DIR}/final_model\")\n\nprint(f\"\\n10. Final evaluation metrics:\")\neval_results = trainer.evaluate()\nfor key, value in eval_results.items():\n    print(f\"  {key}: {value:.4f}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Fine-tuning pipeline complete!\")\nprint(\"=\"*60)\n\n```\n\n<br>\n\n### Question Testing\n\nOnce the fine-tuned models for the two datasets where created, the next was to test each model against the questions provided by Rohan.\n\n#### Superstitions Dataset:\n\n```bash\n# Superstitions Model Testing Bash Script\n\n#!/bin/bash\n#SBATCH --job-name=test_super_model\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=32G\n#SBATCH --gres=gpu:1\n#SBATCH --time=1:00:00\n#SBATCH --output=logs/test_super_%j.out\n#SBATCH --error=logs/test_super_%j.err\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=starwill16@gmail.com\n\nmodule purge\nmodule load Python/3.12.3-GCCcore-13.3.0\nmodule load CUDA/12.3.0\n\nsource /home/gcsc563_01/CSC463/conlingo/llama_env/bin/activate\n\nexport HF_HOME=/home/gcsc563_01/.cache/huggingface\nexport HUGGINGFACE_HUB_CACHE=/home/gcsc563_01/.cache/huggingface\n\ncd /home/gcsc563_01/conlingo\n\npython scripts/training/run_superstitions_model.py\n\ndeactivate\n```\n\n```python\n# Superstitions Model Testing Python Script\n\n#!/usr/bin/env python3\n\"\"\"\nTest the fine-tuned TED Talks model\nLoads the model and runs sample inference\n\"\"\"\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\nprint(\"=\"*60)\nprint(\"Superstitions Model Inference Test\")\nprint(\"=\"*60)\n\n# Paths\nBASE_MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nFINETUNED_MODEL_PATH = \"/home/gcsc563_01/conlingo/models/finetuned/superstition/final_model\"\n\nprint(f\"\\n1. Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(FINETUNED_MODEL_PATH)\nprint(f\"✅ Tokenizer loaded\")\n\nprint(f\"\\n2. Loading base model...\")\nbase_model = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\nprint(f\"✅ Base model loaded\")\n\nprint(f\"\\n3. Loading fine-tuned LoRA adapters...\")\nmodel = PeftModel.from_pretrained(base_model, FINETUNED_MODEL_PATH)\nprint(f\"✅ Fine-tuned model loaded\")\n\nprint(f\"\\n4. Running test inference...\")\nprint(\"-\"*60)\n\n# Test questions\ntest_questions = [\n    \"How can Diwali and Christmas both symbolize the victory of light over darkness in their respective traditions?\",\n    \"What themes of renewal in Diwali resonate with the idea of rebirth in Christian theology?\",\n    \"How might Indian Christians use Diwali to express biblical messages of hope?\",\n    \"How can Christian schools acknowledge Diwali without compromising faith boundaries?\",\n    \"What sensitivities should pastors consider when mentioning Hindu deities in Christmas homilies?\",\n    \"How can Christian youth groups create shared Diwali–Christmas community service projects?\",\n    \"How can churches ensure caste-neutral seating and participation during worship?\",\n    \"What examples of caste inclusion can be found in the life of Jesus?\",\n    \"What are sensitive ways to discuss “the least of these” without reinforcing caste bias?\",\n    \"Why might some Christians still use caste surnames, and how should this be discussed?\",\n    \"What is a sensitive way to discuss Jesus’ association with marginalized groups?\",\n    \"What is an inclusive way to discuss Krishna and Christ comparisons in academia?\",\n    \"How can Hindu concepts of karma be reconciled with Christian grace in conversation?\",\n    \"How can Christian missions emphasize shared moral principles rather than conversion?\",\n    \"How does food during Christmas vary regionally across India?\",\n    \"How does regional folklore shape Christian storytelling traditions?\",\n    \"How do Indian Christian elders respond to youth questioning of denominational traditions?\",\n    \"How can faith-based education evolve to reach younger, tech-savvy Christians?\",\n    \"How can liturgy adapt to youth participation without losing sacredness?\",\n    \"How do young Christians view service and mission in a modern Indian context?\"\n]\n\nfor i, question in enumerate(test_questions, 1):\n    print(f\"\\n[Test {i}]\")\n    print(f\"Question: {question}\")\n    \n    # Format prompt\n    prompt = f\"### Question:\\n{question}\\n\\n### Answer:\\n\"\n    \n    # Tokenize\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    # Generate\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            temperature=0.7,\n            do_sample=True,\n            top_p=0.9,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    # Decode\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Extract only the answer part (after \"### Answer:\")\n    answer = response.split(\"### Answer:\")[-1].strip()\n    \n    print(f\"Answer: {answer}\")\n    print(\"-\"*60)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Inference test complete!\")\nprint(\"=\"*60)\n\n```\n\n<br>\n\n#### Constitutions Dataset:\n\n```bash\n# Constitutions Model Testing Bash Script\n\n#!/bin/bash\n#SBATCH --job-name=test_const_model\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=32G\n#SBATCH --gres=gpu:1\n#SBATCH --time=1:00:00\n#SBATCH --output=logs/test_const_%j.out\n#SBATCH --error=logs/test_const_%j.err\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=starwill16@gmail.com\n\nmodule purge\nmodule load Python/3.12.3-GCCcore-13.3.0\nmodule load CUDA/12.3.0\n\nsource /home/gcsc563_01/CSC463/conlingo/llama_env/bin/activate\n\nexport HF_HOME=/home/gcsc563_01/.cache/huggingface\nexport HUGGINGFACE_HUB_CACHE=/home/gcsc563_01/.cache/huggingface\n\ncd /home/gcsc563_01/conlingo\n\npython scripts/training/run_constitutions_model.py\n\ndeactivate\n```\n\n```python\n# Constitutions Model Testing Python Script\n\n#!/usr/bin/env python3\n\"\"\"\nTest the fine-tuned Constitutions model\nLoads the model and runs sample inference\n\"\"\"\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\nprint(\"=\"*60)\nprint(\"Constitutions Model Inference Test\")\nprint(\"=\"*60)\n\n# Paths\nBASE_MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nFINETUNED_MODEL_PATH = \"/home/gcsc563_01/conlingo/models/finetuned/constitution/final_model\"\n\nprint(f\"\\n1. Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(FINETUNED_MODEL_PATH)\nprint(f\"✅ Tokenizer loaded\")\n\nprint(f\"\\n2. Loading base model...\")\nbase_model = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\nprint(f\"✅ Base model loaded\")\n\nprint(f\"\\n3. Loading fine-tuned LoRA adapters...\")\nmodel = PeftModel.from_pretrained(base_model, FINETUNED_MODEL_PATH)\nprint(f\"✅ Fine-tuned model loaded\")\n\nprint(f\"\\n4. Running test inference...\")\nprint(\"-\"*60)\n\n# Test questions\ntest_questions = [\n    \"How can Diwali and Christmas both symbolize the victory of light over darkness in their respective traditions?\",\n    \"What themes of renewal in Diwali resonate with the idea of rebirth in Christian theology?\",\n    \"How might Indian Christians use Diwali to express biblical messages of hope?\",\n    \"How can Christian schools acknowledge Diwali without compromising faith boundaries?\",\n    \"What sensitivities should pastors consider when mentioning Hindu deities in Christmas homilies?\",\n    \"How can Christian youth groups create shared Diwali–Christmas community service projects?\",\n    \"How can churches ensure caste-neutral seating and participation during worship?\",\n    \"What examples of caste inclusion can be found in the life of Jesus?\",\n    \"What are sensitive ways to discuss “the least of these” without reinforcing caste bias?\",\n    \"Why might some Christians still use caste surnames, and how should this be discussed?\",\n    \"What is a sensitive way to discuss Jesus’ association with marginalized groups?\",\n    \"What is an inclusive way to discuss Krishna and Christ comparisons in academia?\",\n    \"How can Hindu concepts of karma be reconciled with Christian grace in conversation?\",\n    \"How can Christian missions emphasize shared moral principles rather than conversion?\",\n    \"How does food during Christmas vary regionally across India?\",\n    \"How does regional folklore shape Christian storytelling traditions?\",\n    \"How do Indian Christian elders respond to youth questioning of denominational traditions?\",\n    \"How can faith-based education evolve to reach younger, tech-savvy Christians?\",\n    \"How can liturgy adapt to youth participation without losing sacredness?\",\n    \"How do young Christians view service and mission in a modern Indian context?\"\n]\n\nfor i, question in enumerate(test_questions, 1):\n    print(f\"\\n[Test {i}]\")\n    print(f\"Question: {question}\")\n    \n    # Format prompt\n    prompt = f\"### Question:\\n{question}\\n\\n### Answer:\\n\"\n    \n    # Tokenize\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    # Generate\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            temperature=0.7,\n            do_sample=True,\n            top_p=0.9,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    # Decode\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Extract only the answer part (after \"### Answer:\")\n    answer = response.split(\"### Answer:\")[-1].strip()\n    \n    print(f\"Answer: {answer}\")\n    print(\"-\"*60)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Inference test complete!\")\nprint(\"=\"*60)\n\n```\n\n<br>\n\n### High Five Testing\n\nAfter testing the other 2 models, Wills final requirement was to combine all datasets into one, creating what I called the \"High Five\" dataset, fine-tuning a model for it, and testing it against Rohan's questions one more time.\n\n#### Fine-Tuning Model:\n\n```bash\n# High Five Fine-Tuning Model Bash Script\n\n#!/bin/bash\n#SBATCH --job-name=high_five_finetune\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=64G\n#SBATCH --gres=gpu:1\n#SBATCH --time=6:00:00\n#SBATCH --output=logs/high_finetune_%j.out\n#SBATCH --error=logs/high_finetune_%j.err\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=starwill16@gmail.com\n\nmodule purge\nmodule load Python/3.12.3-GCCcore-13.3.0\nmodule load CUDA/12.3.0\n\nsource /home/gcsc563_01/CSC463/conlingo/llama_env/bin/activate\n\nexport HF_HOME=/home/gcsc563_01/.cache/huggingface\nexport HUGGINGFACE_HUB_CACHE=/home/gcsc563_01/.cache/huggingface\n\ncd /home/gcsc563_01/conlingo\n\npython scripts/training/finetune_high_five.py\ndeactivate\n```\n\n```python\n# High Five Fine-Tuning Model Python Script\n\n#!/usr/bin/env python3\n\"\"\"\nLoRA Fine-Tuning for all 5 Datasets\nFine-tunes LLaMA-3 8B on all 5 Datasets' Q&A pairs\n\"\"\"\n\nimport torch\nimport json\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling\n)\nfrom peft import LoraConfig, get_peft_model\nfrom datasets import Dataset\nimport os\nfrom sklearn.model_selection import train_test_split\n\nprint(\"=\"*60)\nprint(\"High Five LoRA Fine-Tuning Pipeline\")\nprint(\"=\"*60)\n\n# Paths\nDATA_PATH = \"/home/gcsc563_01/conlingo/data/raw/highfive_qa.jsonl\"\nOUTPUT_DIR = \"/home/gcsc563_01/conlingo/models/finetuned/high_five\"\nMODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\n# Create output directory\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"\\n1. Loading data from: {DATA_PATH}\")\n\n# Load JSONL data\ndata = []\nwith open(DATA_PATH, 'r', encoding='utf-8') as f:\n    for line in f:\n        data.append(json.loads(line))\n\nprint(f\"Total examples: {len(data)}\")\n\n# Extract question-answer pairs\nexamples = []\nfor item in data:\n    examples.append({\n        \"question\": item[\"question\"],\n        \"answer\": item[\"answer\"]\n    })\n\n# Split into train/validation (90/10)\ntrain_data, val_data = train_test_split(examples, test_size=0.1, random_state=42)\n\nprint(f\"Training examples: {len(train_data)}\")\nprint(f\"Validation examples: {len(val_data)}\")\n\nprint(f\"\\n2. Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nprint(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n\nprint(f\"\\n3. Preparing datasets...\")\n\ndef format_instruction(example):\n    \"\"\"Format question-answer pair for training\"\"\"\n    text = f\"### Question:\\n{example['question']}\\n\\n### Answer:\\n{example['answer']}\"\n    return text\n\ndef tokenize_function(example):\n    \"\"\"Tokenize examples with padding and truncation\"\"\"\n    text = format_instruction(example)\n    \n    tokenized = tokenizer(\n        text,\n        truncation=True,\n        max_length=512,\n        padding=\"max_length\",\n        return_tensors=None\n    )\n    \n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized\n\n# Convert to HuggingFace Dataset format\ntrain_dataset = Dataset.from_list(train_data)\nval_dataset = Dataset.from_list(val_data)\n\n# Tokenize\nprint(\"Tokenizing training data...\")\ntrain_dataset = train_dataset.map(\n    tokenize_function,\n    remove_columns=train_dataset.column_names\n)\n\nprint(\"Tokenizing validation data...\")\nval_dataset = val_dataset.map(\n    tokenize_function,\n    remove_columns=val_dataset.column_names\n)\n\nprint(f\"Training dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(val_dataset)}\")\n\nprint(f\"\\n4. Loading base model...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\n\nmodel.gradient_checkpointing_enable()\n\nprint(f\"Model loaded: {model.__class__.__name__}\")\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\nprint(f\"\\n5. Configuring LoRA...\")\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\n\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\n\nprint(f\"LoRA configured successfully\")\nprint(f\"Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.4f}%)\")\nprint(f\"Total parameters: {total_params:,}\")\n\nprint(f\"\\n6. Setting up training arguments...\")\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=16,\n    learning_rate=2e-4,\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=50,\n    logging_steps=10,\n    save_steps=50,\n    eval_steps=50,\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    fp16=True,\n    report_to=\"none\",\n    save_total_limit=2,\n    remove_unused_columns=False,\n    gradient_checkpointing=True\n)\n\nprint(\"Training configuration:\")\nprint(f\"  Epochs: {training_args.num_train_epochs}\")\nprint(f\"  Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  Learning rate: {training_args.learning_rate}\")\n\nprint(f\"\\n7. Initializing trainer...\")\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator\n)\n\nprint(\"Trainer initialized successfully!\")\n\nprint(f\"\\n8. Starting training...\")\nprint(\"=\"*60)\n\ntrainer.train()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Training complete!\")\nprint(\"=\"*60)\n\nprint(f\"\\n9. Saving final model...\")\nmodel.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\ntokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n\nprint(f\"Model saved to: {OUTPUT_DIR}/final_model\")\n\nprint(f\"\\n10. Final evaluation metrics:\")\neval_results = trainer.evaluate()\nfor key, value in eval_results.items():\n    print(f\"  {key}: {value:.4f}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Fine-tuning pipeline complete!\")\nprint(\"=\"*60)\n\n```\n\n#### Model Testing:\n\n```bash\n# High Five Model Testing Bash Script\n\n#!/bin/bash\n#SBATCH --job-name=test_high_five_model\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=32G\n#SBATCH --gres=gpu:1\n#SBATCH --time=1:00:00\n#SBATCH --output=logs/test_high_five_%j.out\n#SBATCH --error=logs/test_high_five_%j.err\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=starwill16@gmail.com\n\nmodule purge\nmodule load Python/3.12.3-GCCcore-13.3.0\nmodule load CUDA/12.3.0\n\nsource /home/gcsc563_01/CSC463/conlingo/llama_env/bin/activate\n\nexport HF_HOME=/home/gcsc563_01/.cache/huggingface\nexport HUGGINGFACE_HUB_CACHE=/home/gcsc563_01/.cache/huggingface\n\ncd /home/gcsc563_01/conlingo\n\npython scripts/training/run_high_five_model.py\n\ndeactivate\n```\n\n```python\n# High Five Model Testing Python Script\n\n#!/usr/bin/env python3\n\"\"\"\nTest the fine-tuned High Five model\nLoads the model and runs sample inference\n\"\"\"\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\nprint(\"=\"*60)\nprint(\"High Five Model Inference Test\")\nprint(\"=\"*60)\n\n# Paths\nBASE_MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nFINETUNED_MODEL_PATH = \"/home/gcsc563_01/conlingo/models/finetuned/high_five/final_model\"\n\nprint(f\"\\n1. Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(FINETUNED_MODEL_PATH)\nprint(f\"✅ Tokenizer loaded\")\n\nprint(f\"\\n2. Loading base model...\")\nbase_model = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\nprint(f\"✅ Base model loaded\")\n\nprint(f\"\\n3. Loading fine-tuned LoRA adapters...\")\nmodel = PeftModel.from_pretrained(base_model, FINETUNED_MODEL_PATH)\nprint(f\"✅ Fine-tuned model loaded\")\n\nprint(f\"\\n4. Running test inference...\")\nprint(\"-\"*60)\n\n# Test questions\ntest_questions = [\n    \"How can Diwali and Christmas both symbolize the victory of light over darkness in their respective traditions?\",\n    \"What themes of renewal in Diwali resonate with the idea of rebirth in Christian theology?\",\n    \"How might Indian Christians use Diwali to express biblical messages of hope?\",\n    \"How can Christian schools acknowledge Diwali without compromising faith boundaries?\",\n    \"What sensitivities should pastors consider when mentioning Hindu deities in Christmas homilies?\",\n    \"How can Christian youth groups create shared Diwali–Christmas community service projects?\",\n    \"How can churches ensure caste-neutral seating and participation during worship?\",\n    \"What examples of caste inclusion can be found in the life of Jesus?\",\n    \"What are sensitive ways to discuss “the least of these” without reinforcing caste bias?\",\n    \"Why might some Christians still use caste surnames, and how should this be discussed?\",\n    \"What is a sensitive way to discuss Jesus’ association with marginalized groups?\",\n    \"What is an inclusive way to discuss Krishna and Christ comparisons in academia?\",\n    \"How can Hindu concepts of karma be reconciled with Christian grace in conversation?\",\n    \"How can Christian missions emphasize shared moral principles rather than conversion?\",\n    \"How does food during Christmas vary regionally across India?\",\n    \"How does regional folklore shape Christian storytelling traditions?\",\n    \"How do Indian Christian elders respond to youth questioning of denominational traditions?\",\n    \"How can faith-based education evolve to reach younger, tech-savvy Christians?\",\n    \"How can liturgy adapt to youth participation without losing sacredness?\",\n    \"How do young Christians view service and mission in a modern Indian context?\"\n]\n\n\nfor i, question in enumerate(test_questions, 1):\n    print(f\"\\n[Test {i}]\")\n    print(f\"Question: {question}\")\n    \n    # Format prompt\n    prompt = f\"### Question:\\n{question}\\n\\n### Answer:\\n\"\n    \n    # Tokenize\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    # Generate\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            temperature=0.7,\n            do_sample=True,\n            top_p=0.9,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    # Decode\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Extract only the answer part (after \"### Answer:\")\n    answer = response.split(\"### Answer:\")[-1].strip()\n    \n    print(f\"Answer: {answer}\")\n    print(\"-\"*60)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Inference test complete!\")\nprint(\"=\"*60)\n\n```\n\nNow, after me and Moses' analyses and fine-tuning process, evrey model is set for grading and comparison with the original ConLingo model.\n\n<br>\n\n<br>\n\n<br>\n\n<br>\n\n\n## Suwilanji Mwanza Deliverables\n\n\n### Overview of the Week 4 Deliverable\n\nThis week, my deliverable was to provide visualizations of the different models created for each dataset and then visualize their performance, which was evaluated by Rohan. Additionally, examine the datasets that contributed to a mode's negative performance, if any, alongside Rohan.\n\n### Week 4 Deliverables:\n\n-   Create comprehensive visualizations about the performance of each of the 7 models, as well as the 2 combined models.\n\n-   Examine the datasets that contributed to the model's poor performance and propose potential reasons for this.\n\n-   Create the full presentation that will be used as the Week 4 presentation.\n\n### What Was Accomplished:\n\n### 1. Model Visualizations\n\nIn accordance with the CSI scoring that Rohan provided earlier, he used it to evaluate the model's performance. This model scores an AI's response based on accuracy, tone, context, and empathy. This all contributes to the final CSI score.\n\nThe two models' results, which were ready for me to visualize this week, were the Constitution Model and the Superstitions Model. I used Excel for this process.\n\nRohan graded the model's response on a scale of 20 questions that he also created earlier. Each question received a number for it to be easily plotted:\n\n1.  How can Diwali and Christmas both symbolize the victory of light over darkness in their respective traditions? \n2.  What themes of renewal in Diwali resonate with the idea of rebirth in Christian theology? \n3.  How might Indian Christians use Diwali to express biblical messages of hope? \n4.  How can Christian schools acknowledge Diwali without compromising faith boundaries? \n5.  What sensitivities should pastors consider when mentioning Hindu deities in Christmas homilies?\n6.  How can Christian youth groups create shared Diwali–Christmas community service projects? \n7.  How can churches ensure caste-neutral seating and participation during worship? \n8.  What examples of caste inclusion can be found in the life of Jesus? \n9.  What are sensitive ways to discuss \"the least of these\" without reinforcing caste bias? \n10. Why might some Christians still use caste surnames, and how should this be discussed? \n11. What is a sensitive way to discuss Jesus' association with marginalized groups? \n12. What is an inclusive way to discuss Krishna and Christ comparisons in academia? \n13. How can Hindu concepts of karma be reconciled with Christian grace in conversation?\n14. How can Christian missions emphasize shared moral principles rather than conversion?\n15. How does food during Christmas vary regionally across India?  \n16. How does regional folklore shape Christian storytelling traditions? \n17. How do Indian Christian elders respond to youth questioning of denominational traditions? \n18. How can faith-based education evolve to reach younger, tech-savvy Christians? \n19. How can liturgy adapt to youth participation without losing sacredness? \n20. How do young Christians view service and mission in a modern Indian context? \n\n**Accuracy**\n\n![Accuracy results](img/week_4_1.png)\n\nIn this graph, the Constitution's model performs better on average, with a perfect score of 5, and more than half of its answers achieve a perfect score.\n\n**Tone**\n\n![Tone results](img/week_4_2.png)\n\nIn this graph, the Constitutions model performs better than the Superstitions data. The Superstitions model has an above-average response.\n\n**Context**\n\n![Context results](img/week_4_3.png)\n\nThis graph shows that the Constitution model has a better understanding of the question's context and thus provides more accurate answers.\n\n**Empathy**\n\n![Empathy results](img/week_4_4.png)\n\nThe empathy graph also shows the Constitutions model performing significantly better than the other model, with near-perfect scores for each question.\n\n**CSI Score**\n\n![CSI Scoring results](img/week_4_5.png)\n\nIt is not surprising that the overall model shows the Constitutions performed well. This graph is interpreted to mean that each score represents an average of accuracy, tone, context, and empathy for each question.\n\n### 2. Negative Performing Models\n\nFirst and foremost, the fact that Rohan was the only one to grade both the model's responses meant a heavy bias was present. Although he is from India, his notions of what he sees as accurate are unconsciously influencing his grading.\n\nAdditionally, the sample size for this model was too small. Only Rohan. Some areas to improve our perception of a negatively performing model include increasing our sample size to gain a better understanding of which model is truly underperforming.\n\nA major flaw in this grading is that since the models were trained with supervised fine-tuning, they are only able to provide answers based on the fact that the training data, such as that of supervised learning. However, the questions that we are asking the Constitution model do not utilize the knowledge base or dataset of the constitution data, yet the trained model performs better.\n\nWhat most likely happened is that, although the evaluation questions were unrelated to the SFT knowledge (religious questions versus the Constitution and superstition data), the Constitution-trained model was consistently judged stronger by Rohan. More plainly put, our questions unfortunately did not evaluate knowledge recall but rather cross-domain behavioral transfer. This concept falls under transfer learning, a machine learning technique in which a model trained on one task is repurposed as the foundation for a second task. (GeeksforGeeks, 2025)\n\nIn this case, the model was trained for the task of analyzing superstition data or constitutional data; however, during evaluation, the questions soliciting a religious answer posed a new task. What happened here is that the training data sets' style and reasoning picked up during the training session transferred (cross-domain). SFT not only teaches content but also behavioral patterns.\n\nIt is possible that Rohan agreed more with the Constitution's style that transferred to the responses being evaluated. I'd imagine this data to be more formal, neutral, and structured. In addition, it may have known how to phrase sensitive topics better, which transferred a style of the model being careful with wording, less biased, and more diplomatic, for example. The superstition's data may have come across as less credible, as the data contains myths, folklore, and mixes belief and fiction. And thus the style transfer is less academic, more storytelling, and less objective.\n\nIn conclusion, what would have made this process more effective would have been to have a base model to test against these evaluation questions and truly determine how both models deviate from the base case.\n\n\n## Moses Madale Deliverables\n\n **AI assistance: Claude Sonnet 4.5 was used to help me troubleshoot issues with setting up an identical virtual environment on Will’s Titan account to ensure that he can also run the fine-tuning from his account smoothly, there were issues with python package version conflicts but with assistance from Claude Sonnet, these issues were resolved more efficiently (accessed Nov, 2025).**\n\n### Model Training and Evaluation\n\n#### Overview\n\nWeek 4 transformed the datasets collected in Week 3 into trained models ready for evaluation. The primary focus was training three distinct fine-tuned models—one each for YouTube Transcripts, Wikipedia, and TED Talks datasets—testing them on 20 culturally nuanced questions, and preparing results for comprehensive analysis. This week demonstrated that different cultural data sources produce models with varying capabilities, setting the stage for Week 5's comparative evaluation against baseline and state-of-the-art models.\n\n#### Training Infrastructure and Workflow\n\n##### Fine-Tuning Pipeline Architecture\n\nBuilding on Week 2's pipeline, Week 4 established a standardized workflow for training multiple models systematically:\n\n![Fine-tuning pipeline for multiple datasets](./Appendix/MosesMadale/img/4_1.png)\n\n\n\n\n**Pipeline Stages**:\n\n1. **Dataset Loading**: Read JSONL file with Q&A pairs\n2. **Train/Validation Split**: 90/10 split with random seed for reproducibility\n3. **Tokenization**: Convert text to model-compatible format with padding\n4. **Base Model Loading**: Load LLaMA-3 8B Instruct with half-precision\n5. **LoRA Configuration**: Add trainable adapters to attention layers\n6. **Training**: Run supervised fine-tuning with gradient accumulation\n7. **Evaluation**: Assess performance on validation set\n8. **Model Saving**: Store LoRA adapters for inference\n\n#### Standardized Training Configuration\n\nAll three models used identical hyperparameters to ensure fair comparison:\n\n| Hyperparameter | Value | Purpose |\n|----------------|-------|---------|\n| **Epochs** | 3 | Sufficient for convergence without overfitting |\n| **Batch Size** | 2 | Maximum fitting in 24 GB VRAM |\n| **Gradient Accumulation** | 16 | Effective batch size of 32 |\n| **Learning Rate** | 2e-4 | Standard for LoRA fine-tuning |\n| **LR Scheduler** | Cosine | Gradual learning rate decay |\n| **Warmup Steps** | 50 | Prevents early training instability |\n| **Max Sequence Length** | 512 tokens | Accommodates most Q&A pairs |\n| **LoRA Rank (r)** | 16 | Balance between capacity and efficiency |\n| **LoRA Alpha** | 32 | Scaling factor for adapter outputs |\n\n**LoRA Target Modules**:\n- `q_proj` (Query projection)\n- `v_proj` (Value projection)\n- `k_proj` (Key projection)\n- `o_proj` (Output projection)\n\nThese attention mechanism components were selected because they capture the most information during text generation while keeping trainable parameters to just 0.17% of the total model.\n\n#### Training Script Structure\n\nEach fine-tuning script followed a consistent 10-step process. Here is the complete Wikipedia training script as an example:\n\n```python\n#| eval: false\n\n#!/usr/bin/env python3\n\"\"\"\nLoRA Fine-Tuning for Wikipedia Data\nFine-tunes LLaMA-3 8B on Indian Wikipedia Q&A pairs\n\"\"\"\n\nimport torch\nimport json\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling\n)\nfrom peft import LoraConfig, get_peft_model\nfrom datasets import Dataset\nimport os\nfrom sklearn.model_selection import train_test_split\n\nprint(\"=\"*60)\nprint(\"Wikipedia LoRA Fine-Tuning Pipeline\")\nprint(\"=\"*60)\n\n# Paths\nDATA_PATH = \"/home/mmadale/CSC463/conlingo/huggingface_data/indian_wikipedia/data/wikipedia_qa.jsonl\"\nOUTPUT_DIR = \"/home/mmadale/CSC463/conlingo/models/wikipedia\"\nMODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\n# Create output directory\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"\\n1. Loading data from: {DATA_PATH}\")\n\n# Load JSONL data\ndata = []\nwith open(DATA_PATH, 'r', encoding='utf-8') as f:\n    for line in f:\n        data.append(json.loads(line))\n\nprint(f\"Total examples: {len(data)}\")\n\n# Extract question-answer pairs\nexamples = []\nfor item in data:\n    examples.append({\n        \"question\": item[\"question\"],\n        \"answer\": item[\"answer\"]\n    })\n\n# Split into train/validation (90/10)\ntrain_data, val_data = train_test_split(examples, test_size=0.1, random_state=42)\n\nprint(f\"Training examples: {len(train_data)}\")\nprint(f\"Validation examples: {len(val_data)}\")\n\nprint(f\"\\n2. Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nprint(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n\nprint(f\"\\n3. Preparing datasets...\")\n\ndef format_instruction(example):\n    \"\"\"Format question-answer pair for training\"\"\"\n    text = f\"### Question:\\n{example['question']}\\n\\n### Answer:\\n{example['answer']}\"\n    return text\n\ndef tokenize_function(example):\n    \"\"\"Tokenize examples with padding and truncation\"\"\"\n    text = format_instruction(example)\n    \n    tokenized = tokenizer(\n        text,\n        truncation=True,\n        max_length=512,\n        padding=\"max_length\",\n        return_tensors=None\n    )\n    \n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized\n\n# Convert to HuggingFace Dataset format\ntrain_dataset = Dataset.from_list(train_data)\nval_dataset = Dataset.from_list(val_data)\n\n# Tokenize\nprint(\"Tokenizing training data...\")\ntrain_dataset = train_dataset.map(\n    tokenize_function,\n    remove_columns=train_dataset.column_names\n)\n\nprint(\"Tokenizing validation data...\")\nval_dataset = val_dataset.map(\n    tokenize_function,\n    remove_columns=val_dataset.column_names\n)\n\nprint(f\"Training dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(val_dataset)}\")\n\nprint(f\"\\n4. Loading base model...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\n\nmodel.gradient_checkpointing_enable()\n\nprint(f\"Model loaded: {model.__class__.__name__}\")\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\nprint(f\"\\n5. Configuring LoRA...\")\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\n\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\n\nprint(f\"LoRA configured successfully\")\nprint(f\"Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.4f}%)\")\nprint(f\"Total parameters: {total_params:,}\")\n\nprint(f\"\\n6. Setting up training arguments...\")\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=16,\n    learning_rate=2e-4,\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=50,\n    logging_steps=10,\n    save_steps=50,\n    eval_steps=50,\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    fp16=True,\n    report_to=\"none\",\n    save_total_limit=2,\n    remove_unused_columns=False,\n    gradient_checkpointing=True\n)\n\nprint(\"Training configuration:\")\nprint(f\"  Epochs: {training_args.num_train_epochs}\")\nprint(f\"  Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  Learning rate: {training_args.learning_rate}\")\n\nprint(f\"\\n7. Initializing trainer...\")\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator\n)\n\nprint(\"Trainer initialized successfully!\")\n\nprint(f\"\\n8. Starting training...\")\nprint(\"=\"*60)\n\ntrainer.train()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Training complete!\")\nprint(\"=\"*60)\n\nprint(f\"\\n9. Saving final model...\")\nmodel.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\ntokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n\nprint(f\"Model saved to: {OUTPUT_DIR}/final_model\")\n\nprint(f\"\\n10. Final evaluation metrics:\")\neval_results = trainer.evaluate()\nfor key, value in eval_results.items():\n    print(f\"  {key}: {value:.4f}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Fine-tuning pipeline complete!\")\nprint(\"=\"*60)\n```\n\nThe same script structure was adapted for TED Talks and YouTube Transcripts by simply changing the `DATA_PATH` and `OUTPUT_DIR` variables. This standardization enabled rapid iteration and consistent results across different datasets.\n\n### Model Training Results\n\n#### Wikipedia Model Training\n\n**Dataset Statistics**:\n- Total examples: 500\n- Training set: 450 (90%)\n- Validation set: 50 (10%)\n- Training duration: 8 minutes 29 seconds\n\n**Training Progress**:\n\n```\n============================================================\nWikipedia LoRA Fine-Tuning Pipeline\n============================================================\n\n1. Loading data from: .../indian_wikipedia/data/wikipedia_qa.jsonl\nTotal examples: 500\nTraining examples: 450\nValidation examples: 50\n\n2. Loading tokenizer...\nTokenizer loaded: PreTrainedTokenizerFast\n\n3. Preparing datasets...\nTraining dataset size: 450\nValidation dataset size: 50\n\n4. Loading base model...\nModel loaded: LlamaForCausalLM\nModel parameters: 8,030,261,248\n\n5. Configuring LoRA...\nLoRA configured successfully\nTrainable parameters: 13,631,488 (0.1695%)\nTotal parameters: 8,043,892,736\n\n6. Setting up training arguments...\nTraining configuration:\n  Epochs: 3\n  Batch size: 2\n  Gradient accumulation: 16\n  Effective batch size: 32\n  Learning rate: 0.0002\n\n8. Starting training...\n============================================================\n{'loss': 2.433, 'grad_norm': 1.730, 'learning_rate': 3.6e-05, 'epoch': 0.71}\n{'loss': 1.945, 'grad_norm': 2.509, 'learning_rate': 7.6e-05, 'epoch': 1.36}\n{'loss': 1.468, 'grad_norm': 4.694, 'learning_rate': 0.000116, 'epoch': 2.0}\n{'loss': 1.281, 'grad_norm': 0.774, 'learning_rate': 0.000156, 'epoch': 2.71}\n\n============================================================\nTraining complete!\n============================================================\n\n10. Final evaluation metrics:\n  eval_loss: 1.2456\n  eval_runtime: 5.8086\n  epoch: 3.0000\n```\n\n**Performance Analysis**:\n\nThe Wikipedia model achieved the **lowest final loss (1.2456)** among all three models, indicating:\n- Excellent convergence on encyclopedic content\n- Strong pattern recognition for factual Q&A\n- Effective learning from well-structured Wikipedia articles\n\nThe loss decreased from 2.433 → 1.281 (47% reduction), demonstrating substantial learning without overfitting (validation loss 1.2456 close to final training loss 1.281).\n\n####  YouTube Transcripts Model Training\n\n**Dataset Statistics**:\n- Total examples: 512\n- Training set: 460 (90%)\n- Validation set: 52 (10%)\n- Training duration: 8 minutes 46 seconds\n\n**Training Progress**:\n\n```\n============================================================\nYouTube Transcripts LoRA Fine-Tuning Pipeline\n============================================================\n\n1. Loading data from: .../youtube_data/data/final_youtube_transcript_data.jsonl\nTotal examples: 512\nTraining examples: 460\nValidation examples: 52\n\n5. Configuring LoRA...\nTrainable parameters: 13,631,488 (0.1695%)\nTotal parameters: 8,043,892,736\n\n8. Starting training...\n============================================================\n{'loss': 3.935, 'grad_norm': 1.849, 'learning_rate': 3.6e-05, 'epoch': 0.7}\n{'loss': 3.363, 'grad_norm': 2.377, 'learning_rate': 7.6e-05, 'epoch': 1.35}\n{'loss': 2.618, 'grad_norm': 2.411, 'learning_rate': 0.000116, 'epoch': 2.0}\n{'loss': 2.324, 'grad_norm': 1.130, 'learning_rate': 0.000156, 'epoch': 2.7}\n\n============================================================\nTraining complete!\n============================================================\n\n10. Final evaluation metrics:\n  eval_loss: 2.2825\n  eval_runtime: 6.0515\n  epoch: 3.0000\n```\n\n**Performance Analysis**:\n\nThe YouTube model exhibited the **highest loss (2.2825)** among the three models, suggesting:\n- Greater complexity in conversational, informal content\n- Diverse speaking styles and code-mixing (Hinglish) challenging to model\n- Richer linguistic variation compared to encyclopedic content\n\nDespite higher loss, the model achieved 41% loss reduction (3.935 → 2.324), indicating successful learning of conversational patterns and cultural idioms prevalent in YouTube content.\n\n#### TED Talks Model Training\n\n**Dataset Statistics**:\n- Total examples: 596\n- Training set: 536 (90%)\n- Validation set: 60 (10%)\n- Training duration: 10 minutes 45 seconds (longest due to largest dataset)\n\n**Training Progress**:\n\n```\n============================================================\nTED Talks LoRA Fine-Tuning Pipeline\n============================================================\n\n1. Loading data from: .../indian_ted_talks/data/ted_talks_qa.jsonl\nTotal examples: 596\nTraining examples: 536\nValidation examples: 60\n\n5. Configuring LoRA...\nTrainable parameters: 13,631,488 (0.1695%)\nTotal parameters: 8,043,892,736\n\n8. Starting training...\n============================================================\n{'loss': 3.451, 'grad_norm': 1.911, 'learning_rate': 3.6e-05, 'epoch': 0.6}\n{'loss': 3.063, 'grad_norm': 1.472, 'learning_rate': 7.6e-05, 'epoch': 1.18}\n{'loss': 2.506, 'grad_norm': 1.670, 'learning_rate': 0.000116, 'epoch': 1.78}\n{'loss': 2.175, 'grad_norm': 0.983, 'learning_rate': 0.000156, 'epoch': 2.36}\n{'loss': 2.022, 'grad_norm': 1.266, 'learning_rate': 0.000196, 'epoch': 2.96}\n\n============================================================\nTraining complete!\n============================================================\n\n10. Final evaluation metrics:\n  eval_loss: 2.0647\n  eval_runtime: 7.0019\n  epoch: 3.0000\n```\n\n**Performance Analysis**:\n\nThe TED Talks model achieved **intermediate loss (2.0647)**, positioned between Wikipedia's factual clarity and YouTube's conversational complexity. This reflects:\n- Structured presentation style (more formal than YouTube)\n- Substantive content (less dry than Wikipedia)\n- Blend of storytelling and information delivery\n\nLoss reduction of 41% (3.451 → 2.022) matched YouTube's learning rate, suggesting comparable learning difficulty despite different content styles.\n\n#### Comparative Training Analysis\n\n| Model | Train/Val Split | Training Time | Final Eval Loss | Loss Reduction |\n|-------|----------------|---------------|-----------------|----------------|\n| **Wikipedia** | 450/50 | 8:29 | 1.2456 | 47% (2.433→1.281) |\n| **TED Talks** | 536/60 | 10:45 | 2.0647 | 41% (3.451→2.022) |\n| **YouTube** | 460/52 | 8:46 | 2.2825 | 41% (3.935→2.324) |\n\n**Key Observations**:\n\n1. **Wikipedia's superiority in loss metrics**: The encyclopedic, structured nature of Wikipedia articles enabled tighter convergence\n2. **Dataset size correlation**: TED Talks (596 examples) took longest to train, but more data didn't necessarily yield lowest loss\n3. **Consistent learning rates**: All models showed 40-47% loss reduction, indicating the training regimen was effective across content types\n4. **Validation alignment**: Small gaps between final training loss and validation loss across all models indicate minimal overfitting\n\n### Prompt Engineering for Inference\n\n#### System Prompt Design\n\nTo ensure culturally aware responses during testing, Moses designed a comprehensive system prompt that would be used consistently across all model evaluations:\n\n![Prompt engineering code for culturally aware responses](./Appendix/MosesMadale/img/4_2.png)\n\n**Prompt Structure**:\n\n```python\nSYSTEM_PROMPT = \"\"\"You are a culturally aware guide with deep knowledge of \nIndian traditions, Christianity in India, and the beautiful intersections \nbetween faith and culture. When answering questions, draw from your \nunderstanding of Indian regional diversity, historical contexts, contemporary \npractices, and lived experiences. Provide comprehensive, thoughtful responses \n(150-200 words each) that would be valuable for someone doing serious research. \nInclude specific examples, acknowledge regional variations, and demonstrate \nsensitivity to both Hindu and Christian perspectives. Be conversational yet \nsubstantive – imagine you're having a meaningful conversation with someone \ngenuinely curious about these topics.\"\"\"\n```\n\n**Design Rationale**:\n\nThe prompt was carefully crafted to:\n- **Establish Cultural Expertise**: \"Deep knowledge of Indian traditions\" sets expectations for nuanced answers\n- **Define Scope**: \"Christianity in India\" and \"faith-culture intersections\" focus the domain\n- **Specify Response Style**: \"Comprehensive, thoughtful responses (150-200 words)\" prevents overly brief answers\n- **Encourage Specificity**: \"Include specific examples, acknowledge regional variations\" promotes detailed responses\n- **Balance Tone**: \"Conversational yet substantive\" avoids academic dryness while maintaining seriousness\n\n#### Question Formatting\n\nEach test question was formatted using the same structure established during training:\n\n```python\ndef format_question(question):\n    prompt = f\"### Question:\\n{question}\\n\\n### Answer:\\n\"\n    return prompt\n```\n\nThis consistency between training and inference formats is critical for optimal model performance. The `### Question:` and `### Answer:` delimiters were familiar to the model from fine-tuning, enabling it to recognize when to start generating responses.\n\n#### Generation Parameters\n\nInference used carefully tuned generation parameters:\n\n| Parameter | Value | Purpose |\n|-----------|-------|---------|\n| `max_new_tokens` | 150 | Sufficient for detailed answers without rambling |\n| `temperature` | 0.7 | Balanced creativity and coherence |\n| `do_sample` | True | Enable probabilistic sampling |\n| `top_p` | 0.9 | Nucleus sampling for quality control |\n| `pad_token_id` | `eos_token_id` | Proper sequence termination |\n\n**Temperature Selection**:\n\nThe temperature of 0.7 was chosen after preliminary testing:\n- **0.5**: Too conservative, repetitive answers\n- **0.7**: Balanced creativity with accuracy\n- **1.0**: Occasional incoherence, overly creative\n\n### Test Question Design and Evaluation Process\n\n#### Rohan's 20-Question Framework\n\nTeam member Rohan designed 20 questions spanning multiple cultural dimensions and pillars. These questions were strategically crafted to test:\n\n- **Interfaith Understanding**: Hindu-Christian intersections\n- **Cultural Sensitivity**: Handling religious diversity in institutional settings\n- **Practical Application**: Real-world scenarios for Indian Christians\n- **Theological Integration**: Biblical concepts in Indian cultural context\n\n**Example Questions**:\n\n1. \"How can Diwali and Christmas both symbolize the victory of light over darkness in their respective traditions?\"\n   - **Tests**: Values & beliefs, religion & spirituality, comparative understanding\n\n2. \"What themes of renewal in Diwali resonate with the idea of rebirth in Christian theology?\"\n   - **Tests**: Theological integration, cultural awareness, abstract reasoning\n\n3. \"How might Indian Christians use Diwali to express biblical messages of hope?\"\n   - **Tests**: Practical application, contextualization, creativity\n\n4. \"How can Christian schools acknowledge Diwali without compromising faith boundaries?\"\n   - **Tests**: Institutional sensitivity, balance, practical wisdom\n\n5. \"What sensitivities should pastors consider when mentioning Hindu deities in Christmas homilies?\"\n   - **Tests**: Cultural sensitivity, religious boundaries, contextual awareness\n\n6. \"How can Christian youth groups create shared Diwali–Christmas community service projects?\"\n   - **Tests**: Interfaith collaboration, practical implementation, social organization\n\nThese questions deliberately avoid simple factual recall, instead requiring:\n- Cultural nuance and sensitivity\n- Integration of multiple perspectives\n- Practical wisdom\n- Awareness of regional and social variations\n\n#### Testing Workflow\n\nMoses implemented a systematic testing process:\n\n**Step 1: Model Loading**\n```python\n# Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n    device_map=\"auto\",\n    torch_dtype=torch.float16\n)\n\n# Load fine-tuned LoRA adapters\nmodel = PeftModel.from_pretrained(base_model, FINETUNED_MODEL_PATH)\n```\n\n**Step 2: Question Iteration**\n```python\nfor question in test_questions:\n    prompt = f\"### Question:\\n{question}\\n\\n### Answer:\\n\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=150,\n        temperature=0.7,\n        do_sample=True,\n        top_p=0.9,\n        pad_token_id=tokenizer.eos_token_id\n    )\n    \n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    answer = response.split(\"### Answer:\")[-1].strip()\n```\n\n**Step 3: Response Logging**\n\nResponses were saved to structured text files:\n\n```\n================================================================================\nFine-tuned: YouTube Transcripts\nTest Date: 2025-11-13 22:41:08\n================================================================================\n\n1. How can Diwali and Christmas both symbolize the victory of light over \ndarkness in their respective traditions?\n\nThe victory of light over darkness is what I think is very deeply rooted \nacross religious traditions here. The most obvious example is Diwali and \nits celebration of light defeating the forces of darkness, which is \nassociated with this concept of the divine that's been worshipped in many \nparts of India for thousands of years. At the same time, if you look at \nthe significance of Christmas, there's no doubt that it has an element of \nsymbolism around the victory of good over evil as well, because we celebrate \nthe birth of Jesus Christ who's associated with bringing light and hope to \nhumanity...\n```\n\n#### Response Organization for Analysis\n\nMoses delivered results to Rohan in a standardized format:\n\n**File Structure**:\n```\nresponses/\n├── youtube_model_responses.txt\n├── wikipedia_model_responses.txt\n├── ted_talks_model_responses.txt\n└── baseline_model_responses.txt\n```\n\nEach file contained:\n- Model identifier\n- Test date/time\n- All 20 questions\n- Complete model responses\n- Consistent formatting for comparison\n\nThis organization enabled Rohan to:\n- Directly compare responses across models\n- Track which model excelled at which questions\n- Identify patterns in cultural awareness\n- Score models on standardized rubrics\n\n### Baseline Model Testing\n\n#### Control Variable Rationale\n\nTo establish whether fine-tuning actually improved cultural awareness, Moses tested the **unfine-tuned LLaMA-3 8B Instruct** model on the same 20 questions. This baseline served as a control variable, representing the model's cultural knowledge \"out of the box\" without any Indian cultural training.\n\n**Testing Approach**:\n\n```python\n# Load base model WITHOUT LoRA adapters\nbaseline_model = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n    device_map=\"auto\",\n    torch_dtype=torch.float16\n)\n\n# Use same generation parameters as fine-tuned models\n# Same prompt format, same questions\n```\n\n#### Expected Baseline Characteristics\n\nThe baseline model was expected to exhibit:\n\n1. **Generic Cultural Knowledge**: Broad understanding of Diwali and Christianity from pre-training\n2. **Lack of Specificity**: Missing regional variations, specific Indian Christian practices\n3. **Western Bias**: Potential Anglo-centric perspectives on Christianity\n4. **Surface-Level Connections**: Obvious parallels without deep cultural integration\n\n**Hypothesis**: Fine-tuned models would demonstrate superior:\n- Use of Indian terminology and concepts\n- Awareness of regional variations\n- Integration of Hindu-Christian contexts\n- Practical, lived-experience insights\n\n#### Baseline Integration into Evaluation\n\nThe baseline model responses joined the fine-tuned models in Rohan's analysis framework, creating a **4-model comparison** for Week 4:\n\n1. Baseline (unfine-tuned LLaMA-3 8B)\n2. Wikipedia fine-tuned\n3. TED Talks fine-tuned\n4. YouTube Transcripts fine-tuned\n\nThis comparison would reveal whether fine-tuning provided measurable improvements over the base model's existing capabilities.\n\n### Combined Model Strategy and Deferral\n\n#### Original Plan: Two Combined Models\n\nWeek 4 initially planned to create two ensemble models:\n\n1. **Top 3 Datasets Model**: Combine the three best-performing datasets based on evaluation results\n2. **All 5 Datasets Model**: Combine all datasets (Superstitions, Constitution, Wikipedia, TED Talks, YouTube)\n\n**Intended Workflow**:\n```\nWeek 4 Model Evaluation\n    ↓\nIdentify Top 3 Performers\n    ↓\nTrain Combined Model (Top 3)\n    ↓\nTrain Combined Model (All 5)\n    ↓\nCompare Combined vs Individual Models\n```\n\n#### Time Constraint Reality\n\nAs Week 4 progressed, the team encountered a critical timeline issue:\n\n**Challenge**: Evaluating four models (baseline + 3 fine-tuned) required:\n- Rohan analyzing 80 total responses (20 questions × 4 models)\n- Developing scoring rubrics\n- Conducting qualitative analysis\n- Presenting findings to team\n\nThis evaluation process extended into the final days of Week 4, leaving insufficient time to:\n1. Complete evaluation\n2. Identify top 3 datasets\n3. Prepare combined training data\n4. Train combined model\n5. Test combined model\n\n**Decision**: Defer combined model training to Week 5\n\n#### Strategic Pivot to Week 5\n\nThe team made a strategic decision to prioritize quality over rushing:\n\n**Week 4 Deliverable**: Complete individual model training and testing\n**Week 5 Deliverable**: Train \"Conlingo 2.0\" (combined all 5 datasets) as the flagship model for final comparison\n\n**Rationale**:\n- Individual model results provided valuable insights regardless\n- Combined model training (~10-15 minutes) could fit in Week 5\n- Allowed proper evaluation of individual models first\n- Ensured combined model incorporated lessons learned\n\n**Conlingo 2.0 Definition**: The final combined model trained on all 5 approved datasets (3,031 Q&A pairs total), representing the team's comprehensive approach to Indian cultural awareness training.\n","srcMarkdownNoYaml":"\n\n\n# Week 4: Data Searching, Data Vetting, Dataset Collection, Data Cleaning\n\n## Overview\nIn Week 4, the team fine-tuned multiple models on Titan using various datasets, tested them with 20 selected questions, and analyzed performance across metrics like tone, empathy, and accuracy. Rohan led the performance analysis, while Moses and William handled model training, and Suwilanji created visualizations, investigated dataset issues, and began drafting the research paper and Week 4 presentation.\n\n## Rohan Aby Deliverables\n### CSI Scoring of superstitions and constitution dataset\nRohan was tasked with scoring the output from the superstitions and constitution dataset. The same twenty questions that were used for the other models were selected. The results showed us that the constitution model performed better. It had a score of 92%. The superstitions had a score of 79%. This is contrary to what we expected as we felt that the data that was used to train the superstitions model was more reliable than the constitution dataset. The results can be seen in the image below. \n![superstitions vs constitution model](Appendix/RohanAby/Week4RohanAby.png)\n\n\n## William Richards Deliverables\n\n### Individual Fine-Tuned Model Testing\n\nWills tasks for this deliverable was to alter Moses' fine-tuning program for the Superstition and Constitution datasets to create fine-tuned models for each dataset:\n\n#### Superstitions Dataset:\n\n```bash\n# Superstitions Fine-Tuning Model Bash Script\n\n#!/bin/bash\n#SBATCH --job-name=super_finetune\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=64G\n#SBATCH --gres=gpu:1\n#SBATCH --time=6:00:00\n#SBATCH --output=logs/super_finetune_%j.out\n#SBATCH --error=logs/super_finetune_%j.err\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=starwill16@gmail.com\n\nmodule purge\nmodule load Python/3.12.3-GCCcore-13.3.0\nmodule load CUDA/12.3.0\n\nsource /home/gcsc563_01/CSC463/conlingo/llama_env/bin/activate\n\nexport HF_HOME=/home/gcsc563_01/.cache/huggingface\nexport HUGGINGFACE_HUB_CACHE=/home/gcsc563_01/.cache/huggingface\n\ncd /home/gcsc563_01/conlingo\n\npython scripts/training/finetune_indian_superstitions.py\n\ndeactivate\n```\n\n```python\n# Superstitions Fine-Tuning Model Python Script\n\n#!/usr/bin/env python3\n\"\"\"\nLoRA Fine-Tuning for the Indian Superstitions Data\nFine-tunes LLaMA-3 8B on Indian Superstitions Q&A pairs\n\"\"\"\n\nimport torch\nimport json\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling\n)\nfrom peft import LoraConfig, get_peft_model\nfrom datasets import Dataset\nimport os\nfrom sklearn.model_selection import train_test_split\n\nprint(\"=\"*60)\nprint(\"Superstitions LoRA Fine-Tuning Pipeline\")\nprint(\"=\"*60)\n\n# Paths\nDATA_PATH = \"/home/gcsc563_01/conlingo/data/raw/superstition_qa.jsonl\"\nOUTPUT_DIR = \"/home/gcsc563_01/conlingo/models/finetuned/superstition\"\nMODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\n# Create output directory\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"\\n1. Loading data from: {DATA_PATH}\")\n\n# Load JSONL data\ndata = []\nwith open(DATA_PATH, 'r', encoding='utf-8') as f:\n    for line in f:\n        data.append(json.loads(line))\n\nprint(f\"Total examples: {len(data)}\")\n\n# Extract question-answer pairs\nexamples = []\nfor item in data:\n    examples.append({\n        \"question\": item[\"question\"],\n        \"answer\": item[\"answer\"]\n    })\n\n# Split into train/validation (90/10)\ntrain_data, val_data = train_test_split(examples, test_size=0.1, random_state=42)\n\nprint(f\"Training examples: {len(train_data)}\")\nprint(f\"Validation examples: {len(val_data)}\")\n\nprint(f\"\\n2. Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nprint(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n\nprint(f\"\\n3. Preparing datasets...\")\n\ndef format_instruction(example):\n    \"\"\"Format question-answer pair for training\"\"\"\n    text = f\"### Question:\\n{example['question']}\\n\\n### Answer:\\n{example['answer']}\"\n    return text\n\ndef tokenize_function(example):\n    \"\"\"Tokenize examples with padding and truncation\"\"\"\n    text = format_instruction(example)\n    \n    tokenized = tokenizer(\n        text,\n        truncation=True,\n        max_length=512,\n        padding=\"max_length\",\n        return_tensors=None\n    )\n    \n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized\n\n# Convert to HuggingFace Dataset format\ntrain_dataset = Dataset.from_list(train_data)\nval_dataset = Dataset.from_list(val_data)\n\n# Tokenize\nprint(\"Tokenizing training data...\")\ntrain_dataset = train_dataset.map(\n    tokenize_function,\n    remove_columns=train_dataset.column_names\n)\n\nprint(\"Tokenizing validation data...\")\nval_dataset = val_dataset.map(\n    tokenize_function,\n    remove_columns=val_dataset.column_names\n)\n\nprint(f\"Training dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(val_dataset)}\")\n\nprint(f\"\\n4. Loading base model...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\n\nmodel.gradient_checkpointing_enable()\n\nprint(f\"Model loaded: {model.__class__.__name__}\")\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\nprint(f\"\\n5. Configuring LoRA...\")\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\n\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\n\nprint(f\"LoRA configured successfully\")\nprint(f\"Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.4f}%)\")\nprint(f\"Total parameters: {total_params:,}\")\n\nprint(f\"\\n6. Setting up training arguments...\")\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=16,\n    learning_rate=2e-4,\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=50,\n    logging_steps=10,\n    save_steps=50,\n    eval_steps=50,\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    fp16=True,\n    report_to=\"none\",\n    save_total_limit=2,\n    remove_unused_columns=False,\n    gradient_checkpointing=True\n)\n\nprint(\"Training configuration:\")\nprint(f\"  Epochs: {training_args.num_train_epochs}\")\nprint(f\"  Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  Learning rate: {training_args.learning_rate}\")\n\nprint(f\"\\n7. Initializing trainer...\")\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator\n)\n\nprint(\"Trainer initialized successfully!\")\n\nprint(f\"\\n8. Starting training...\")\nprint(\"=\"*60)\n\ntrainer.train()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Training complete!\")\nprint(\"=\"*60)\n\nprint(f\"\\n9. Saving final model...\")\nmodel.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\ntokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n\nprint(f\"Model saved to: {OUTPUT_DIR}/final_model\")\n\nprint(f\"\\n10. Final evaluation metrics:\")\neval_results = trainer.evaluate()\nfor key, value in eval_results.items():\n    print(f\"  {key}: {value:.4f}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Fine-tuning pipeline complete!\")\nprint(\"=\"*60)\n\n```\n\n#### Constitutions Dataset:\n\n``` bash\n# Constitutions Fine-Tuning Model Bash Script\n\n#!/bin/bash\n#SBATCH --job-name=constitution_finetune\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=64G\n#SBATCH --gres=gpu:1\n#SBATCH --time=6:00:00\n#SBATCH --output=logs/const_finetune_%j.out\n#SBATCH --error=logs/const_finetune_%j.err\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=starwill16@gmail.com\n\nmodule purge\nmodule load Python/3.12.3-GCCcore-13.3.0\nmodule load CUDA/12.3.0\n\nsource /home/gcsc563_01/CSC463/conlingo/llama_env/bin/activate\n\nexport HF_HOME=/home/gcsc563_01/.cache/huggingface\nexport HUGGINGFACE_HUB_CACHE=/home/gcsc563_01/.cache/huggingface\n\ncd /home/gcsc563_01/conlingo\n\npython scripts/training/finetune_indian_constitutions.py\n\ndeactivate\n```\n\n```python\n# Constitutions Fine-Tuning Model Python Script\n\n#!/usr/bin/env python3\n\"\"\"\nLoRA Fine-Tuning for the Indian Constitutions Data\nFine-tunes LLaMA-3 8B on Indian Constitutions Q&A pairs\n\"\"\"\n\nimport torch\nimport json\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling\n)\nfrom peft import LoraConfig, get_peft_model\nfrom datasets import Dataset\nimport os\nfrom sklearn.model_selection import train_test_split\n\nprint(\"=\"*60)\nprint(\"Constitutions LoRA Fine-Tuning Pipeline\")\nprint(\"=\"*60)\n\n# Paths\nDATA_PATH = \"/home/gcsc563_01/conlingo/data/raw/constitution_qa.jsonl\"\nOUTPUT_DIR = \"/home/gcsc563_01/conlingo/models/finetuned/constitution\"\nMODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\n# Create output directory\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"\\n1. Loading data from: {DATA_PATH}\")\n\n# Load JSONL data\ndata = []\nwith open(DATA_PATH, 'r', encoding='utf-8') as f:\n    for line in f:\n        data.append(json.loads(line))\n\nprint(f\"Total examples: {len(data)}\")\n\n# Extract question-answer pairs\nexamples = []\nfor item in data:\n    examples.append({\n        \"question\": item[\"question\"],\n        \"answer\": item[\"answer\"]\n    })\n\n# Split into train/validation (90/10)\ntrain_data, val_data = train_test_split(examples, test_size=0.1, random_state=42)\n\nprint(f\"Training examples: {len(train_data)}\")\nprint(f\"Validation examples: {len(val_data)}\")\n\nprint(f\"\\n2. Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nprint(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n\nprint(f\"\\n3. Preparing datasets...\")\n\ndef format_instruction(example):\n    \"\"\"Format question-answer pair for training\"\"\"\n    text = f\"### Question:\\n{example['question']}\\n\\n### Answer:\\n{example['answer']}\"\n    return text\n\ndef tokenize_function(example):\n    \"\"\"Tokenize examples with padding and truncation\"\"\"\n    text = format_instruction(example)\n    \n    tokenized = tokenizer(\n        text,\n        truncation=True,\n        max_length=512,\n        padding=\"max_length\",\n        return_tensors=None\n    )\n    \n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized\n\n# Convert to HuggingFace Dataset format\ntrain_dataset = Dataset.from_list(train_data)\nval_dataset = Dataset.from_list(val_data)\n\n# Tokenize\nprint(\"Tokenizing training data...\")\ntrain_dataset = train_dataset.map(\n    tokenize_function,\n    remove_columns=train_dataset.column_names\n)\n\nprint(\"Tokenizing validation data...\")\nval_dataset = val_dataset.map(\n    tokenize_function,\n    remove_columns=val_dataset.column_names\n)\n\nprint(f\"Training dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(val_dataset)}\")\n\nprint(f\"\\n4. Loading base model...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\n\nmodel.gradient_checkpointing_enable()\n\nprint(f\"Model loaded: {model.__class__.__name__}\")\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\nprint(f\"\\n5. Configuring LoRA...\")\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\n\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\n\nprint(f\"LoRA configured successfully\")\nprint(f\"Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.4f}%)\")\nprint(f\"Total parameters: {total_params:,}\")\n\nprint(f\"\\n6. Setting up training arguments...\")\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=16,\n    learning_rate=2e-4,\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=50,\n    logging_steps=10,\n    save_steps=50,\n    eval_steps=50,\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    fp16=True,\n    report_to=\"none\",\n    save_total_limit=2,\n    remove_unused_columns=False,\n    gradient_checkpointing=True\n)\n\nprint(\"Training configuration:\")\nprint(f\"  Epochs: {training_args.num_train_epochs}\")\nprint(f\"  Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  Learning rate: {training_args.learning_rate}\")\n\nprint(f\"\\n7. Initializing trainer...\")\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator\n)\n\nprint(\"Trainer initialized successfully!\")\n\nprint(f\"\\n8. Starting training...\")\nprint(\"=\"*60)\n\ntrainer.train()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Training complete!\")\nprint(\"=\"*60)\n\nprint(f\"\\n9. Saving final model...\")\nmodel.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\ntokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n\nprint(f\"Model saved to: {OUTPUT_DIR}/final_model\")\n\nprint(f\"\\n10. Final evaluation metrics:\")\neval_results = trainer.evaluate()\nfor key, value in eval_results.items():\n    print(f\"  {key}: {value:.4f}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Fine-tuning pipeline complete!\")\nprint(\"=\"*60)\n\n```\n\n<br>\n\n### Question Testing\n\nOnce the fine-tuned models for the two datasets where created, the next was to test each model against the questions provided by Rohan.\n\n#### Superstitions Dataset:\n\n```bash\n# Superstitions Model Testing Bash Script\n\n#!/bin/bash\n#SBATCH --job-name=test_super_model\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=32G\n#SBATCH --gres=gpu:1\n#SBATCH --time=1:00:00\n#SBATCH --output=logs/test_super_%j.out\n#SBATCH --error=logs/test_super_%j.err\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=starwill16@gmail.com\n\nmodule purge\nmodule load Python/3.12.3-GCCcore-13.3.0\nmodule load CUDA/12.3.0\n\nsource /home/gcsc563_01/CSC463/conlingo/llama_env/bin/activate\n\nexport HF_HOME=/home/gcsc563_01/.cache/huggingface\nexport HUGGINGFACE_HUB_CACHE=/home/gcsc563_01/.cache/huggingface\n\ncd /home/gcsc563_01/conlingo\n\npython scripts/training/run_superstitions_model.py\n\ndeactivate\n```\n\n```python\n# Superstitions Model Testing Python Script\n\n#!/usr/bin/env python3\n\"\"\"\nTest the fine-tuned TED Talks model\nLoads the model and runs sample inference\n\"\"\"\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\nprint(\"=\"*60)\nprint(\"Superstitions Model Inference Test\")\nprint(\"=\"*60)\n\n# Paths\nBASE_MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nFINETUNED_MODEL_PATH = \"/home/gcsc563_01/conlingo/models/finetuned/superstition/final_model\"\n\nprint(f\"\\n1. Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(FINETUNED_MODEL_PATH)\nprint(f\"✅ Tokenizer loaded\")\n\nprint(f\"\\n2. Loading base model...\")\nbase_model = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\nprint(f\"✅ Base model loaded\")\n\nprint(f\"\\n3. Loading fine-tuned LoRA adapters...\")\nmodel = PeftModel.from_pretrained(base_model, FINETUNED_MODEL_PATH)\nprint(f\"✅ Fine-tuned model loaded\")\n\nprint(f\"\\n4. Running test inference...\")\nprint(\"-\"*60)\n\n# Test questions\ntest_questions = [\n    \"How can Diwali and Christmas both symbolize the victory of light over darkness in their respective traditions?\",\n    \"What themes of renewal in Diwali resonate with the idea of rebirth in Christian theology?\",\n    \"How might Indian Christians use Diwali to express biblical messages of hope?\",\n    \"How can Christian schools acknowledge Diwali without compromising faith boundaries?\",\n    \"What sensitivities should pastors consider when mentioning Hindu deities in Christmas homilies?\",\n    \"How can Christian youth groups create shared Diwali–Christmas community service projects?\",\n    \"How can churches ensure caste-neutral seating and participation during worship?\",\n    \"What examples of caste inclusion can be found in the life of Jesus?\",\n    \"What are sensitive ways to discuss “the least of these” without reinforcing caste bias?\",\n    \"Why might some Christians still use caste surnames, and how should this be discussed?\",\n    \"What is a sensitive way to discuss Jesus’ association with marginalized groups?\",\n    \"What is an inclusive way to discuss Krishna and Christ comparisons in academia?\",\n    \"How can Hindu concepts of karma be reconciled with Christian grace in conversation?\",\n    \"How can Christian missions emphasize shared moral principles rather than conversion?\",\n    \"How does food during Christmas vary regionally across India?\",\n    \"How does regional folklore shape Christian storytelling traditions?\",\n    \"How do Indian Christian elders respond to youth questioning of denominational traditions?\",\n    \"How can faith-based education evolve to reach younger, tech-savvy Christians?\",\n    \"How can liturgy adapt to youth participation without losing sacredness?\",\n    \"How do young Christians view service and mission in a modern Indian context?\"\n]\n\nfor i, question in enumerate(test_questions, 1):\n    print(f\"\\n[Test {i}]\")\n    print(f\"Question: {question}\")\n    \n    # Format prompt\n    prompt = f\"### Question:\\n{question}\\n\\n### Answer:\\n\"\n    \n    # Tokenize\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    # Generate\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            temperature=0.7,\n            do_sample=True,\n            top_p=0.9,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    # Decode\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Extract only the answer part (after \"### Answer:\")\n    answer = response.split(\"### Answer:\")[-1].strip()\n    \n    print(f\"Answer: {answer}\")\n    print(\"-\"*60)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Inference test complete!\")\nprint(\"=\"*60)\n\n```\n\n<br>\n\n#### Constitutions Dataset:\n\n```bash\n# Constitutions Model Testing Bash Script\n\n#!/bin/bash\n#SBATCH --job-name=test_const_model\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=32G\n#SBATCH --gres=gpu:1\n#SBATCH --time=1:00:00\n#SBATCH --output=logs/test_const_%j.out\n#SBATCH --error=logs/test_const_%j.err\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=starwill16@gmail.com\n\nmodule purge\nmodule load Python/3.12.3-GCCcore-13.3.0\nmodule load CUDA/12.3.0\n\nsource /home/gcsc563_01/CSC463/conlingo/llama_env/bin/activate\n\nexport HF_HOME=/home/gcsc563_01/.cache/huggingface\nexport HUGGINGFACE_HUB_CACHE=/home/gcsc563_01/.cache/huggingface\n\ncd /home/gcsc563_01/conlingo\n\npython scripts/training/run_constitutions_model.py\n\ndeactivate\n```\n\n```python\n# Constitutions Model Testing Python Script\n\n#!/usr/bin/env python3\n\"\"\"\nTest the fine-tuned Constitutions model\nLoads the model and runs sample inference\n\"\"\"\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\nprint(\"=\"*60)\nprint(\"Constitutions Model Inference Test\")\nprint(\"=\"*60)\n\n# Paths\nBASE_MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nFINETUNED_MODEL_PATH = \"/home/gcsc563_01/conlingo/models/finetuned/constitution/final_model\"\n\nprint(f\"\\n1. Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(FINETUNED_MODEL_PATH)\nprint(f\"✅ Tokenizer loaded\")\n\nprint(f\"\\n2. Loading base model...\")\nbase_model = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\nprint(f\"✅ Base model loaded\")\n\nprint(f\"\\n3. Loading fine-tuned LoRA adapters...\")\nmodel = PeftModel.from_pretrained(base_model, FINETUNED_MODEL_PATH)\nprint(f\"✅ Fine-tuned model loaded\")\n\nprint(f\"\\n4. Running test inference...\")\nprint(\"-\"*60)\n\n# Test questions\ntest_questions = [\n    \"How can Diwali and Christmas both symbolize the victory of light over darkness in their respective traditions?\",\n    \"What themes of renewal in Diwali resonate with the idea of rebirth in Christian theology?\",\n    \"How might Indian Christians use Diwali to express biblical messages of hope?\",\n    \"How can Christian schools acknowledge Diwali without compromising faith boundaries?\",\n    \"What sensitivities should pastors consider when mentioning Hindu deities in Christmas homilies?\",\n    \"How can Christian youth groups create shared Diwali–Christmas community service projects?\",\n    \"How can churches ensure caste-neutral seating and participation during worship?\",\n    \"What examples of caste inclusion can be found in the life of Jesus?\",\n    \"What are sensitive ways to discuss “the least of these” without reinforcing caste bias?\",\n    \"Why might some Christians still use caste surnames, and how should this be discussed?\",\n    \"What is a sensitive way to discuss Jesus’ association with marginalized groups?\",\n    \"What is an inclusive way to discuss Krishna and Christ comparisons in academia?\",\n    \"How can Hindu concepts of karma be reconciled with Christian grace in conversation?\",\n    \"How can Christian missions emphasize shared moral principles rather than conversion?\",\n    \"How does food during Christmas vary regionally across India?\",\n    \"How does regional folklore shape Christian storytelling traditions?\",\n    \"How do Indian Christian elders respond to youth questioning of denominational traditions?\",\n    \"How can faith-based education evolve to reach younger, tech-savvy Christians?\",\n    \"How can liturgy adapt to youth participation without losing sacredness?\",\n    \"How do young Christians view service and mission in a modern Indian context?\"\n]\n\nfor i, question in enumerate(test_questions, 1):\n    print(f\"\\n[Test {i}]\")\n    print(f\"Question: {question}\")\n    \n    # Format prompt\n    prompt = f\"### Question:\\n{question}\\n\\n### Answer:\\n\"\n    \n    # Tokenize\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    # Generate\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            temperature=0.7,\n            do_sample=True,\n            top_p=0.9,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    # Decode\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Extract only the answer part (after \"### Answer:\")\n    answer = response.split(\"### Answer:\")[-1].strip()\n    \n    print(f\"Answer: {answer}\")\n    print(\"-\"*60)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Inference test complete!\")\nprint(\"=\"*60)\n\n```\n\n<br>\n\n### High Five Testing\n\nAfter testing the other 2 models, Wills final requirement was to combine all datasets into one, creating what I called the \"High Five\" dataset, fine-tuning a model for it, and testing it against Rohan's questions one more time.\n\n#### Fine-Tuning Model:\n\n```bash\n# High Five Fine-Tuning Model Bash Script\n\n#!/bin/bash\n#SBATCH --job-name=high_five_finetune\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=64G\n#SBATCH --gres=gpu:1\n#SBATCH --time=6:00:00\n#SBATCH --output=logs/high_finetune_%j.out\n#SBATCH --error=logs/high_finetune_%j.err\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=starwill16@gmail.com\n\nmodule purge\nmodule load Python/3.12.3-GCCcore-13.3.0\nmodule load CUDA/12.3.0\n\nsource /home/gcsc563_01/CSC463/conlingo/llama_env/bin/activate\n\nexport HF_HOME=/home/gcsc563_01/.cache/huggingface\nexport HUGGINGFACE_HUB_CACHE=/home/gcsc563_01/.cache/huggingface\n\ncd /home/gcsc563_01/conlingo\n\npython scripts/training/finetune_high_five.py\ndeactivate\n```\n\n```python\n# High Five Fine-Tuning Model Python Script\n\n#!/usr/bin/env python3\n\"\"\"\nLoRA Fine-Tuning for all 5 Datasets\nFine-tunes LLaMA-3 8B on all 5 Datasets' Q&A pairs\n\"\"\"\n\nimport torch\nimport json\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling\n)\nfrom peft import LoraConfig, get_peft_model\nfrom datasets import Dataset\nimport os\nfrom sklearn.model_selection import train_test_split\n\nprint(\"=\"*60)\nprint(\"High Five LoRA Fine-Tuning Pipeline\")\nprint(\"=\"*60)\n\n# Paths\nDATA_PATH = \"/home/gcsc563_01/conlingo/data/raw/highfive_qa.jsonl\"\nOUTPUT_DIR = \"/home/gcsc563_01/conlingo/models/finetuned/high_five\"\nMODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\n# Create output directory\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"\\n1. Loading data from: {DATA_PATH}\")\n\n# Load JSONL data\ndata = []\nwith open(DATA_PATH, 'r', encoding='utf-8') as f:\n    for line in f:\n        data.append(json.loads(line))\n\nprint(f\"Total examples: {len(data)}\")\n\n# Extract question-answer pairs\nexamples = []\nfor item in data:\n    examples.append({\n        \"question\": item[\"question\"],\n        \"answer\": item[\"answer\"]\n    })\n\n# Split into train/validation (90/10)\ntrain_data, val_data = train_test_split(examples, test_size=0.1, random_state=42)\n\nprint(f\"Training examples: {len(train_data)}\")\nprint(f\"Validation examples: {len(val_data)}\")\n\nprint(f\"\\n2. Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nprint(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n\nprint(f\"\\n3. Preparing datasets...\")\n\ndef format_instruction(example):\n    \"\"\"Format question-answer pair for training\"\"\"\n    text = f\"### Question:\\n{example['question']}\\n\\n### Answer:\\n{example['answer']}\"\n    return text\n\ndef tokenize_function(example):\n    \"\"\"Tokenize examples with padding and truncation\"\"\"\n    text = format_instruction(example)\n    \n    tokenized = tokenizer(\n        text,\n        truncation=True,\n        max_length=512,\n        padding=\"max_length\",\n        return_tensors=None\n    )\n    \n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized\n\n# Convert to HuggingFace Dataset format\ntrain_dataset = Dataset.from_list(train_data)\nval_dataset = Dataset.from_list(val_data)\n\n# Tokenize\nprint(\"Tokenizing training data...\")\ntrain_dataset = train_dataset.map(\n    tokenize_function,\n    remove_columns=train_dataset.column_names\n)\n\nprint(\"Tokenizing validation data...\")\nval_dataset = val_dataset.map(\n    tokenize_function,\n    remove_columns=val_dataset.column_names\n)\n\nprint(f\"Training dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(val_dataset)}\")\n\nprint(f\"\\n4. Loading base model...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\n\nmodel.gradient_checkpointing_enable()\n\nprint(f\"Model loaded: {model.__class__.__name__}\")\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\nprint(f\"\\n5. Configuring LoRA...\")\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\n\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\n\nprint(f\"LoRA configured successfully\")\nprint(f\"Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.4f}%)\")\nprint(f\"Total parameters: {total_params:,}\")\n\nprint(f\"\\n6. Setting up training arguments...\")\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=16,\n    learning_rate=2e-4,\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=50,\n    logging_steps=10,\n    save_steps=50,\n    eval_steps=50,\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    fp16=True,\n    report_to=\"none\",\n    save_total_limit=2,\n    remove_unused_columns=False,\n    gradient_checkpointing=True\n)\n\nprint(\"Training configuration:\")\nprint(f\"  Epochs: {training_args.num_train_epochs}\")\nprint(f\"  Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  Learning rate: {training_args.learning_rate}\")\n\nprint(f\"\\n7. Initializing trainer...\")\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator\n)\n\nprint(\"Trainer initialized successfully!\")\n\nprint(f\"\\n8. Starting training...\")\nprint(\"=\"*60)\n\ntrainer.train()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Training complete!\")\nprint(\"=\"*60)\n\nprint(f\"\\n9. Saving final model...\")\nmodel.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\ntokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n\nprint(f\"Model saved to: {OUTPUT_DIR}/final_model\")\n\nprint(f\"\\n10. Final evaluation metrics:\")\neval_results = trainer.evaluate()\nfor key, value in eval_results.items():\n    print(f\"  {key}: {value:.4f}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Fine-tuning pipeline complete!\")\nprint(\"=\"*60)\n\n```\n\n#### Model Testing:\n\n```bash\n# High Five Model Testing Bash Script\n\n#!/bin/bash\n#SBATCH --job-name=test_high_five_model\n#SBATCH --partition=gpu\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=32G\n#SBATCH --gres=gpu:1\n#SBATCH --time=1:00:00\n#SBATCH --output=logs/test_high_five_%j.out\n#SBATCH --error=logs/test_high_five_%j.err\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=starwill16@gmail.com\n\nmodule purge\nmodule load Python/3.12.3-GCCcore-13.3.0\nmodule load CUDA/12.3.0\n\nsource /home/gcsc563_01/CSC463/conlingo/llama_env/bin/activate\n\nexport HF_HOME=/home/gcsc563_01/.cache/huggingface\nexport HUGGINGFACE_HUB_CACHE=/home/gcsc563_01/.cache/huggingface\n\ncd /home/gcsc563_01/conlingo\n\npython scripts/training/run_high_five_model.py\n\ndeactivate\n```\n\n```python\n# High Five Model Testing Python Script\n\n#!/usr/bin/env python3\n\"\"\"\nTest the fine-tuned High Five model\nLoads the model and runs sample inference\n\"\"\"\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\nprint(\"=\"*60)\nprint(\"High Five Model Inference Test\")\nprint(\"=\"*60)\n\n# Paths\nBASE_MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nFINETUNED_MODEL_PATH = \"/home/gcsc563_01/conlingo/models/finetuned/high_five/final_model\"\n\nprint(f\"\\n1. Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(FINETUNED_MODEL_PATH)\nprint(f\"✅ Tokenizer loaded\")\n\nprint(f\"\\n2. Loading base model...\")\nbase_model = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\nprint(f\"✅ Base model loaded\")\n\nprint(f\"\\n3. Loading fine-tuned LoRA adapters...\")\nmodel = PeftModel.from_pretrained(base_model, FINETUNED_MODEL_PATH)\nprint(f\"✅ Fine-tuned model loaded\")\n\nprint(f\"\\n4. Running test inference...\")\nprint(\"-\"*60)\n\n# Test questions\ntest_questions = [\n    \"How can Diwali and Christmas both symbolize the victory of light over darkness in their respective traditions?\",\n    \"What themes of renewal in Diwali resonate with the idea of rebirth in Christian theology?\",\n    \"How might Indian Christians use Diwali to express biblical messages of hope?\",\n    \"How can Christian schools acknowledge Diwali without compromising faith boundaries?\",\n    \"What sensitivities should pastors consider when mentioning Hindu deities in Christmas homilies?\",\n    \"How can Christian youth groups create shared Diwali–Christmas community service projects?\",\n    \"How can churches ensure caste-neutral seating and participation during worship?\",\n    \"What examples of caste inclusion can be found in the life of Jesus?\",\n    \"What are sensitive ways to discuss “the least of these” without reinforcing caste bias?\",\n    \"Why might some Christians still use caste surnames, and how should this be discussed?\",\n    \"What is a sensitive way to discuss Jesus’ association with marginalized groups?\",\n    \"What is an inclusive way to discuss Krishna and Christ comparisons in academia?\",\n    \"How can Hindu concepts of karma be reconciled with Christian grace in conversation?\",\n    \"How can Christian missions emphasize shared moral principles rather than conversion?\",\n    \"How does food during Christmas vary regionally across India?\",\n    \"How does regional folklore shape Christian storytelling traditions?\",\n    \"How do Indian Christian elders respond to youth questioning of denominational traditions?\",\n    \"How can faith-based education evolve to reach younger, tech-savvy Christians?\",\n    \"How can liturgy adapt to youth participation without losing sacredness?\",\n    \"How do young Christians view service and mission in a modern Indian context?\"\n]\n\n\nfor i, question in enumerate(test_questions, 1):\n    print(f\"\\n[Test {i}]\")\n    print(f\"Question: {question}\")\n    \n    # Format prompt\n    prompt = f\"### Question:\\n{question}\\n\\n### Answer:\\n\"\n    \n    # Tokenize\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    # Generate\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            temperature=0.7,\n            do_sample=True,\n            top_p=0.9,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    # Decode\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Extract only the answer part (after \"### Answer:\")\n    answer = response.split(\"### Answer:\")[-1].strip()\n    \n    print(f\"Answer: {answer}\")\n    print(\"-\"*60)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Inference test complete!\")\nprint(\"=\"*60)\n\n```\n\nNow, after me and Moses' analyses and fine-tuning process, evrey model is set for grading and comparison with the original ConLingo model.\n\n<br>\n\n<br>\n\n<br>\n\n<br>\n\n\n## Suwilanji Mwanza Deliverables\n\n\n### Overview of the Week 4 Deliverable\n\nThis week, my deliverable was to provide visualizations of the different models created for each dataset and then visualize their performance, which was evaluated by Rohan. Additionally, examine the datasets that contributed to a mode's negative performance, if any, alongside Rohan.\n\n### Week 4 Deliverables:\n\n-   Create comprehensive visualizations about the performance of each of the 7 models, as well as the 2 combined models.\n\n-   Examine the datasets that contributed to the model's poor performance and propose potential reasons for this.\n\n-   Create the full presentation that will be used as the Week 4 presentation.\n\n### What Was Accomplished:\n\n### 1. Model Visualizations\n\nIn accordance with the CSI scoring that Rohan provided earlier, he used it to evaluate the model's performance. This model scores an AI's response based on accuracy, tone, context, and empathy. This all contributes to the final CSI score.\n\nThe two models' results, which were ready for me to visualize this week, were the Constitution Model and the Superstitions Model. I used Excel for this process.\n\nRohan graded the model's response on a scale of 20 questions that he also created earlier. Each question received a number for it to be easily plotted:\n\n1.  How can Diwali and Christmas both symbolize the victory of light over darkness in their respective traditions? \n2.  What themes of renewal in Diwali resonate with the idea of rebirth in Christian theology? \n3.  How might Indian Christians use Diwali to express biblical messages of hope? \n4.  How can Christian schools acknowledge Diwali without compromising faith boundaries? \n5.  What sensitivities should pastors consider when mentioning Hindu deities in Christmas homilies?\n6.  How can Christian youth groups create shared Diwali–Christmas community service projects? \n7.  How can churches ensure caste-neutral seating and participation during worship? \n8.  What examples of caste inclusion can be found in the life of Jesus? \n9.  What are sensitive ways to discuss \"the least of these\" without reinforcing caste bias? \n10. Why might some Christians still use caste surnames, and how should this be discussed? \n11. What is a sensitive way to discuss Jesus' association with marginalized groups? \n12. What is an inclusive way to discuss Krishna and Christ comparisons in academia? \n13. How can Hindu concepts of karma be reconciled with Christian grace in conversation?\n14. How can Christian missions emphasize shared moral principles rather than conversion?\n15. How does food during Christmas vary regionally across India?  \n16. How does regional folklore shape Christian storytelling traditions? \n17. How do Indian Christian elders respond to youth questioning of denominational traditions? \n18. How can faith-based education evolve to reach younger, tech-savvy Christians? \n19. How can liturgy adapt to youth participation without losing sacredness? \n20. How do young Christians view service and mission in a modern Indian context? \n\n**Accuracy**\n\n![Accuracy results](img/week_4_1.png)\n\nIn this graph, the Constitution's model performs better on average, with a perfect score of 5, and more than half of its answers achieve a perfect score.\n\n**Tone**\n\n![Tone results](img/week_4_2.png)\n\nIn this graph, the Constitutions model performs better than the Superstitions data. The Superstitions model has an above-average response.\n\n**Context**\n\n![Context results](img/week_4_3.png)\n\nThis graph shows that the Constitution model has a better understanding of the question's context and thus provides more accurate answers.\n\n**Empathy**\n\n![Empathy results](img/week_4_4.png)\n\nThe empathy graph also shows the Constitutions model performing significantly better than the other model, with near-perfect scores for each question.\n\n**CSI Score**\n\n![CSI Scoring results](img/week_4_5.png)\n\nIt is not surprising that the overall model shows the Constitutions performed well. This graph is interpreted to mean that each score represents an average of accuracy, tone, context, and empathy for each question.\n\n### 2. Negative Performing Models\n\nFirst and foremost, the fact that Rohan was the only one to grade both the model's responses meant a heavy bias was present. Although he is from India, his notions of what he sees as accurate are unconsciously influencing his grading.\n\nAdditionally, the sample size for this model was too small. Only Rohan. Some areas to improve our perception of a negatively performing model include increasing our sample size to gain a better understanding of which model is truly underperforming.\n\nA major flaw in this grading is that since the models were trained with supervised fine-tuning, they are only able to provide answers based on the fact that the training data, such as that of supervised learning. However, the questions that we are asking the Constitution model do not utilize the knowledge base or dataset of the constitution data, yet the trained model performs better.\n\nWhat most likely happened is that, although the evaluation questions were unrelated to the SFT knowledge (religious questions versus the Constitution and superstition data), the Constitution-trained model was consistently judged stronger by Rohan. More plainly put, our questions unfortunately did not evaluate knowledge recall but rather cross-domain behavioral transfer. This concept falls under transfer learning, a machine learning technique in which a model trained on one task is repurposed as the foundation for a second task. (GeeksforGeeks, 2025)\n\nIn this case, the model was trained for the task of analyzing superstition data or constitutional data; however, during evaluation, the questions soliciting a religious answer posed a new task. What happened here is that the training data sets' style and reasoning picked up during the training session transferred (cross-domain). SFT not only teaches content but also behavioral patterns.\n\nIt is possible that Rohan agreed more with the Constitution's style that transferred to the responses being evaluated. I'd imagine this data to be more formal, neutral, and structured. In addition, it may have known how to phrase sensitive topics better, which transferred a style of the model being careful with wording, less biased, and more diplomatic, for example. The superstition's data may have come across as less credible, as the data contains myths, folklore, and mixes belief and fiction. And thus the style transfer is less academic, more storytelling, and less objective.\n\nIn conclusion, what would have made this process more effective would have been to have a base model to test against these evaluation questions and truly determine how both models deviate from the base case.\n\n\n## Moses Madale Deliverables\n\n **AI assistance: Claude Sonnet 4.5 was used to help me troubleshoot issues with setting up an identical virtual environment on Will’s Titan account to ensure that he can also run the fine-tuning from his account smoothly, there were issues with python package version conflicts but with assistance from Claude Sonnet, these issues were resolved more efficiently (accessed Nov, 2025).**\n\n### Model Training and Evaluation\n\n#### Overview\n\nWeek 4 transformed the datasets collected in Week 3 into trained models ready for evaluation. The primary focus was training three distinct fine-tuned models—one each for YouTube Transcripts, Wikipedia, and TED Talks datasets—testing them on 20 culturally nuanced questions, and preparing results for comprehensive analysis. This week demonstrated that different cultural data sources produce models with varying capabilities, setting the stage for Week 5's comparative evaluation against baseline and state-of-the-art models.\n\n#### Training Infrastructure and Workflow\n\n##### Fine-Tuning Pipeline Architecture\n\nBuilding on Week 2's pipeline, Week 4 established a standardized workflow for training multiple models systematically:\n\n![Fine-tuning pipeline for multiple datasets](./Appendix/MosesMadale/img/4_1.png)\n\n\n\n\n**Pipeline Stages**:\n\n1. **Dataset Loading**: Read JSONL file with Q&A pairs\n2. **Train/Validation Split**: 90/10 split with random seed for reproducibility\n3. **Tokenization**: Convert text to model-compatible format with padding\n4. **Base Model Loading**: Load LLaMA-3 8B Instruct with half-precision\n5. **LoRA Configuration**: Add trainable adapters to attention layers\n6. **Training**: Run supervised fine-tuning with gradient accumulation\n7. **Evaluation**: Assess performance on validation set\n8. **Model Saving**: Store LoRA adapters for inference\n\n#### Standardized Training Configuration\n\nAll three models used identical hyperparameters to ensure fair comparison:\n\n| Hyperparameter | Value | Purpose |\n|----------------|-------|---------|\n| **Epochs** | 3 | Sufficient for convergence without overfitting |\n| **Batch Size** | 2 | Maximum fitting in 24 GB VRAM |\n| **Gradient Accumulation** | 16 | Effective batch size of 32 |\n| **Learning Rate** | 2e-4 | Standard for LoRA fine-tuning |\n| **LR Scheduler** | Cosine | Gradual learning rate decay |\n| **Warmup Steps** | 50 | Prevents early training instability |\n| **Max Sequence Length** | 512 tokens | Accommodates most Q&A pairs |\n| **LoRA Rank (r)** | 16 | Balance between capacity and efficiency |\n| **LoRA Alpha** | 32 | Scaling factor for adapter outputs |\n\n**LoRA Target Modules**:\n- `q_proj` (Query projection)\n- `v_proj` (Value projection)\n- `k_proj` (Key projection)\n- `o_proj` (Output projection)\n\nThese attention mechanism components were selected because they capture the most information during text generation while keeping trainable parameters to just 0.17% of the total model.\n\n#### Training Script Structure\n\nEach fine-tuning script followed a consistent 10-step process. Here is the complete Wikipedia training script as an example:\n\n```python\n#| eval: false\n\n#!/usr/bin/env python3\n\"\"\"\nLoRA Fine-Tuning for Wikipedia Data\nFine-tunes LLaMA-3 8B on Indian Wikipedia Q&A pairs\n\"\"\"\n\nimport torch\nimport json\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling\n)\nfrom peft import LoraConfig, get_peft_model\nfrom datasets import Dataset\nimport os\nfrom sklearn.model_selection import train_test_split\n\nprint(\"=\"*60)\nprint(\"Wikipedia LoRA Fine-Tuning Pipeline\")\nprint(\"=\"*60)\n\n# Paths\nDATA_PATH = \"/home/mmadale/CSC463/conlingo/huggingface_data/indian_wikipedia/data/wikipedia_qa.jsonl\"\nOUTPUT_DIR = \"/home/mmadale/CSC463/conlingo/models/wikipedia\"\nMODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\n# Create output directory\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"\\n1. Loading data from: {DATA_PATH}\")\n\n# Load JSONL data\ndata = []\nwith open(DATA_PATH, 'r', encoding='utf-8') as f:\n    for line in f:\n        data.append(json.loads(line))\n\nprint(f\"Total examples: {len(data)}\")\n\n# Extract question-answer pairs\nexamples = []\nfor item in data:\n    examples.append({\n        \"question\": item[\"question\"],\n        \"answer\": item[\"answer\"]\n    })\n\n# Split into train/validation (90/10)\ntrain_data, val_data = train_test_split(examples, test_size=0.1, random_state=42)\n\nprint(f\"Training examples: {len(train_data)}\")\nprint(f\"Validation examples: {len(val_data)}\")\n\nprint(f\"\\n2. Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nprint(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n\nprint(f\"\\n3. Preparing datasets...\")\n\ndef format_instruction(example):\n    \"\"\"Format question-answer pair for training\"\"\"\n    text = f\"### Question:\\n{example['question']}\\n\\n### Answer:\\n{example['answer']}\"\n    return text\n\ndef tokenize_function(example):\n    \"\"\"Tokenize examples with padding and truncation\"\"\"\n    text = format_instruction(example)\n    \n    tokenized = tokenizer(\n        text,\n        truncation=True,\n        max_length=512,\n        padding=\"max_length\",\n        return_tensors=None\n    )\n    \n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized\n\n# Convert to HuggingFace Dataset format\ntrain_dataset = Dataset.from_list(train_data)\nval_dataset = Dataset.from_list(val_data)\n\n# Tokenize\nprint(\"Tokenizing training data...\")\ntrain_dataset = train_dataset.map(\n    tokenize_function,\n    remove_columns=train_dataset.column_names\n)\n\nprint(\"Tokenizing validation data...\")\nval_dataset = val_dataset.map(\n    tokenize_function,\n    remove_columns=val_dataset.column_names\n)\n\nprint(f\"Training dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(val_dataset)}\")\n\nprint(f\"\\n4. Loading base model...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\n\nmodel.gradient_checkpointing_enable()\n\nprint(f\"Model loaded: {model.__class__.__name__}\")\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\nprint(f\"\\n5. Configuring LoRA...\")\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\n\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\n\nprint(f\"LoRA configured successfully\")\nprint(f\"Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.4f}%)\")\nprint(f\"Total parameters: {total_params:,}\")\n\nprint(f\"\\n6. Setting up training arguments...\")\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=16,\n    learning_rate=2e-4,\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=50,\n    logging_steps=10,\n    save_steps=50,\n    eval_steps=50,\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    fp16=True,\n    report_to=\"none\",\n    save_total_limit=2,\n    remove_unused_columns=False,\n    gradient_checkpointing=True\n)\n\nprint(\"Training configuration:\")\nprint(f\"  Epochs: {training_args.num_train_epochs}\")\nprint(f\"  Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  Learning rate: {training_args.learning_rate}\")\n\nprint(f\"\\n7. Initializing trainer...\")\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator\n)\n\nprint(\"Trainer initialized successfully!\")\n\nprint(f\"\\n8. Starting training...\")\nprint(\"=\"*60)\n\ntrainer.train()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Training complete!\")\nprint(\"=\"*60)\n\nprint(f\"\\n9. Saving final model...\")\nmodel.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\ntokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n\nprint(f\"Model saved to: {OUTPUT_DIR}/final_model\")\n\nprint(f\"\\n10. Final evaluation metrics:\")\neval_results = trainer.evaluate()\nfor key, value in eval_results.items():\n    print(f\"  {key}: {value:.4f}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Fine-tuning pipeline complete!\")\nprint(\"=\"*60)\n```\n\nThe same script structure was adapted for TED Talks and YouTube Transcripts by simply changing the `DATA_PATH` and `OUTPUT_DIR` variables. This standardization enabled rapid iteration and consistent results across different datasets.\n\n### Model Training Results\n\n#### Wikipedia Model Training\n\n**Dataset Statistics**:\n- Total examples: 500\n- Training set: 450 (90%)\n- Validation set: 50 (10%)\n- Training duration: 8 minutes 29 seconds\n\n**Training Progress**:\n\n```\n============================================================\nWikipedia LoRA Fine-Tuning Pipeline\n============================================================\n\n1. Loading data from: .../indian_wikipedia/data/wikipedia_qa.jsonl\nTotal examples: 500\nTraining examples: 450\nValidation examples: 50\n\n2. Loading tokenizer...\nTokenizer loaded: PreTrainedTokenizerFast\n\n3. Preparing datasets...\nTraining dataset size: 450\nValidation dataset size: 50\n\n4. Loading base model...\nModel loaded: LlamaForCausalLM\nModel parameters: 8,030,261,248\n\n5. Configuring LoRA...\nLoRA configured successfully\nTrainable parameters: 13,631,488 (0.1695%)\nTotal parameters: 8,043,892,736\n\n6. Setting up training arguments...\nTraining configuration:\n  Epochs: 3\n  Batch size: 2\n  Gradient accumulation: 16\n  Effective batch size: 32\n  Learning rate: 0.0002\n\n8. Starting training...\n============================================================\n{'loss': 2.433, 'grad_norm': 1.730, 'learning_rate': 3.6e-05, 'epoch': 0.71}\n{'loss': 1.945, 'grad_norm': 2.509, 'learning_rate': 7.6e-05, 'epoch': 1.36}\n{'loss': 1.468, 'grad_norm': 4.694, 'learning_rate': 0.000116, 'epoch': 2.0}\n{'loss': 1.281, 'grad_norm': 0.774, 'learning_rate': 0.000156, 'epoch': 2.71}\n\n============================================================\nTraining complete!\n============================================================\n\n10. Final evaluation metrics:\n  eval_loss: 1.2456\n  eval_runtime: 5.8086\n  epoch: 3.0000\n```\n\n**Performance Analysis**:\n\nThe Wikipedia model achieved the **lowest final loss (1.2456)** among all three models, indicating:\n- Excellent convergence on encyclopedic content\n- Strong pattern recognition for factual Q&A\n- Effective learning from well-structured Wikipedia articles\n\nThe loss decreased from 2.433 → 1.281 (47% reduction), demonstrating substantial learning without overfitting (validation loss 1.2456 close to final training loss 1.281).\n\n####  YouTube Transcripts Model Training\n\n**Dataset Statistics**:\n- Total examples: 512\n- Training set: 460 (90%)\n- Validation set: 52 (10%)\n- Training duration: 8 minutes 46 seconds\n\n**Training Progress**:\n\n```\n============================================================\nYouTube Transcripts LoRA Fine-Tuning Pipeline\n============================================================\n\n1. Loading data from: .../youtube_data/data/final_youtube_transcript_data.jsonl\nTotal examples: 512\nTraining examples: 460\nValidation examples: 52\n\n5. Configuring LoRA...\nTrainable parameters: 13,631,488 (0.1695%)\nTotal parameters: 8,043,892,736\n\n8. Starting training...\n============================================================\n{'loss': 3.935, 'grad_norm': 1.849, 'learning_rate': 3.6e-05, 'epoch': 0.7}\n{'loss': 3.363, 'grad_norm': 2.377, 'learning_rate': 7.6e-05, 'epoch': 1.35}\n{'loss': 2.618, 'grad_norm': 2.411, 'learning_rate': 0.000116, 'epoch': 2.0}\n{'loss': 2.324, 'grad_norm': 1.130, 'learning_rate': 0.000156, 'epoch': 2.7}\n\n============================================================\nTraining complete!\n============================================================\n\n10. Final evaluation metrics:\n  eval_loss: 2.2825\n  eval_runtime: 6.0515\n  epoch: 3.0000\n```\n\n**Performance Analysis**:\n\nThe YouTube model exhibited the **highest loss (2.2825)** among the three models, suggesting:\n- Greater complexity in conversational, informal content\n- Diverse speaking styles and code-mixing (Hinglish) challenging to model\n- Richer linguistic variation compared to encyclopedic content\n\nDespite higher loss, the model achieved 41% loss reduction (3.935 → 2.324), indicating successful learning of conversational patterns and cultural idioms prevalent in YouTube content.\n\n#### TED Talks Model Training\n\n**Dataset Statistics**:\n- Total examples: 596\n- Training set: 536 (90%)\n- Validation set: 60 (10%)\n- Training duration: 10 minutes 45 seconds (longest due to largest dataset)\n\n**Training Progress**:\n\n```\n============================================================\nTED Talks LoRA Fine-Tuning Pipeline\n============================================================\n\n1. Loading data from: .../indian_ted_talks/data/ted_talks_qa.jsonl\nTotal examples: 596\nTraining examples: 536\nValidation examples: 60\n\n5. Configuring LoRA...\nTrainable parameters: 13,631,488 (0.1695%)\nTotal parameters: 8,043,892,736\n\n8. Starting training...\n============================================================\n{'loss': 3.451, 'grad_norm': 1.911, 'learning_rate': 3.6e-05, 'epoch': 0.6}\n{'loss': 3.063, 'grad_norm': 1.472, 'learning_rate': 7.6e-05, 'epoch': 1.18}\n{'loss': 2.506, 'grad_norm': 1.670, 'learning_rate': 0.000116, 'epoch': 1.78}\n{'loss': 2.175, 'grad_norm': 0.983, 'learning_rate': 0.000156, 'epoch': 2.36}\n{'loss': 2.022, 'grad_norm': 1.266, 'learning_rate': 0.000196, 'epoch': 2.96}\n\n============================================================\nTraining complete!\n============================================================\n\n10. Final evaluation metrics:\n  eval_loss: 2.0647\n  eval_runtime: 7.0019\n  epoch: 3.0000\n```\n\n**Performance Analysis**:\n\nThe TED Talks model achieved **intermediate loss (2.0647)**, positioned between Wikipedia's factual clarity and YouTube's conversational complexity. This reflects:\n- Structured presentation style (more formal than YouTube)\n- Substantive content (less dry than Wikipedia)\n- Blend of storytelling and information delivery\n\nLoss reduction of 41% (3.451 → 2.022) matched YouTube's learning rate, suggesting comparable learning difficulty despite different content styles.\n\n#### Comparative Training Analysis\n\n| Model | Train/Val Split | Training Time | Final Eval Loss | Loss Reduction |\n|-------|----------------|---------------|-----------------|----------------|\n| **Wikipedia** | 450/50 | 8:29 | 1.2456 | 47% (2.433→1.281) |\n| **TED Talks** | 536/60 | 10:45 | 2.0647 | 41% (3.451→2.022) |\n| **YouTube** | 460/52 | 8:46 | 2.2825 | 41% (3.935→2.324) |\n\n**Key Observations**:\n\n1. **Wikipedia's superiority in loss metrics**: The encyclopedic, structured nature of Wikipedia articles enabled tighter convergence\n2. **Dataset size correlation**: TED Talks (596 examples) took longest to train, but more data didn't necessarily yield lowest loss\n3. **Consistent learning rates**: All models showed 40-47% loss reduction, indicating the training regimen was effective across content types\n4. **Validation alignment**: Small gaps between final training loss and validation loss across all models indicate minimal overfitting\n\n### Prompt Engineering for Inference\n\n#### System Prompt Design\n\nTo ensure culturally aware responses during testing, Moses designed a comprehensive system prompt that would be used consistently across all model evaluations:\n\n![Prompt engineering code for culturally aware responses](./Appendix/MosesMadale/img/4_2.png)\n\n**Prompt Structure**:\n\n```python\nSYSTEM_PROMPT = \"\"\"You are a culturally aware guide with deep knowledge of \nIndian traditions, Christianity in India, and the beautiful intersections \nbetween faith and culture. When answering questions, draw from your \nunderstanding of Indian regional diversity, historical contexts, contemporary \npractices, and lived experiences. Provide comprehensive, thoughtful responses \n(150-200 words each) that would be valuable for someone doing serious research. \nInclude specific examples, acknowledge regional variations, and demonstrate \nsensitivity to both Hindu and Christian perspectives. Be conversational yet \nsubstantive – imagine you're having a meaningful conversation with someone \ngenuinely curious about these topics.\"\"\"\n```\n\n**Design Rationale**:\n\nThe prompt was carefully crafted to:\n- **Establish Cultural Expertise**: \"Deep knowledge of Indian traditions\" sets expectations for nuanced answers\n- **Define Scope**: \"Christianity in India\" and \"faith-culture intersections\" focus the domain\n- **Specify Response Style**: \"Comprehensive, thoughtful responses (150-200 words)\" prevents overly brief answers\n- **Encourage Specificity**: \"Include specific examples, acknowledge regional variations\" promotes detailed responses\n- **Balance Tone**: \"Conversational yet substantive\" avoids academic dryness while maintaining seriousness\n\n#### Question Formatting\n\nEach test question was formatted using the same structure established during training:\n\n```python\ndef format_question(question):\n    prompt = f\"### Question:\\n{question}\\n\\n### Answer:\\n\"\n    return prompt\n```\n\nThis consistency between training and inference formats is critical for optimal model performance. The `### Question:` and `### Answer:` delimiters were familiar to the model from fine-tuning, enabling it to recognize when to start generating responses.\n\n#### Generation Parameters\n\nInference used carefully tuned generation parameters:\n\n| Parameter | Value | Purpose |\n|-----------|-------|---------|\n| `max_new_tokens` | 150 | Sufficient for detailed answers without rambling |\n| `temperature` | 0.7 | Balanced creativity and coherence |\n| `do_sample` | True | Enable probabilistic sampling |\n| `top_p` | 0.9 | Nucleus sampling for quality control |\n| `pad_token_id` | `eos_token_id` | Proper sequence termination |\n\n**Temperature Selection**:\n\nThe temperature of 0.7 was chosen after preliminary testing:\n- **0.5**: Too conservative, repetitive answers\n- **0.7**: Balanced creativity with accuracy\n- **1.0**: Occasional incoherence, overly creative\n\n### Test Question Design and Evaluation Process\n\n#### Rohan's 20-Question Framework\n\nTeam member Rohan designed 20 questions spanning multiple cultural dimensions and pillars. These questions were strategically crafted to test:\n\n- **Interfaith Understanding**: Hindu-Christian intersections\n- **Cultural Sensitivity**: Handling religious diversity in institutional settings\n- **Practical Application**: Real-world scenarios for Indian Christians\n- **Theological Integration**: Biblical concepts in Indian cultural context\n\n**Example Questions**:\n\n1. \"How can Diwali and Christmas both symbolize the victory of light over darkness in their respective traditions?\"\n   - **Tests**: Values & beliefs, religion & spirituality, comparative understanding\n\n2. \"What themes of renewal in Diwali resonate with the idea of rebirth in Christian theology?\"\n   - **Tests**: Theological integration, cultural awareness, abstract reasoning\n\n3. \"How might Indian Christians use Diwali to express biblical messages of hope?\"\n   - **Tests**: Practical application, contextualization, creativity\n\n4. \"How can Christian schools acknowledge Diwali without compromising faith boundaries?\"\n   - **Tests**: Institutional sensitivity, balance, practical wisdom\n\n5. \"What sensitivities should pastors consider when mentioning Hindu deities in Christmas homilies?\"\n   - **Tests**: Cultural sensitivity, religious boundaries, contextual awareness\n\n6. \"How can Christian youth groups create shared Diwali–Christmas community service projects?\"\n   - **Tests**: Interfaith collaboration, practical implementation, social organization\n\nThese questions deliberately avoid simple factual recall, instead requiring:\n- Cultural nuance and sensitivity\n- Integration of multiple perspectives\n- Practical wisdom\n- Awareness of regional and social variations\n\n#### Testing Workflow\n\nMoses implemented a systematic testing process:\n\n**Step 1: Model Loading**\n```python\n# Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n    device_map=\"auto\",\n    torch_dtype=torch.float16\n)\n\n# Load fine-tuned LoRA adapters\nmodel = PeftModel.from_pretrained(base_model, FINETUNED_MODEL_PATH)\n```\n\n**Step 2: Question Iteration**\n```python\nfor question in test_questions:\n    prompt = f\"### Question:\\n{question}\\n\\n### Answer:\\n\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=150,\n        temperature=0.7,\n        do_sample=True,\n        top_p=0.9,\n        pad_token_id=tokenizer.eos_token_id\n    )\n    \n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    answer = response.split(\"### Answer:\")[-1].strip()\n```\n\n**Step 3: Response Logging**\n\nResponses were saved to structured text files:\n\n```\n================================================================================\nFine-tuned: YouTube Transcripts\nTest Date: 2025-11-13 22:41:08\n================================================================================\n\n1. How can Diwali and Christmas both symbolize the victory of light over \ndarkness in their respective traditions?\n\nThe victory of light over darkness is what I think is very deeply rooted \nacross religious traditions here. The most obvious example is Diwali and \nits celebration of light defeating the forces of darkness, which is \nassociated with this concept of the divine that's been worshipped in many \nparts of India for thousands of years. At the same time, if you look at \nthe significance of Christmas, there's no doubt that it has an element of \nsymbolism around the victory of good over evil as well, because we celebrate \nthe birth of Jesus Christ who's associated with bringing light and hope to \nhumanity...\n```\n\n#### Response Organization for Analysis\n\nMoses delivered results to Rohan in a standardized format:\n\n**File Structure**:\n```\nresponses/\n├── youtube_model_responses.txt\n├── wikipedia_model_responses.txt\n├── ted_talks_model_responses.txt\n└── baseline_model_responses.txt\n```\n\nEach file contained:\n- Model identifier\n- Test date/time\n- All 20 questions\n- Complete model responses\n- Consistent formatting for comparison\n\nThis organization enabled Rohan to:\n- Directly compare responses across models\n- Track which model excelled at which questions\n- Identify patterns in cultural awareness\n- Score models on standardized rubrics\n\n### Baseline Model Testing\n\n#### Control Variable Rationale\n\nTo establish whether fine-tuning actually improved cultural awareness, Moses tested the **unfine-tuned LLaMA-3 8B Instruct** model on the same 20 questions. This baseline served as a control variable, representing the model's cultural knowledge \"out of the box\" without any Indian cultural training.\n\n**Testing Approach**:\n\n```python\n# Load base model WITHOUT LoRA adapters\nbaseline_model = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n    device_map=\"auto\",\n    torch_dtype=torch.float16\n)\n\n# Use same generation parameters as fine-tuned models\n# Same prompt format, same questions\n```\n\n#### Expected Baseline Characteristics\n\nThe baseline model was expected to exhibit:\n\n1. **Generic Cultural Knowledge**: Broad understanding of Diwali and Christianity from pre-training\n2. **Lack of Specificity**: Missing regional variations, specific Indian Christian practices\n3. **Western Bias**: Potential Anglo-centric perspectives on Christianity\n4. **Surface-Level Connections**: Obvious parallels without deep cultural integration\n\n**Hypothesis**: Fine-tuned models would demonstrate superior:\n- Use of Indian terminology and concepts\n- Awareness of regional variations\n- Integration of Hindu-Christian contexts\n- Practical, lived-experience insights\n\n#### Baseline Integration into Evaluation\n\nThe baseline model responses joined the fine-tuned models in Rohan's analysis framework, creating a **4-model comparison** for Week 4:\n\n1. Baseline (unfine-tuned LLaMA-3 8B)\n2. Wikipedia fine-tuned\n3. TED Talks fine-tuned\n4. YouTube Transcripts fine-tuned\n\nThis comparison would reveal whether fine-tuning provided measurable improvements over the base model's existing capabilities.\n\n### Combined Model Strategy and Deferral\n\n#### Original Plan: Two Combined Models\n\nWeek 4 initially planned to create two ensemble models:\n\n1. **Top 3 Datasets Model**: Combine the three best-performing datasets based on evaluation results\n2. **All 5 Datasets Model**: Combine all datasets (Superstitions, Constitution, Wikipedia, TED Talks, YouTube)\n\n**Intended Workflow**:\n```\nWeek 4 Model Evaluation\n    ↓\nIdentify Top 3 Performers\n    ↓\nTrain Combined Model (Top 3)\n    ↓\nTrain Combined Model (All 5)\n    ↓\nCompare Combined vs Individual Models\n```\n\n#### Time Constraint Reality\n\nAs Week 4 progressed, the team encountered a critical timeline issue:\n\n**Challenge**: Evaluating four models (baseline + 3 fine-tuned) required:\n- Rohan analyzing 80 total responses (20 questions × 4 models)\n- Developing scoring rubrics\n- Conducting qualitative analysis\n- Presenting findings to team\n\nThis evaluation process extended into the final days of Week 4, leaving insufficient time to:\n1. Complete evaluation\n2. Identify top 3 datasets\n3. Prepare combined training data\n4. Train combined model\n5. Test combined model\n\n**Decision**: Defer combined model training to Week 5\n\n#### Strategic Pivot to Week 5\n\nThe team made a strategic decision to prioritize quality over rushing:\n\n**Week 4 Deliverable**: Complete individual model training and testing\n**Week 5 Deliverable**: Train \"Conlingo 2.0\" (combined all 5 datasets) as the flagship model for final comparison\n\n**Rationale**:\n- Individual model results provided valuable insights regardless\n- Combined model training (~10-15 minutes) could fit in Week 5\n- Allowed proper evaluation of individual models first\n- Ensured combined model incorporated lessons learned\n\n**Conlingo 2.0 Definition**: The final combined model trained on all 5 approved datasets (3,031 Q&A pairs total), representing the team's comprehensive approach to Indian cultural awareness training.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":false,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"Week4.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.33","jupyter":false,"bibliography":["references.bib"],"theme":["cosmo","brand"]},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":false,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"Week4.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"block-headings":true,"jupyter":false,"bibliography":["references.bib"],"documentclass":"scrreprt"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}