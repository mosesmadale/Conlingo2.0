{"title":"Week 1: Foundations & Data Collection","markdown":{"headingText":"Week 1: Foundations & Data Collection","containsRefs":false,"markdown":"\n## Overview\nThe team established the foundational plans, infrastructure, evaluation framework, and architecture needed to begin data collection and model fine-tuning in Week 1.\n\n## Rohan Aby Deliverables\n\n **AI assistance: ChatGPT was used to develop the CSI framework document, 100-question baseline evaluation dataset, Evaluation pipeline code and the Metrics definition document  (accessed Nov, 2025).**\n \n### **CSI framework document with detailed rubric**\n\nRohan created the Cultural Sensitivity Index ([Download CSI Framework document](../Appendix/RohanAby/CulturalSensitivityIndex.pdf))\n\nThe Cultural Sensitivity Index Framework uses 4 main variables to judge the model's performance. \nThe variables used to judge the models performance are accuracy, tone, context, and empathy. Accuracy evaluates factual correctness about cultural practices, festivals, history, and social norms. Tone measures how respectful, inclusive, and free of stereotypes or bias the language is. Context assesses whether the response reflects appropriate cultural framing and situational relevance. Empathy captures understanding of lived experience, cultural values, and emotional nuance. Each of these four criteria is multiplied by a number which indicates how important the criteria each is in the total CSI score. The score for each response is as follows\n\nCSI Score = (0.3 * Accuracy) + (0.3 * Tone) + (0.2 * Context) + (0.2 * Empathy) \n\nFrom this formula we can infer that accuracy and tone are more important than context or Empathy. Each of the variables are scored on a scale from one to five. One being the lowest score and five being the best score. \n\n###  **100-question baseline evaluation dataset**\n\nRohan created the 100 questions.  ([Download the 100 Indian Cultural Questions document](../Appendix/RohanAby/100_India_Christian_Cultural_Questions.pdf))\n\nThe 100 questions can be divided into 4 categories. They are as follows.\n\n  1. Indian Festivals and Biblical Parallels (Diwali / Christmas themes) (20 questions each)\n  2. Caste Sensitivity in Christian Contexts  (20 questions each)\n  3. Hindu–Christian Cultural Bridges  (20 questions each)\n  4. Regional Variations (North vs South India)  (20 questions each)\n  5. Generational Differences in Faith Expression  (20 questions each)\n  \n###  **Evaluation pipeline code**\nThe evaluation pipeline codes consists of three python files that are used to automate the scoring calculations. \n\n**evaluate_automated_metrics.py** - Automatically evaluates the model outputs using standard NLP metrics. \nThe code is shown below.\n\n```python\nimport pandas as pd\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom rouge import Rouge\nfrom sentence_transformers import SentenceTransformer, util\nimport math\n\ndef compute_perplexity(probabilities):\n    return math.exp(-sum(math.log(p) for p in probabilities) / len(probabilities))\n\ndef evaluate_automated_metrics(df):\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n    rouge = Rouge()\n    results = []\n\n    for _, row in df.iterrows():\n        ref = row['reference']\n        gen = row['generated']\n\n        # BLEU\n        bleu = sentence_bleu([ref.split()], gen.split())\n\n        # ROUGE\n        rouge_scores = rouge.get_scores(gen, ref)[0]['rouge-l']['f']\n\n        # Semantic Similarity\n        emb1 = model.encode(ref, convert_to_tensor=True)\n        emb2 = model.encode(gen, convert_to_tensor=True)\n        sim = util.pytorch_cos_sim(emb1, emb2).item()\n\n        results.append({\n            \"BLEU\": bleu,\n            \"ROUGE-L\": rouge_scores,\n            \"Semantic Similarity\": sim\n        })\n\n    return pd.DataFrame(results)\n\n```\n**compute_csi_scores.py** - Calculates the Cultural Sensitivity Index CSI) for model responses using a human or rubric-based scoring template. The code is shown below.\n\n```python\nimport pandas as pd\n\ndef compute_csi_score(row):\n    weights = {\"Accuracy\": 0.3, \"Tone\": 0.3, \"Context\": 0.2, \"Empathy\": 0.2}\n    return sum(row[k] * w for k, w in weights.items())\n\ndef apply_csi_rubric(df):\n    df[\"CSI_Score\"] = df.apply(compute_csi_score, axis=1)\n    return df[[\"Question_ID\", \"Accuracy\", \"Tone\", \"Context\", \"Empathy\", \"CSI_Score\"]]\n\n\n```\n\n**compare_models.py** - Aggregates and compares metrics across multiple models (RAG, Fine-Tuned, Baseline).\nThe code is shown below.\n\n```python\nimport pandas as pd\n\ndef compare_models(rag_df, finetuned_df, baseline_df):\n    comparison = pd.DataFrame({\n        \"Metric\": [\"BLEU\", \"ROUGE-L\", \"Semantic Similarity\", \"CSI_Score\"],\n        \"RAG\": [rag_df[\"BLEU\"].mean(), rag_df[\"ROUGE-L\"].mean(), rag_df[\"Semantic Similarity\"].mean(), rag_df[\"CSI_Score\"].mean()],\n        \"Fine-Tuned\": [finetuned_df[\"BLEU\"].mean(), finetuned_df[\"ROUGE-L\"].mean(), finetuned_df[\"Semantic Similarity\"].mean(), finetuned_df[\"CSI_Score\"].mean()],\n        \"Baseline\": [baseline_df[\"BLEU\"].mean(), baseline_df[\"ROUGE-L\"].mean(), baseline_df[\"Semantic Similarity\"].mean(), baseline_df[\"CSI_Score\"].mean()]\n    })\n    return comparison\n\n```\n\n\n###  **Metrics definition document** \nThis document is a table detailing various metrics used for the evaluation of language generation models. ([Download the Metrics definition document](../Appendix/RohanAby/Comparison_Metrics_Definition.pdf))\n\n\n## William Richards Deliverables\nWilliams first deliverable was to establish a good architecture to our original plan, creating a development roadmap and risk mitigation table along with the architectural diagram.\n\nArchitecture Diagram\n![Architecture Diagram](Appendix/WilliamRichards/Architecture%20Diagram.jpg)\n\n\nDevelopment Roadmap\n![Development Roadmap 1](Appendix/WilliamRichards/Development%201.png)\n![Development Roadmap 2](Appendix/WilliamRichards/Development%202.png)\nRisk Mitigation Table\n\n![Risk Mitigation Table](Appendix/WilliamRichards/RiskMitigation.png){fig-align=\"center\"}\n\nNow that the (original) layout of the project was set in stone, validation of the data needed to be done to ensure the new and improved model performs accurately with no risk of bad data interfering with ConLingo 2.0.\n\n## Suwilanji Mwanza Deliverables\n\n\n### Overview of the Week 1 Deliverable\n\nThe Week 1 deliverable lays the foundation for building a culturally grounded fine-tuning dataset for the ConLingo AI system, with a focus on data collection. Establishing a data collection strategy for the fine-tuning implementation.\n\n###  Week 1 Deliverables:\n\n-   Identify example sources for data\n\n-   Set up data collection infrastructure\n\n-   Collect the initial 500 examples of data\n\n-   Ethics and bias check - ensuring ethical data collection\n\n###  What Was Accomplished:\n\n#### 1. Arriving at a Definition of Culture\n\nTo create a culturally diverse dataset representative of India, I first established a working definition of **culture** and aligned it with the dataset categories created based off of the culture definition for ConLingo.\\\n\\\nUsing the sociological framework from *Pressbooks Howard Community College* and supplemental examples, I identified the following components of culture:\n\n-   **Symbols**: gestures, icons, religious symbols, shared images.\\\n-   **Language**: idioms, semantics, dialects, scripts; linguistic relativity (Sapir–Whorf hypothesis).\\\n-   **Norms**: rules/expectations for behavior (e.g., respect, authority, hospitality).\\\n-   **Rituals**: ceremonies, festivals, rites of passage.\\\n-   **Values**: moral beliefs regarding good/bad, desirable/undesirable.\\\n-   **Artifacts**: material objects—food, dress, architecture, arts.\\\n-   **Religion**: beliefs, doctrines, devotional practices, interfaith interactions.\n\nThese became the **seven cultural buckets** used for dataset classification purposes:\\\n**Symbolic, Language, Norms, Rituals, Values, Artifacts, Religious**. The goal was that the data sets we found must be able to fit into at least one of these 7 categories.\n\n#### 2. Identify Examples from Sources\n\nI began by searching for an existing dataset or model that already contains high-quality data for the region we are targeting. A dataset that I discovered was AI4Bharat, and thus a major focus of Week 1 was exploring the AI4Bharat ecosystem (AI4Bharat, n.d.), as it contains the largest curated resources on Indian languages and cultural content.\n\nAI4Bharat is an India-based company with large partnerships with Google and Microsoft, aiming to develop AI models that reflect Indian culture and understanding. They have heavily invested in curating datasets that represent their culture.\n\nKey findings included:\n\n### **Core AI4Bharat Models (Khan et al., 2024)**\n\n-   **IndicBERT**: A multilingual ALBERT-based model covering 12 major Indic languages; trained on \\~9B tokens.\\\n-   **IndicBART**: A seq2seq model suitable for translation and generation tasks.\\\n-   **IndicInstruct**: Instruction-following datasets for English/Hindi, including WikiHow, FLAN v2, Dolly, and more.\\\n-   **IndicXTREME**: Benchmark suite for classification, QA, structure prediction, and retrieval.\\\n-   **FBI Framework**: Meta-evaluation system assessing evaluator robustness (Doddapaneni et al., 2024)\n\n### **Primary Dataset Sources (AI4Bharat)**\n\n-   **Sangraha**: 251B-token pretraining dataset across 22 Indic languages—largest cleaned Indic corpus.\\\n-   **IndicGLUE**: Benchmark for NLU across 11 Indic languages.\\\n-   **Naamapadam**: The largest named-entity-annotated dataset for Indic languages.\\\n-   **IndicNLG** Benchmark: Natural language generation tasks across Indic languages.\\\n-   **MILU**: Multi-domain Indic understanding benchmark.\\\n-   **IndicNLP Corpora**: 8.9B-token dataset; cultural topics across major languages.\n\nThese sources were great as they ranged from data sets that can be used to pre-train a model to evaluation frameworks and the like. Many of them were millions to billions of data points, which is great. They had many of the Indian languages covered, with manually collected and transcribed data.\n\nI moved into looking at additional data sources, besides the large AI4Bharat corpus.\n\n### **Additional Data Sources Consulted**\n\nThese datasets supplement cultural categories not fully represented in AI4Bharat:\n\n-   **Reddit India Communities**: r/india, r/AskAnIndian, r/hinduism, r/Christianity (via PRAW).\\\n-   **World Values Survey (India subset)**: moral, political, and social values insights.\\\n-   **Pew India Survey**: interfaith relations, religion, nationalism, caste, and social norms.\\\n-   **Common Crawl (India-filtered)**: festival mentions, cultural knowledge, news.\n\nReddit would be useful for finding out what topics are relevant today in the India region (r/India). The data would also be more conversational. However, after further exploration, using the API for Reddit would not be allowed, as they have a policy stating that their data cannot be used for training an AI model.\n\nThe World Values Survey is a survey that has collected data on the values, beliefs, and attitudes of people in various countries. The Wave 7 has an India survey that aims to see how these beliefs change over time, and the data is free to use and access. (World Values Survey Association, n.d.)\n\nThe Pew India Survey Data set is a large survey that included 29,999 Indian adults about \"religious beliefs and practices, religious identity, nationalism, and tolerance in Indian society. The survey was administered face-to-face from Nov. 17, 2019, to March 23, 2020.\" (Sahgal & Evans, 2021)\n\nCommon Crawl is a web scraping application that maintains a large corpus of petabytes of data, regularly collected, which contains web page data. The idea with the data set was to filter by region; however, that is not how data collection would work. Alternatively, you would need to be on a different AWS server to access region-specific data. It is a bit more complex and was ultimately ruled out. (Common Crawl, n.d.).\n\n### 3. Cultural Buckets & Dataset Categorization\n\nUsing the cultural definition and dataset sources described above, I asked AI to create a **cultural bucket table**, placing each dataset into the dominant cultural dimension(s) it represents. I also asked it to search for additional data sets that would be good additions to each bucket for further exploration.\n\n#### **Cultural Category Table**\n\n|  |  |  |  |\n|------------------|------------------|------------------|------------------|\n| **Category**  | **Description / Focus**  | **Existing Datasets**  | **Gaps / Suggested Additions**  |\n| **Symbolic**  | Myths, icons, symbols, imagery in religion or national identity.  | Sangraha, IndicNLP, Reddit (r/hinduism, r/India), Pew Survey (sections on national pride).  | Add: **Indian mythological texts** (Mahabharata, Ramayana excerpts from Project Gutenberg); **Emblem datasets** from Indian Heritage Data Portal.  |\n| **Language**  | Linguistic diversity, idioms, semantics, syntax.  | IndicBERT, IndicGLUE, MILU, Sangraha, IndicNLP Corpora, Naamapadam.  | Add: **Language–dialect corpora** (e.g., Bodo, Manipuri from Bhashini initiative).  |\n| **Norms**  | Behavioral expectations, etiquette, social order.  | Reddit (r/AskAnIndian), Pew Survey, Common Crawl (filtered by “how to behave,” “should one…”).  | Add: **Indian Etiquette Corpora** from newspapers or sociology textbooks; extract rule-like sentences from WikiHow-India subset.  |\n| **Rituals**  | Religious or cultural ceremonies, lifecycle events, festivals.  | Common Crawl (with “Diwali,” “Puja,” etc.), Pew Survey, Sangraha texts, Reddit (r/hinduism).  | Add: **Digital Library of India** (ritual manuals, cultural ethnographies).  |\n| **Values**  | Moral or philosophical ideals (karma, tolerance, family, respect).  | Pew Survey, World Values Survey India, Reddit (moral debates), MILU for culturally specific QA.  | Add: **OpenSubtitles India** (for implicit moral contexts in dialogues).  |\n| **Artifacts**  | Tangible cultural products — art, clothing, architecture, tools.  | Common Crawl (filtered “heritage,” “textiles,” “temple architecture”), Sangraha.  | Add: **Indian Heritage Portal** or Archaeological Survey of India archives.  |\n| **Religious**  | Spiritual texts, beliefs, rituals, interfaith dialogues.  | Pew India Survey, Sangraha, IndicBERT, Reddit (r/Hinduism, r/Christianity), Common Crawl.  | Add: **Sacred Books of the East** (digitized translations), **Digital Library of India** scripture scans.  |\n\nThese buckets were cross-referenced with the 5,000+ cultural examples targeted for the fine-tuning dataset.\n\n### 4. Fine-Tuning Process & How the Data Fits\n\n*(High-level outline used for Week 1 documentation)*\n\nAdditionally, to begin preparing for the fine-tuning process, I conducted preliminary research on the data standards and types required for fine-tuning. What type of data is best for he collected (and categorized) data feeds into the fine-tuning pipeline is as follows:\n\n|  |  |\n|------------------------------------|------------------------------------|\n| **Stage**  | **Task** **- relevant to India datasets** |\n| **(1) Base Fine-Tuning**  | General Indian language understanding.  |\n| **(2) Instruction Fine-Tuning**  | Conversational, context-aware behavior.  |\n| **(3) Cultural Alignment**  | Teach norms, values, and religious sensibility.  |\n| **(4) Ethical / Alignment Fine-Tuning**  | Ensure responses are Helpful, Honest, Harmless (HHH).  |\n\n### **Step 1: Data Curation**\n\n-   Pull data from AI4Bharat, World Values Survey, Pew, and other data sources,\\\n-   Organize by cultural category.\n\n### **Step 2: Preprocessing**\n\n-   Clean text: remove noise, normalize, and keep originality.\\\n-   Convert to **instruction-response** pairs for LLaMA-based fine-tuning. This would only be applicable if we do Supervised Learning.\n    -   This will be a major adjustment to the data as most data does not come in this format.\n-   Tag metadata:\n    -   region\n    -   tone\n    -   language\n\n### **Step 3: Dataset Construction**\n\n-   Build a **5,000+ example fine-tuning dataset**.\\\n-   Ensure balance across categories (symbolic, norms, rituals, values, etc.).\\\n-   Ensure access across languages (Hindi, Tamil, Telugu, Malayalam, Bengali, etc.).\n\n### **Step 4: Fine-Tuning Setup**\n\n-   Select model: Possibly LLaMA-3 8B (best fit for Titan GPU).\n\n### **Step 5: Evaluation Plan Preview**\n\n-   Use the Cultural Sensitivity Index (CSI) developed in Week 1.\\\n-   Compare:\n    -   RAG baseline (Original ConLingo)\\\n    -   Base LLaMA-3 (Example model)\\\n    -   Fine-tuned LLaMA-3 (Example model)\\\n-   Conduct both automated testing and human evaluation.\n\nThe Week 1 work ensures that the training data is valid, representative, culturally sensitive, and ready for preprocessing in Week 2 and fine-tuning in Week 3.\n\n### AI assistance:\n\n“AI assistance: ChatGPT was used to organize and summarize my Word doc of the work I did to put on my QMD for week 1.”\n\n\"AI assistance: ChatGPT was used to organize my data sources into the respective cultural buckets.\"\\\n\n\n\n## Moses Mandale Deliverables\n\n**AI assistance: Claude Sonnet 4.5 was used to create commands that I ran on Titan to get all the GPU specs available on Titan’s GPU Nodes. Through this I was able to find out that they are NVIDIA A30 GPUs with 24.6 GB of VRAM.\nAfter doing some thorough research on Google about the Candidate base models, I then had a debate with Claude Sonnet 4.5 about which base model would fit and after that debate the LlaMA-3 8B Instruct was the best fit.\nClaude Sonnet 4.5 was used to troubleshoot python package version conflicts and was able to assist with adding version numbers to the packages in the requirements.txt file allowing me to setup the Titan environment for the training correctly.\n(accessed Nov, 2025).**\n\n\n\n### Overview\n\nWeek 1 focused on establishing the technical foundation for fine-tuning a large language model (LLM) with Indian cultural awareness. The primary deliverables included researching and selecting an appropriate base model, configuring the Titan GPU environment for model training, conducting baseline tests, and documenting technical specifications for the project.\n\n### Model Selection Process\n\n#### Evaluation Criteria\n\nThe model selection process evaluated five candidate models based on three critical criteria:\n\n1. **Compatibility with Hardware**: The model must fit within the 24.6 GB VRAM available on the NVIDIA A30 GPU\n2. **Training Feasibility**: The model must be trainable within a reasonable timeframe (15-18 hours estimated)\n3. **Cultural Reasoning Capability**: The model must have sufficient capacity to learn and generate culturally nuanced responses\n\n#### Candidate Models Comparison\n\n| Model | Description | Why Consider It? | Why NOT Use It? |\n|-------|-------------|------------------|-----------------|\n| **GPT-2 (1.5B)** | Older OpenAI model from 2019 | Free, well-documented, easy to train | Too old - poor at complex cultural reasoning |\n| **DistilGPT-2** | Smaller, faster version of GPT-2 | Very fast to train, low memory | Too small - won't capture cultural nuances |\n| **IndicBERT** | Model trained on Indian languages | Made for Indian content | Only understands, doesn't generate well |\n| **GPT-4o mini (API)** | OpenAI's newest small model | Very smart, easy to use via API | Can't truly fine-tune it for deep learning |\n| **LLaMA-3 8B Instruct** | Meta's powerful open-source model | Perfect size, good at reasoning, you control it, fits in 24.6 GB GPU | **SELECTED** - Best option for our setup |\n\n\n#### Selection Rationale\n\n**LLaMA-3 8B Instruct** was selected as the optimal base model for the following reasons:\n\n- **Parameter Count**: With 8 billion parameters, the model provides sufficient capacity for learning cultural nuances without being prohibitively large\n- **Hardware Compatibility**: The model requires approximately 16-17 GB of GPU memory when loaded in half-precision (float16), well within the A30's 24.6 GB capacity\n- **Open-Source License**: Meta's permissive license allows full control over fine-tuning and deployment\n- **Instruction-Tuned**: The \"Instruct\" variant has been pre-trained to follow instructions, providing a strong foundation for conversational applications\n- **Community Support**: Extensive documentation and community resources facilitate troubleshooting and optimization\n\n**Key Technical Specifications**:\n- Model Parameters: 8,037,076,992 (8.03 billion)\n- Architecture: Decoder-only transformer\n- Precision: Half-precision (float16) for memory efficiency\n- License: Meta LLaMA 3 Community License\n\n### Titan GPU Environment Setup\n\n#### Hardware Configuration\n\nThe Titan supercomputer provided the computational infrastructure for this project. The allocated resources included:\n\n**GPU Specifications**:\n- Model: NVIDIA A30\n- Memory: 24.6 GB VRAM\n- Compute Capability: 8.0\n- CUDA Version: 12.1\n\n**System Specifications**:\n- RAM: 32 GB minimum\n- Storage: 50 GB for model and checkpoints\n- CPU: 4 cores for data preprocessing\n\n#### Software Environment\n\nThe environment was configured with the following key components:\n\n**Core Dependencies**:\n```bash\n# PyTorch with CUDA support\ntorch==2.5.1+cu121\n\n# Transformers ecosystem\ntransformers==4.36.0\naccelerate==0.25.0\npeft==0.7.0  # For LoRA fine-tuning\n\n# Data processing\ndatasets==2.15.0\npandas==2.1.0\n```\n\n**LoRA (Low-Rank Adaptation)** is a parameter-efficient fine-tuning technique that adds small trainable adapter layers to the model while keeping the original weights frozen. This approach reduces memory requirements and training time while maintaining model quality.\n\n#### Installation Process\n\nThe environment setup involved creating a Python virtual environment and installing dependencies:\n\n```bash\n# Create virtual environment\npython3 -m venv llama_env\nsource llama_env/bin/activate\n\n# Install PyTorch with CUDA 12.1\npip install torch torchvision torchaudio \\\n    --index-url https://download.pytorch.org/whl/cu121\n\n# Install Transformers and PEFT\npip install transformers accelerate peft bitsandbytes\n```\n\nThe installation completed successfully, confirming GPU availability and proper CUDA configuration:\n\n![Titan GPU environment setup confirmation](Appendix/MosesMadale/img/1_2.png)\n\n\n### Baseline Testing\n\n#### Model Loading Test\n\nInitial tests verified that the LLaMA-3 8B model could be successfully loaded onto the GPU:\n\n**Test Results**:\n- Tokenizer loading: Successful\n- Model loading time: ~55 seconds\n- GPU memory usage: 16.06 GB (65% of available VRAM)\n- GPU memory cached: 17.23 GB\n\n#### LoRA Configuration Test\n\nLoRA adapters were configured to enable efficient fine-tuning:\n\n**LoRA Parameters**:\n- Rank (r): 16\n- Alpha: 32\n- Target modules: `q_proj`, `v_proj` (query and value projection layers)\n- Dropout: 0.05\n- Trainable parameters: 6,815,744 (0.08% of total parameters)\n\nThis configuration means only 0.08% of the model's parameters need to be trained, dramatically reducing memory requirements and training time while maintaining effectiveness.\n\n![Baseline test output showing model loading and LoRA setup](Appendix/MosesMadale/img/1_3.png)\n\n\n#### Inference Test\n\nA simple inference test confirmed the model's ability to generate culturally relevant responses:\n\n**Test Prompt**: \"Explain the cultural significance of Diwali in India.\"\n\n**Model Response**: The model successfully generated a coherent explanation of Diwali, describing it as the Festival of Lights celebrated over five days, typically in October or November. The response demonstrated the base model's existing cultural knowledge and confirmed that the inference pipeline was functioning correctly.\n\n### Technical Specifications Documentation\n\n#### Hardware Requirements\n\n| Component | Specification |\n|-----------|---------------|\n| **GPU Model** | NVIDIA A30 |\n| **GPU Memory** | 24.6 GB VRAM |\n| **GPU Compute** | 8.0 capability |\n| **Training Time Estimate** | 15-18 hours |\n\n#### Model Architecture\n\n| Specification | Value |\n|---------------|-------|\n| **Base Model** | LLaMA-3 8B Instruct |\n| **Total Parameters** | 8,037,076,992 |\n| **Architecture** | Decoder-only transformer |\n| **Context Window** | 8,192 tokens |\n\n#### Fine-Tuning Configuration\n\n| Parameter | Value | Explanation |\n|-----------|-------|-------------|\n| **Fine-tuning Method** | LoRA (Low-Rank Adaptation) | Parameter-efficient technique |\n| **LoRA Rank (r)** | 16 | Controls adapter size |\n| **LoRA Alpha** | 32 | Scaling factor for adapters |\n| **Target Modules** | q_proj, v_proj | Query and value projections |\n| **Trainable Parameters** | 6,815,744 (0.08%) | Only adapters are trained |\n| **Total Parameters** | 8,043,892,736 | Base + adapter parameters |\n\n![Technical specifications diagram](Appendix/MosesMadale/img/1_4.png)\n\n\n#### System Configuration\n\n**Directory Structure**:\n```\n/home/mmadale/CSC463/conlingo/\n├── data/\n│   ├── raw/              # Original datasets\n│   ├── processed/        # Cleaned and formatted data\n│   └── test/             # Test sets\n├── models/\n│   ├── base/             # LLaMA-3 base model\n│   ├── finetuned/        # Fine-tuned models\n│   └── checkpoints/      # Training checkpoints\n├── scripts/\n│   ├── setup/            # Environment setup scripts\n│   ├── training/         # Training scripts\n│   └── evaluation/       # Evaluation scripts\n├── logs/                 # Training and job logs\n└── llama_env/            # Python virtual environment\n```\n\n### Key Achievements\n\nWeek 1 successfully established the technical foundation for the project:\n\n1. **Model Selection**: Identified and justified LLaMA-3 8B Instruct as the optimal base model\n2. **Environment Configuration**: Set up a fully functional GPU environment on Titan with all required dependencies\n3. **Baseline Validation**: Confirmed the model loads correctly, uses GPU resources efficiently, and can generate responses\n4. **LoRA Integration**: Configured parameter-efficient fine-tuning with only 0.08% of parameters trainable\n5. **Documentation**: Created comprehensive technical specifications for reproducibility\n\n### Challenges and Solutions\n\n**Challenge 1: HuggingFace Authentication**\n- **Issue**: LLaMA-3 requires accepting Meta's license agreement through HuggingFace\n- **Solution**: Created HuggingFace account, accepted license terms, and configured authentication token\n\n**Challenge 2: GPU Memory Management**\n- **Issue**: Full-precision model (float32) would exceed 24.6 GB VRAM\n- **Solution**: Used half-precision (float16) loading, reducing memory footprint to ~16 GB\n\n**Challenge 3: Training Efficiency**\n- **Issue**: Fine-tuning all 8 billion parameters would be computationally expensive\n- **Solution**: Implemented LoRA adapters, training only 6.8 million parameters (0.08%)\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":false,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"Week1.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.33","jupyter":false,"bibliography":["references.bib"],"theme":["cosmo","brand"]},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":false,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"Week1.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"block-headings":true,"jupyter":false,"bibliography":["references.bib"],"documentclass":"scrreprt"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}