---
title: "Deliverables Week 2"
author: "Suwilanji Mwanza"
format: html
date: 10/15/2025
---

# ConLingo Project 2.0

# Overview of the Week 2 Deliverable

Week 2's Deliverable was to solidify the training datasets for the fine-tuning process and then begin the cleaning process. In addition, ensuring that any unethical data was removed.

# Week 2 Deliverables:

1.  2,500 raw examples collected (cumulative with Week 1: 3,000 total)
2.  Initial cleaning applied
3.  Source distribution documented
4.  Bias check on collected data

# What was Accomplished:

## 1. Reaching out to the Reddit API

This week, I focused on identifying quality data and what it entails. I reached out to Reddit to obtain its API key for more relevant data, as I await its response to see if it would still be possible to use its data. (Filled in a form).

## 2. Finalize Data Sources

I collected data from these 7 sources, all of which perform a specific role or fit into the buckets from last week. These data sets were collected primarily from Kaggle and the rest HuggingFace, or webpages.

1.  IndicQuestionGeneration (hindu) (Source: AI4Bharat HuggingFace)
2.  Regional Indian Superstitions & Beliefs (Source: Kaggle)
3.  World Values Survey India (Source: WVS website)
4.  Top Spotify Podcast Episodes (Updated Daily) India (Source: Kaggle)
5.  Pew India Survey Dataset (Source: Pew Research Center)
6.  Hinglish TOP Dataset (Source: Kaggle)
7.  India News Headlines Dataset (Source: Kaggle)

-   The **IndicQuestionGeneration** dataset has 1,078,297 rows. It is part of the IndicNLG Benchmark that consists of one of AI4Bharat's models. It is a large multilingual collection designed for Natural Language Generation (NLG) tasks and is in 11 Indic languages. It is formatted in Question Generation format, which means that when given a context and an answer, it aims to generate a question that leads to that answer. This was derived from the Stanford Question Answering Dataset (SQuAD). There are \~98,000 examples per language, totaling around 1.08 **million examples** across all 11 languages. This dataset would be useful for training and evaluating models that generate comprehension questions in Indian languages. This dataset would be easy to use because it is in a question-answer pair format.

-   The **Regional Indian Superstitions & Beliefs** dataset captures regional superstitions and beliefs from all 28 states and 8 union territories of India, showcasing the fascinating and often lesser-known cultural fabric that shapes daily life across the country. It is split into 500+ training entries and 100+ testing entries, each representing a specific superstition or folk belief. It is a data set for developing AI assistants that understand regional nuances.

-   The **World Values Survey – India, Wave 7 (2017–2022)**, is a nationally representative dataset that captures the evolving social, political, economic, and cultural values of Indian society. Conducted through face-to-face interviews across multiple regions and languages, it surveyed approximately **2,400 respondents** aged 18 and above. The dataset comprises nearly 400 variables, encompassing priorities in life (family, work, religion, and politics), child-rearing values, trust in institutions, gender roles, democracy, corruption, globalization, and religious beliefs.

-   The **Top Spotify Podcast Episodes (Updated Daily)** India contains a daily snapshot of Spotify's top 200 podcast episodes for every country. It also includes detailed information about podcast episodes and shows from the Spotify API. Daily data collection began on 2025-10-11; additionally, some data is available from 2024-09-02 to 2024-10-23. This dataset will help take a different angle on culture by comparingpodcast popularity to understand cultural influences on podcast consumption. India is one of the regions.

-   The **Pew India Survey Dataset** is the raw data from a large-scale, face-to-face survey of 29,999 Indian adults (ages 18 and older) conducted by the [Pew Research Center](https://www.pewresearch.org/dataset/india-survey-dataset/) across 26 states and three union territories between November 17, 2019, and March 23, 2020. Conducted in 17 languages using computer-assisted personal interviews (CAPI), the sample was designed with an oversampling strategy to capture a broad view of religious demographics, including interviews with 22,975 Hindus, 3,336 Muslims, 1,782 Sikhs, 1,011 Christians, 719 Buddhists, and 109 Jains. This dataset provides researchers with detailed information on a wide range of social attitudes and beliefs in India, specifically covering topics such as religious identity, beliefs, practices, nationalism, and tolerance in Indian society.

-   The **Hinglish TOP Dataset** comprises a large (10K) human-annotated code-switched semantic parsing dataset, which contains 10,000 real examples of people speaking in Hinglish commands. Humans have manually labeled or "tagged" the computer-readable meaning for each one. Additionally, the 170K generated utterances using the CST5 augmentation technique introduced in the paper were utilized, which significantly expanded the dataset. To achieve this, researchers employed a special trick called **CST5** to create **170,000 new, synthetic** (computer-generated) Hinglish commands based on the existing data. Queries are derived from TOPv2, a multi-domain task-oriented semantic parsing dataset.

-   The **India News Headlines Dataset** is a comprehensive historical archive of notable events in the Indian subcontinent from 2001 to Q2 2023, recorded in real-time by journalists in India. It contains approximately 3.8 million events published by The Times of India. There are 3,876,557 rows in the CSV. Due to the heavy daily volume (avg. 600 articles) over multiple years, this data offers a deep insight into Indian society, its priorities, events, issues, and talking points, and how they have unfolded over time.

These seven sources will be used to train our data. However, because these sets combined are millions of data points, we will only use 6,000 of them to start. I will also be manually validating them as much as I can for any toxicity.

In this instance, **toxicity** is defined as data that may be considered harmful or detrimental. Sexual, indecent behaviour, curse words, and the like.

An **Excel sheet** with the following categories of data was collected and shared with the team.

## 3. Distribution of Data

After deciding on these datasets, due to the large amount of data in each, I had to narrow down the dataset sizes to ensure we have 6,000 high-quality examples to train our model. We can gather these examples by numbering each data point and doing a random sample, with the no. of examples as the goal.

The breakdown of the datasets and their approximate number of examples are:

| Subset                                             | No. of Examples |
|----------------------------------------------------|-----------------|
| IndicQuestionGeneration                            | 1000            |
| Regional Indian Superstitions & Beliefs            | 660             |
| World Values Survey India                          | 1000            |
| Top Spotify Podcast Episodes (Updated Daily) India | 500             |
| Pew India Survey Dataset                           | 1000            |
| Hinglish TOP Dataset                               | 1000            |
| India News Headlines Dataset                       | 840             |
| **Total:**                                         | **6,000**       |

This shows the distribution of our data set piece:

![Titan GPU environment setup confirmation](img/week_2.png)

The bottleneck in this process is that the examples must be handpicked.

## 4. Cleaning Script

The initial cleaning script will be better created. I integrated Claude into its development, so I can understand what automating that part looks like and the different functions. I will then create my own based on the final recommendation from Moses on how he wants the data to be structured.

```{md}
# Fine-Tuning Pipeline for ConLingo 2.0
# 10/30/25
# ETL Pipeline

# Datasets included:
# - India News Headlines Datasets 2001-01-01 ; End Date: 2023-06-30 (for example)

"""
Data Cleaning & Preprocessing Pipeline for Fine-tuning

Steps:
1. Load datasets
2. Clean text (remove URLs, excessive punctuation, spam)
3. Deduplicate (cosine similarity > 0.9)
4. Flag toxic content (Detoxify)
"""

import os
import re
import string
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from detoxify import Detoxify
from tqdm import tqdm

# ---------------------------
# CONFIGURATION
# ---------------------------
DATASETS = {
    "IndicQuestionGeneration_hi": "path/to/IndicQuestionGeneration.csv",
    "RegionalIndianSuperstitions": "path/to/RegionalIndianSuperstitions.csv",
    "WorldValuesSurveyIndia": "path/to/WorldValuesSurveyIndia.csv",
    "TopSpotifyPodcastEpisodes": "path/to/TopSpotifyPodcastEpisodes.csv",
    "IndiaPewSurvey": "path/to/IndiaPewSurvey.csv",
    "HinglishTOP": "path/to/HinglishTOP.csv",
    "IndiaNewsHeadlines": "path/to/IndiaNewsHeadlines.csv"
}

TEXT_COLUMN = "text"   # Change this if your datasets have a different column name

# ---------------------------
# TEXT CLEANING FUNCTIONS
# ---------------------------
def clean_text(text: str) -> str:
    """Remove URLs, excessive punctuation, and obvious spam."""
    if not isinstance(text, str):
        return ""
    text = re.sub(r"http\S+|www\S+|https\S+", "", text)  # remove URLs
    text = re.sub(r"\s+", " ", text)  # collapse whitespace
    text = re.sub(rf"[{re.escape(string.punctuation)}]", " ", text)  # remove punctuation
    text = re.sub(r"\b(buy now|click here|free money|subscribe)\b", "", text, flags=re.I)
    return text.strip()

# ---------------------------
# DEDUPLICATION
# ---------------------------
def deduplicate_texts(df: pd.DataFrame, column: str, threshold=0.9) -> pd.DataFrame:
    """Remove near-duplicates using cosine similarity."""
    texts = df[column].fillna("").tolist()
    vectorizer = TfidfVectorizer().fit_transform(texts)
    similarity = cosine_similarity(vectorizer)
    to_drop = set()
    for i in range(len(texts)):
        for j in range(i + 1, len(texts)):
            if similarity[i, j] > threshold:
                to_drop.add(j)
    return df.drop(df.index[list(to_drop)])

# ---------------------------
# TOXICITY FLAGGING
# ---------------------------
def flag_toxicity(df: pd.DataFrame, column: str) -> pd.DataFrame:
    """Add a column with toxicity scores using Detoxify."""
    model = Detoxify('original')
    toxicity_scores = []
    for text in tqdm(df[column], desc="Toxicity Scoring"):
        score = model.predict(str(text))
        toxicity_scores.append(score["toxicity"])
    df["toxicity_score"] = toxicity_scores
    df["is_toxic_flag"] = df["toxicity_score"] > 0.5  # manual review threshold
    return df

# ---------------------------
# PIPELINE EXECUTION
# ---------------------------
def process_dataset(name, path):
    print(f"\nProcessing {name}...")
    df = pd.read_csv(path)
    if TEXT_COLUMN not in df.columns:
        raise ValueError(f"Column '{TEXT_COLUMN}' not found in {path}")

    # Step 1: Clean
    df[TEXT_COLUMN] = df[TEXT_COLUMN].apply(clean_text)

    # Step 2: Deduplicate
    df = deduplicate_texts(df, TEXT_COLUMN)

    # Step 3: Flag toxicity
    df = flag_toxicity(df, TEXT_COLUMN)

    # Save processed version
    out_path = f"cleaned_{name}.csv"
    df.to_csv(out_path, index=False)
    print(f"Saved cleaned dataset to {out_path}")

# ---------------------------
# MAIN SCRIPT
# ---------------------------
if __name__ == "__main__":
    for name, path in DATASETS.items():
        if os.path.exists(path):
            process_dataset(name, path)
        else:
            print(f" Skipping {name}: file not found at {path}")

```

-   This script will aid in Removing URLs, excessive punctuation, and obvious spam. Deduplicate or Near-duplicate detection (cosine similarity \> 0.9) and lastly flag any toxic content using Detoxify classifier (which will require additional manual review).

# Next Steps:

-   Implement Dr V's recommendations
-   5,000+ examples collected, cleaned, annotated
-   Train/val/test splits finalized (4,000/750/250)
-   Documenting any insights for a research paper

# References:

-   "AI assistance: Gemini was used to summarize the data source pages of some of the sources for me to write my paragraphs."

-   "AI assistance: Claude was used to generate an example script for a very simple backbone pipeline that would be expanded upon."
